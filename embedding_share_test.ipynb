{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-10T07:50:38.980414Z",
     "start_time": "2024-01-10T07:50:35.522861Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/qcqced/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from configuration import CFG\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\"\"\" 인스턴스 상태로 임베딩 셰어링 하는 것과 개별 가중치 매트릭스 단위로 접근해서 셰여링 하는게 좋은지 \"\"\"\n",
    "\n",
    "\n",
    "class GeneratorEmbedding(nn.Module):\n",
    "    \"\"\" BERT Embedding Module class\n",
    "    This module has option => whether or not to use ALBERT Style Factorized Embedding\n",
    "    This Module set & initialize 3 Embedding Layers:\n",
    "        1) Word Embedding 2) Absolute Positional Embedding\n",
    "    Args:\n",
    "        cfg: configuration.py\n",
    "    Notes:\n",
    "        Absolute Positional Embedding added at bottom layers\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: CFG) -> None:\n",
    "        super(GeneratorEmbedding, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.max_seq = cfg.max_seq\n",
    "        self.word_embedding = nn.Embedding(len(cfg.tokenizer), cfg.dim_model)\n",
    "        self.abs_pos_emb = nn.Embedding(cfg.max_seq, cfg.dim_model)  # Absolute Position Embedding for EMD Layer\n",
    "        self.layer_norm1 = nn.LayerNorm(cfg.dim_model, eps=cfg.layer_norm_eps)  # for word embedding\n",
    "        self.layer_norm2 = nn.LayerNorm(cfg.dim_model, eps=cfg.layer_norm_eps)  # for word embedding\n",
    "        self.hidden_dropout = nn.Dropout(p=cfg.hidden_dropout_prob)\n",
    "\n",
    "        # ALBERT Style Factorized Embedding\n",
    "        if self.cfg.is_mf_embedding:\n",
    "            self.word_embedding = nn.Embedding(len(cfg.tokenizer), int(cfg.dim_model/6))\n",
    "            self.projector = nn.Linear(int(cfg.dim_model/6), cfg.dim_model)  # project to original hidden dim\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tuple[nn.Embedding, nn.Embedding]:\n",
    "        if self.cfg.is_mf_embedding:\n",
    "            word_embeddings = self.hidden_dropout(\n",
    "                self.layer_norm1(self.projector(self.word_embedding(inputs)))\n",
    "            )\n",
    "        else:\n",
    "            word_embeddings = self.hidden_dropout(\n",
    "                self.layer_norm1(self.word_embedding(inputs))\n",
    "            )\n",
    "        abs_pos_emb = self.hidden_dropout(\n",
    "            self.layer_norm2(self.abs_pos_emb(torch.arange(inputs.shape[1], device=\"cuda\").repeat(inputs.shape[0]).view(inputs.shape[0], -1)))\n",
    "        )\n",
    "        return word_embeddings, abs_pos_emb\n",
    "    \n",
    "class DiscriminatorEmbedding(nn.Module):\n",
    "    \"\"\" BERT Embedding Module class\n",
    "    This module has option => whether or not to use ALBERT Style Factorized Embedding\n",
    "    This Module set & initialize 3 Embedding Layers:\n",
    "        1) Word Embedding 2) Absolute Positional Embedding\n",
    "    Args:\n",
    "        cfg: configuration.py\n",
    "    Notes:\n",
    "        Absolute Positional Embedding added at bottom layers\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: CFG) -> None:\n",
    "        super(DiscriminatorEmbedding, self).__init__()\n",
    "        self.cfg = cfg\n",
    "        self.max_seq = cfg.max_seq\n",
    "        self.word_embedding = nn.Embedding(len(cfg.tokenizer), 1024)\n",
    "        self.abs_pos_emb = nn.Embedding(cfg.max_seq, 1024)  # Absolute Position Embedding for EMD Layer\n",
    "        self.layer_norm1 = nn.LayerNorm(cfg.dim_model, eps=1e-9)  # for word embedding\n",
    "        self.layer_norm2 = nn.LayerNorm(cfg.dim_model, eps=1e-9)  # for word embedding\n",
    "        self.hidden_dropout = nn.Dropout(p=cfg.hidden_dropout_prob)\n",
    "\n",
    "        # ALBERT Style Factorized Embedding\n",
    "        if self.cfg.is_mf_embedding:\n",
    "            self.word_embedding = nn.Embedding(len(cfg.tokenizer), int(cfg.dim_model/6))\n",
    "            self.projector = nn.Linear(int(cfg.dim_model/6), cfg.dim_model)  # project to original hidden dim\n",
    "\n",
    "    def forward(self, inputs: Tensor) -> Tuple[nn.Embedding, nn.Embedding]:\n",
    "        if self.cfg.is_mf_embedding:\n",
    "            word_embeddings = self.hidden_dropout(\n",
    "                self.layer_norm1(self.projector(self.word_embedding(inputs)))\n",
    "            )\n",
    "        else:\n",
    "            word_embeddings = self.hidden_dropout(\n",
    "                self.layer_norm1(self.word_embedding(inputs))\n",
    "            )\n",
    "        abs_pos_emb = self.hidden_dropout(\n",
    "            self.layer_norm2(self.abs_pos_emb(torch.arange(inputs.shape[1], device=\"cuda\").repeat(inputs.shape[0]).view(inputs.shape[0], -1)))\n",
    "        )\n",
    "        return word_embeddings, abs_pos_emb"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T07:52:41.167377Z",
     "start_time": "2024-01-10T07:52:41.165719Z"
    }
   },
   "id": "2d314e9b16b92c2"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "generator = GeneratorEmbedding(CFG)\n",
    "discriminator = DiscriminatorEmbedding(CFG)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T07:53:13.112977Z",
     "start_time": "2024-01-10T07:53:10.626292Z"
    }
   },
   "id": "86c70ba065cddf4b"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(GeneratorEmbedding(\n   (word_embedding): Embedding(128001, 768)\n   (abs_pos_emb): Embedding(512, 768)\n   (layer_norm1): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n   (layer_norm2): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n   (hidden_dropout): Dropout(p=0.1, inplace=False)\n ),\n DiscriminatorEmbedding(\n   (word_embedding): Embedding(128001, 1024)\n   (abs_pos_emb): Embedding(512, 1024)\n   (layer_norm1): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n   (layer_norm2): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n   (hidden_dropout): Dropout(p=0.1, inplace=False)\n ))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator, discriminator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T07:53:13.117428Z",
     "start_time": "2024-01-10T07:53:13.113247Z"
    }
   },
   "id": "692738750e576838"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "GeneratorEmbedding(\n  (word_embedding): Embedding(128001, 768)\n  (abs_pos_emb): Embedding(512, 768)\n  (layer_norm1): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n  (layer_norm2): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n  (hidden_dropout): Dropout(p=0.1, inplace=False)\n)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 인스턴스 자체를 공유하는 경우, 해당 인스턴 내부에 포함된 다른 모듈 정보도 복사,\n",
    "이것을 원치 않는다면 임베딩 관련 attr만 찍어서 셰어링 할 것\n",
    "\"\"\"\n",
    "\n",
    "discriminator = generator\n",
    "discriminator"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T07:52:53.430582Z",
     "start_time": "2024-01-10T07:52:53.426237Z"
    }
   },
   "id": "2f6258b1317f5d3b"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "discriminator.word_embedding.weight = generator.word_embedding.weight\n",
    "discriminator.abs_pos_emb.weight = generator.abs_pos_emb.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T08:00:27.741798Z",
     "start_time": "2024-01-10T08:00:27.738637Z"
    }
   },
   "id": "3532a2a4c87b1f8"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "(Parameter containing:\n tensor([[ 2.2952, -1.1103, -0.6854,  ..., -0.4859,  0.7391,  0.0829],\n         [-0.0511,  0.2567,  0.4269,  ...,  0.3777, -0.4286, -1.1887],\n         [ 1.4421,  0.0392,  0.1582,  ..., -0.8192, -0.0563,  0.0259],\n         ...,\n         [-1.1109, -1.9360,  0.7205,  ...,  0.8326,  0.7596, -0.2808],\n         [-0.5263,  1.5916, -1.1737,  ..., -2.0828,  1.2085, -0.7531],\n         [ 0.2427, -1.7848, -1.4473,  ...,  1.2797,  1.2451, -1.5539]],\n        requires_grad=True),\n Parameter containing:\n tensor([[ 2.2952, -1.1103, -0.6854,  ..., -0.4859,  0.7391,  0.0829],\n         [-0.0511,  0.2567,  0.4269,  ...,  0.3777, -0.4286, -1.1887],\n         [ 1.4421,  0.0392,  0.1582,  ..., -0.8192, -0.0563,  0.0259],\n         ...,\n         [-1.1109, -1.9360,  0.7205,  ...,  0.8326,  0.7596, -0.2808],\n         [-0.5263,  1.5916, -1.1737,  ..., -2.0828,  1.2085, -0.7531],\n         [ 0.2427, -1.7848, -1.4473,  ...,  1.2797,  1.2451, -1.5539]],\n        requires_grad=True))"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.word_embedding.weight, discriminator.word_embedding.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T08:00:28.871261Z",
     "start_time": "2024-01-10T08:00:28.867034Z"
    }
   },
   "id": "bdcc42681a72634d"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "(Parameter containing:\n tensor([[-0.1623,  1.3104, -0.6516,  ..., -0.5250, -0.0222, -1.2774],\n         [ 0.3852,  0.7240, -0.7078,  ..., -0.6339,  0.3923, -0.4718],\n         [-0.4134, -0.7664,  1.3895,  ..., -1.0974,  1.5678, -0.4249],\n         ...,\n         [ 0.7117,  1.4549, -0.5336,  ...,  0.1409,  0.2749,  1.7711],\n         [-0.5776, -0.6636,  1.5503,  ..., -1.1855,  0.1657,  0.4000],\n         [ 1.3280, -1.6936, -1.2606,  ..., -0.4680,  0.9651, -0.9574]],\n        requires_grad=True),\n Parameter containing:\n tensor([[-0.1623,  1.3104, -0.6516,  ..., -0.5250, -0.0222, -1.2774],\n         [ 0.3852,  0.7240, -0.7078,  ..., -0.6339,  0.3923, -0.4718],\n         [-0.4134, -0.7664,  1.3895,  ..., -1.0974,  1.5678, -0.4249],\n         ...,\n         [ 0.7117,  1.4549, -0.5336,  ...,  0.1409,  0.2749,  1.7711],\n         [-0.5776, -0.6636,  1.5503,  ..., -1.1855,  0.1657,  0.4000],\n         [ 1.3280, -1.6936, -1.2606,  ..., -0.4680,  0.9651, -0.9574]],\n        requires_grad=True))"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.abs_pos_emb.weight, discriminator.abs_pos_emb.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T08:00:29.336231Z",
     "start_time": "2024-01-10T08:00:29.332259Z"
    }
   },
   "id": "f958393868070ec2"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "(Parameter containing:\n tensor([[ 0.0128,  0.4399,  0.2408, -0.3927,  0.1460],\n         [-0.0897,  0.1777, -0.0144,  0.2699,  0.3550],\n         [-0.3313, -0.3217,  0.1335,  0.2754, -0.0162],\n         [ 0.1812, -0.0031, -0.0985,  0.0917,  0.4457],\n         [ 0.4281, -0.1821,  0.3798, -0.3857,  0.0316]], requires_grad=True),\n Parameter containing:\n tensor([[ 0.0360, -0.2234, -0.3651, -0.3511, -0.2490],\n         [ 0.2894, -0.3327,  0.2404,  0.3974, -0.3460],\n         [ 0.0452,  0.0221, -0.1934, -0.3077,  0.0728],\n         [ 0.4087,  0.2177, -0.2958, -0.2741, -0.1044],\n         [ 0.0724,  0.2305,  0.1937, -0.0123, -0.2719]], requires_grad=True))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = nn.Linear(5, 5)\n",
    "test2 = nn.Linear(5, 5)\n",
    "\n",
    "test.weight, test2.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T07:56:17.714137Z",
     "start_time": "2024-01-10T07:56:17.708362Z"
    }
   },
   "id": "415fc9765dd56b43"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[-0.4118, -0.1691,  0.0908, -0.0913,  0.3356],\n        [ 0.2840, -0.2020,  0.3218, -0.1826,  0.3488],\n        [-0.4029,  0.2939, -0.1424,  0.0622,  0.3741],\n        [ 0.1679,  0.1159, -0.0755,  0.2663, -0.2795],\n        [-0.3321,  0.1732,  0.3661,  0.0217,  0.2214]], requires_grad=True)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.weight = test2.weight\n",
    "test.weight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T07:55:16.094903Z",
     "start_time": "2024-01-10T07:55:16.077311Z"
    }
   },
   "id": "3c260576288c56a2"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "\"\"\" to cuda 하기 싫으면 이거 쓰기 \"\"\"\n",
    "\n",
    "test.register_buffer(\"test\", torch.ones(5))\n",
    "test.test.requires_grad = True\n",
    "test.test.requires_grad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-10T07:57:06.468956Z",
     "start_time": "2024-01-10T07:57:06.454226Z"
    }
   },
   "id": "113b438b81abac0c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
