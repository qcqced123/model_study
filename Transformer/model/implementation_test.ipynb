{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-17T05:39:29.010479Z",
     "start_time": "2023-07-17T05:39:29.005551Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    \"\"\"\n",
    "    Making projection matrix(Q, K, V) for each attention head\n",
    "    When you call this class, it returns projection matrix of each attention head\n",
    "    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)\n",
    "    Args:\n",
    "        num_heads: number of heads in MHA, default 8\n",
    "        dim_head: dimension of each attention head, default 64\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads: int = 8, dim_head: int = 64) -> None:\n",
    "        super(Projector, self).__init__()\n",
    "        self.dim_model = num_heads * dim_head\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_head = dim_head\n",
    "\n",
    "    def __call__(self):\n",
    "        fc_q = nn.Linear(self.dim_model, self.dim_head)\n",
    "        fc_k = nn.Linear(self.dim_model, self.dim_head)\n",
    "        fc_v = nn.Linear(self.dim_model, self.dim_head)\n",
    "        return fc_q, fc_k, fc_v\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for multi-head attention (MHA) module in vanilla transformer\n",
    "    We apply linear transformation to input vector by each attention head's projection matrix (8, 512, 64)\n",
    "    Other approaches are possible, such as using one projection matrix for all attention heads (1, 512, 512)\n",
    "    and then split into each attention heads (8. 512, 64)\n",
    "    Args:\n",
    "        dim_model: dimension of model's latent vector space, default 512 from official paper\n",
    "        num_heads: number of heads in MHA, default 8 from official paper\n",
    "        dropout: dropout rate, default 0.1\n",
    "    Math:\n",
    "        MHA(Q, K, V) = Concat(Head1, Head2, ... Head8) * W_concat\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/1706.03762\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dim = dim_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dim_head = int(self.dim / self.num_heads)  # dimension of each attention head\n",
    "        self.dot_scale = torch.sqrt(torch.tensor(self.dim_head))  # scale factor for Qâ€¢K^T Result\n",
    "\n",
    "        # linear combination: projection matrix(Q_1, K_1, V_1, ... Q_n, K_n, V_n) for each attention head\n",
    "        self.projector = Projector(self.num_heads, self.dim_head)  # init instance\n",
    "        self.projector_list = [list(self.projector()) for _ in range(self.num_heads)]  # call instance\n",
    "        self.fc_concat = nn.Linear(self.dim, self.dim)  # for concatenation of each attention head\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: bool = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        1) make Q, K, V matrix for each attention head: [BS, HEAD, SEQ_LEN, DIM_HEAD], ex) [10, 8, 512, 64]\n",
    "        2) Do self-attention in each attention head\n",
    "            - Matmul (Q, K^T) with scale factor (sqrt(DIM_HEAD))\n",
    "            - Mask for padding token (Option for Decoder)\n",
    "            - Softmax\n",
    "            - Matmul (Softmax, V)\n",
    "        3) Concatenate each attention head & linear transformation (512, 512)\n",
    "        \"\"\"\n",
    "        # 1) make Q, K, V matrix for each attention head\n",
    "        Q, K, V = [], [], []\n",
    "\n",
    "        for i in range(self.num_heads):\n",
    "            Q.append(self.projector_list[i][0](x))\n",
    "            K.append(self.projector_list[i][1](x))\n",
    "            V.append(self.projector_list[i][2](x))\n",
    "\n",
    "        Q = torch.stack(Q, dim=1)\n",
    "        K = torch.stack(K, dim=1)\n",
    "        V = torch.stack(V, dim=1)\n",
    "        # 2) Do self-attention in each attention head\n",
    "        attention_score = torch.matmul(Q, K.transpose(-1, -2)) / self.dot_scale\n",
    "        if mask is not None:  # for padding token\n",
    "            attention_score[mask] = float('-inf')\n",
    "        attention_dist = F.softmax(attention_score, dim=-1)  # [BS, HEAD, SEQ_LEN, SEQ_LEN]\n",
    "        attention_matrix = torch.matmul(attention_dist, V).transpose(1, 2).reshape(x.shape[0], x.shape[1], self.dim)  # [BS, SEQ_LEN, DIM]\n",
    "\n",
    "        # 3) Concatenate each attention head & linear transformation (512, 512)\n",
    "        x = self.fc_concat(attention_matrix)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T12:40:38.537521Z",
     "start_time": "2023-07-16T12:40:38.535113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-0.0150,  0.0380, -0.0050,  ..., -0.0405, -0.0748, -0.0368],\n          [-0.0525,  0.0485, -0.0082,  ..., -0.0480, -0.0552, -0.0323],\n          [-0.0359,  0.0470, -0.0003,  ..., -0.0431, -0.0615, -0.0242],\n          ...,\n          [-0.0433,  0.0437, -0.0037,  ..., -0.0463, -0.0659, -0.0287],\n          [-0.0339,  0.0363, -0.0071,  ..., -0.0500, -0.0526, -0.0325],\n          [-0.0377,  0.0323, -0.0083,  ..., -0.0482, -0.0634, -0.0343]],\n \n         [[-0.0337,  0.0676, -0.0072,  ..., -0.0437, -0.0553, -0.0332],\n          [-0.0287,  0.0731, -0.0007,  ..., -0.0512, -0.0519, -0.0123],\n          [-0.0415,  0.0732, -0.0023,  ..., -0.0372, -0.0561, -0.0157],\n          ...,\n          [-0.0395,  0.0665, -0.0115,  ..., -0.0480, -0.0525, -0.0363],\n          [-0.0365,  0.0787, -0.0015,  ..., -0.0318, -0.0453, -0.0211],\n          [-0.0487,  0.0730, -0.0136,  ..., -0.0303, -0.0521, -0.0293]],\n \n         [[-0.0191,  0.0416, -0.0245,  ..., -0.0369, -0.0279, -0.0012],\n          [-0.0197,  0.0395, -0.0098,  ..., -0.0320, -0.0212,  0.0067],\n          [-0.0176,  0.0348, -0.0076,  ..., -0.0326, -0.0350, -0.0068],\n          ...,\n          [-0.0150,  0.0530, -0.0043,  ..., -0.0311, -0.0454, -0.0051],\n          [-0.0144,  0.0421, -0.0126,  ..., -0.0314, -0.0473,  0.0034],\n          [-0.0273,  0.0376, -0.0044,  ..., -0.0295, -0.0433, -0.0039]],\n \n         ...,\n \n         [[-0.0339,  0.0308, -0.0085,  ..., -0.0615, -0.0557, -0.0519],\n          [-0.0371,  0.0276, -0.0167,  ..., -0.0604, -0.0725, -0.0551],\n          [-0.0392,  0.0381, -0.0188,  ..., -0.0759, -0.0671, -0.0497],\n          ...,\n          [-0.0339,  0.0378, -0.0087,  ..., -0.0461, -0.0621, -0.0448],\n          [-0.0210,  0.0416,  0.0066,  ..., -0.0714, -0.0722, -0.0555],\n          [-0.0368,  0.0355, -0.0103,  ..., -0.0645, -0.0588, -0.0569]],\n \n         [[-0.0038,  0.0683,  0.0139,  ..., -0.0228, -0.0563, -0.0052],\n          [-0.0097,  0.0542, -0.0045,  ..., -0.0282, -0.0364, -0.0046],\n          [-0.0175,  0.0582,  0.0043,  ..., -0.0166, -0.0447, -0.0113],\n          ...,\n          [-0.0101,  0.0620,  0.0021,  ..., -0.0186, -0.0381, -0.0006],\n          [-0.0220,  0.0625,  0.0080,  ..., -0.0394, -0.0457, -0.0170],\n          [-0.0184,  0.0578,  0.0024,  ..., -0.0259, -0.0632,  0.0025]],\n \n         [[-0.0140,  0.0357, -0.0178,  ..., -0.0658, -0.0450, -0.0060],\n          [-0.0160,  0.0391, -0.0285,  ..., -0.0775, -0.0397, -0.0095],\n          [-0.0253,  0.0458, -0.0147,  ..., -0.0735, -0.0381, -0.0277],\n          ...,\n          [-0.0191,  0.0468, -0.0230,  ..., -0.0755, -0.0446, -0.0209],\n          [-0.0118,  0.0467, -0.0154,  ..., -0.0719, -0.0395, -0.0177],\n          [-0.0094,  0.0355, -0.0260,  ..., -0.0806, -0.0458, -0.0197]]],\n        grad_fn=<ViewBackward0>),\n torch.Size([10, 512, 512]))"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Debug for MultiHeadAttention \"\"\"\n",
    "\n",
    "x = torch.randn(10, 512, 512)\n",
    "test_head = MultiHeadAttention()\n",
    "test_result = test_head(x)\n",
    "test_result, test_result.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T12:40:38.798045Z",
     "start_time": "2023-07-16T12:40:38.689980Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
