{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-02T03:39:25.662680Z",
     "start_time": "2024-01-02T03:39:20.399022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/qcqced/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from typing import Dict, List, Tuple, Callable, Any\n",
    "from configuration import CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World 123  Example 테스트 文字列\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Helper Function \"\"\"\n",
    "\n",
    "def select_alphanumeric_and_non_english(text: str) -> str:\n",
    "    pattern = re.compile(r'[^\\w\\d\\s]|_')\n",
    "    result = pattern.sub('', text)\n",
    "    return result\n",
    "\n",
    "def select_tokens(tokens: List[str]) -> List[str]:\n",
    "    selected_tokens = [token for token in tokens if re.match(r'^[\\w\\d\\s]+$', token) and '_' not in token]\n",
    "    return selected_tokens\n",
    "\n",
    "def select_string(token: str) -> str:\n",
    "    flag = True if re.match(r'^[\\w\\d\\s]+$', token) else False\n",
    "    return selected_token\n",
    "\n",
    "text = \"Hello, World! 123 / Example_____ 테스트 文字列\"\n",
    "result = select_alphanumeric_and_non_english(text)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T04:11:04.267661Z",
     "start_time": "2024-01-02T04:11:04.255037Z"
    }
   },
   "id": "b5cfbf1438f64e13"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_valid_start(token, pattern):\n",
    "    # 정규표현식: 알파벳, 숫자, 다른 언어의 문자열 선택 (언더스코어 제외)\n",
    "    regex_pattern = re.compile(r'[^\\w\\d\\s]|_')\n",
    "    \n",
    "    # 주어진 문자열이 특정 패턴으로 시작하며, 언더스코어를 포함하지 않는지 확인\n",
    "    return token.startswith(pattern) and not bool(regex_pattern.search(token))\n",
    "\n",
    "# 예시 토큰 리스트\n",
    "tokens = ['▁trained', ',', '▁train', ',', '▁Pre', 'trained', ',', '▁Pre', 'training', '.', '▁Pre', 'trained', ',', '▁Pre', 'training', ',', '▁Pre', 'trained', ',', '▁Pre', 'training', ',', '▁Pre', 'trained', ',', '▁Pre', 'training', ',', '▁Pre', 'trained', ',', '▁Pre', 'training', ',', '▁Pre', 'trained', ',', '▁Pre', 'training', ',', '▁Pre', 'trained', ',', '▁Pre', 'training', ',', '▁Pre', 'trained', ',', '▁Pre', 'training', ',', '▁Pre', 'trained', ',', '▁Pre', 'training', ',', '▁Pre', 'trained', ',', '▁Pre', 'training', ',', '▁Pre']\n",
    "\n",
    "# 특정 패턴으로 시작하는 토큰만 선택\n",
    "pattern_to_start_with = '▁Pre'\n",
    "selected_tokens = [token for token in tokens if is_valid_start(token, pattern_to_start_with)]\n",
    "\n",
    "print(selected_tokens)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T04:17:32.218762Z",
     "start_time": "2024-01-02T04:17:32.210277Z"
    }
   },
   "id": "c733b34d55e71b94"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS trained  train  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training  Pre trained  Pre training SEP\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Masked Text: [MASK] trained, train[MASK] Pretrained,[MASK][MASK]. Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining[MASK] Pretrained,[MASK][MASK], Pretrained, Pretraining, Pre[MASK], Pretraining[MASK] Pre[MASK], Pretraining, Pretrained, Pretraining, Pretrained, Pre[MASK],[MASK]trained, Pretraining, Pretrained,[MASK]training, Pretrained, Pretraining, Pretrained, Pretraining[MASK] Pretrained, Pretraining. Pretrained, Pretraining, Pretrained, Pre[MASK], Pretrained[MASK] Pretraining, Pretrained, Pre[MASK],[MASK]trained,[MASK][MASK], Pretrained, Pretraining, Pretrained,[MASK]training,[MASK][MASK], Pretraining,[MASK][MASK][MASK][MASK]training, Pretrained, Pretraining, Pretrained,[MASK]training, Pretrained, Pretraining, Pre[MASK], Pretraining, Pre[MASK], Pretraining[SEP]\n",
      "Mask Labels: [1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import random\n",
    "\n",
    "def whole_word_masking(text, tokenizer, mlm_probability=0.15):\n",
    "    # Tokenize the text using the Unicode tokenizer\n",
    "    tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "    print(select_alphanumeric_and_non_english(str(tokens)))\n",
    "    # Whole Word Masking\n",
    "    mask_labels = [0] * len(tokens)\n",
    "    print(mask_labels)\n",
    "    for i, token in enumerate(tokens):\n",
    "        # Set probability of masking each word\n",
    "        if random.random() < mlm_probability:\n",
    "            mask_labels[i] = 1\n",
    "\n",
    "            # Find the boundaries of the whole word\n",
    "            j = i + 1\n",
    "            while j < len(tokens) and tokens[j].startswith(\"\"):\n",
    "                mask_labels[j] = 1\n",
    "                j += 1\n",
    "\n",
    "    # Convert mask labels to tensor\n",
    "    mask_labels = torch.tensor(mask_labels, dtype=torch.long)\n",
    "\n",
    "    # Apply masking to input tokens\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "\n",
    "    input_ids[mask_labels == 1] = tokenizer.mask_token_id\n",
    "\n",
    "    return input_ids, mask_labels\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'your_unicode_tokenizer' with the actual model name or path\n",
    "tokenizer = CFG.tokenizer\n",
    "# Sample input for testing\n",
    "input_text = \"trained, train, Pretrained, Pretraining. Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining. Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining\"\n",
    "\n",
    "# Apply Whole Word Masking\n",
    "masked_input, mask_labels = whole_word_masking(input_text, tokenizer)\n",
    "\n",
    "# Print results\n",
    "print(\"Masked Text:\", tokenizer.decode(masked_input.tolist()))\n",
    "print(\"Mask Labels:\", mask_labels.tolist())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T04:07:37.216096Z",
     "start_time": "2024-01-02T04:07:37.207430Z"
    }
   },
   "id": "ea8547c517d888c9"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/qcqced/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/transformers/data/data_collator.py:951: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[64], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m masked_tokens \u001B[38;5;241m=\u001B[39m DataCollatorForWholeWordMask(tokenizer)([input_tokens])\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Convert masked tokens back to text for visualization\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m masked_text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(\u001B[43mmasked_tokens\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Print results\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput Text:\u001B[39m\u001B[38;5;124m\"\u001B[39m, input_text)\n",
      "\u001B[0;31mKeyError\u001B[0m: 0"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForWholeWordMask\n",
    "import torch\n",
    "\n",
    "# Example input text\n",
    "input_text = \"This is an example sentence for Span Masking algorithm. It is important to keep the token length below 100 for demonstration purposes.\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "\n",
    "# Tokenize input text\n",
    "input_tokens = tokenizer.tokenize(input_text)\n",
    "\n",
    "# Whole Word Masking: Create a list of random labels (1 for [MASK], 0 for others)\n",
    "mask_labels = [1 if token.startswith(\"▁\") else 0 for token in input_tokens]\n",
    "\n",
    "# Whole Word Masking: Apply masking to input tokens\n",
    "input_tokens = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "masked_tokens = DataCollatorForWholeWordMask(tokenizer)([input_tokens], mask_labels=mask_labels)\n",
    "\n",
    "# Convert masked tokens back to text for visualization\n",
    "masked_text = tokenizer.decode(masked_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Print results\n",
    "print(\"Input Text:\", input_text)\n",
    "print(\"Masked Text:\", masked_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T16:36:44.754214Z",
     "start_time": "2024-01-01T16:36:42.941551Z"
    }
   },
   "id": "cf7725e28162b34d"
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     1,   3266,    261, 128000,    261,   3810,  16676,    261,   3810,\n",
      "          18782,    260,   3810, 128000,    261,   3810, 128000,    261, 128000,\n",
      "         128000, 128000, 128000, 128000, 128000, 128000,  16676,    261,   3810,\n",
      "          18782,    261,   3810,  16676,    261,   3810,  18782,    261,   3810,\n",
      "          16676, 128000, 102707,  18782,    261,   3810,  16676, 128000, 128000,\n",
      "          18782,    261,   3810,  16676,    261,   3810,  18782, 128000, 128000,\n",
      "          16676,    261,   3810,  18782,    261,   3810,  16676, 128000, 128000,\n",
      "          18782,    261,   3810,  16676,    261,   3810,  18782,    261,   3810,\n",
      "         128000,    261,   3810,  18782, 128000,   3810,  16676, 128000, 128000,\n",
      "          18782,    261,   3810,  16676,    261,   3810,  18782,    261,   3810,\n",
      "          16676, 128000,   3810,  18782,    261,   3810,  16676,    261,   3810,\n",
      "          18782,    260,   3810,  16676,    261,   3810,  18782,    261,   3810,\n",
      "         128000,    261,   3810, 128000,    261,   3810,  16676,    261,   3810,\n",
      "          18782,    261,   3810,  16676,    261,   3810,  18782,    261,   3810,\n",
      "          16676, 128000, 128000, 128000,    261,   3810, 128000,    261,   3810,\n",
      "          18782, 128000,   3810,  16676,    261,   3810, 128000,    261,   3810,\n",
      "         128000, 128000, 128000, 128000, 128000, 128000,  16676,    261,   3810,\n",
      "         128000, 128000, 128000,  16676,    261,   3810,  18782,    261,   3810,\n",
      "         128000,    261, 128000, 128000,    261,   3810,  16676, 128000, 128000,\n",
      "          18782, 128000, 128000,  16676,    261,   3810,  18782,    261,   3810,\n",
      "          16676, 128000, 128000,  18782,    261,      2]])\n",
      "\n",
      "tensor([[ -100,  -100,   261,  2184,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100, 16676,  -100,  -100, 18782,   261,  3810, 16676,   261,\n",
      "          3810, 18782,   261,  3810,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   261,  3810,  -100,\n",
      "          -100,  -100,  -100,   261,  3810,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,   261,  3810,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,   261,  3810,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100, 16676,  -100,  -100,  -100,   261,  3810,  -100,   261,\n",
      "          3810,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,   261,  3810,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 16676,  -100,\n",
      "          -100, 18782,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,   261,  3810, 18782,\n",
      "          -100,  -100, 16676,  -100,  -100,  -100,   261,  3810,  -100,  -100,\n",
      "          -100, 18782,  -100,  -100, 16676,   261,  3810, 18782,   261,  3810,\n",
      "          -100,  -100,  -100, 18782,   261,  3810,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100, 16676,   261,  3810, 18782,  -100,  -100, 16676,   261,\n",
      "          3810,  -100,   261,  3810,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,   261,  3810,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "mlm_probability = 0.30\n",
    "input_tokens = \"\"\"\n",
    "trained, train, Pretrained, Pretraining. Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining. Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, \n",
    "\"\"\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = CFG.tokenizer\n",
    "input_tokens = tokenizer(\n",
    "    input_tokens,\n",
    "    return_offsets_mapping=True,\n",
    ")\n",
    "input_tokens[\"input_ids\"]\n",
    "\n",
    "\n",
    "def get_padding_mask(input_id: Tensor) -> Tensor:\n",
    "    return torch.zeros(input_id.shape).bool()\n",
    "\n",
    "\n",
    "def _whole_word_mask(\n",
    "        input_tokens: List[str],\n",
    "        max_predictions: int = CFG.max_seq\n",
    ") -> List[int]:\n",
    "    cand_indexes = []\n",
    "    for i, token in enumerate(input_tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "\n",
    "        if len(cand_indexes) >= 1 and token.startswith(\"▁\"):\n",
    "            cand_indexes[-1].append(i)\n",
    "        else:\n",
    "            cand_indexes.append([i])\n",
    "\n",
    "    random.shuffle(cand_indexes)\n",
    "    num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * mlm_probability))))\n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    for index_set in cand_indexes:\n",
    "        if len(masked_lms) >= num_to_predict:\n",
    "            break\n",
    "        if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "            continue\n",
    "        is_any_index_covered = False\n",
    "        for index in index_set:\n",
    "            if index in covered_indexes:\n",
    "                is_any_index_covered = True\n",
    "                break\n",
    "        if is_any_index_covered:\n",
    "            continue\n",
    "        for index in index_set:\n",
    "            covered_indexes.add(index)\n",
    "            masked_lms.append(index)\n",
    "\n",
    "    if len(covered_indexes) != len(masked_lms):\n",
    "        raise ValueError(\"Length of covered_indexes is not equal to length of masked_lms.\")\n",
    "    mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\n",
    "    return mask_labels\n",
    "\n",
    "def get_mask_tokens(inputs, mask_labels):\n",
    "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling(15%):\n",
    "    80% MASK, 10% random, 10% original. Set 'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref\n",
    "    \"\"\"\n",
    "    labels = inputs.clone()\n",
    "    probability_matrix = mask_labels\n",
    "\n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    if tokenizer.pad_token is not None:\n",
    "        padding_mask = labels.eq(tokenizer.pad_token_id)\n",
    "        probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "\n",
    "    masked_indices = probability_matrix.bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "\n",
    "    # 10% of the time, we replace masked input tokens with random word\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "    inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    return inputs, labels\n",
    "\n",
    "def testing(batched):\n",
    "    \"\"\" Masking for MLM with whole-word tokenizing \"\"\"\n",
    "    batched = batched[\"input_ids\"]\n",
    "    input_ids = [torch.tensor(batched)]\n",
    "    padding_mask = [get_padding_mask(x) for x in input_ids]\n",
    "    padding_mask = pad_sequence(padding_mask, batch_first=True, padding_value=True)\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "    mask_labels = []\n",
    "    ref_tokens = []\n",
    "    for input_id in batched:\n",
    "        token = tokenizer._convert_id_to_token(input_id)\n",
    "        ref_tokens.append(token)\n",
    "    mask_labels.append(_whole_word_mask(ref_tokens))\n",
    "\n",
    "    mask_labels = [torch.tensor(x) for x in mask_labels]\n",
    "    mask_labels = pad_sequence(mask_labels, batch_first=True, padding_value=0)\n",
    "    inputs, labels = get_mask_tokens(\n",
    "        input_ids,\n",
    "        mask_labels\n",
    "    )\n",
    "    return inputs, labels\n",
    "\n",
    "inputs, labels = testing(input_tokens)\n",
    "print(inputs, end=\"\\n\\n\")\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T17:23:28.919623Z",
     "start_time": "2024-01-01T17:23:28.913609Z"
    }
   },
   "id": "b68423c9f9b82f9f"
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "data": {
      "text/plain": "','"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._convert_id_to_token(1010)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T17:18:19.173822Z",
     "start_time": "2024-01-01T17:18:19.163511Z"
    }
   },
   "id": "869f4b4ae995b1b2"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "['[CLS]',\n '▁trained',\n ',',\n '▁train',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n '.',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n '.',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '▁Pre',\n 'trained',\n ',',\n '▁Pre',\n 'training',\n ',',\n '[SEP]']"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens.tokens() "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T17:03:52.895456Z",
     "start_time": "2024-01-01T17:03:52.888701Z"
    }
   },
   "id": "da3597987ae11391"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "[None,\n 0,\n 0,\n 1,\n 1,\n 2,\n 2,\n 2,\n 3,\n 3,\n 3,\n 4,\n 4,\n 4,\n 5,\n 5,\n 5,\n 6,\n 6,\n 6,\n 7,\n 7,\n 7,\n 8,\n 8,\n 8,\n 9,\n 9,\n 9,\n 10,\n 10,\n 10,\n 11,\n 11,\n 11,\n 12,\n 12,\n 12,\n 13,\n 13,\n 13,\n 14,\n 14,\n 14,\n 15,\n 15,\n 15,\n 16,\n 16,\n 16,\n 17,\n 17,\n 17,\n 18,\n 18,\n 18,\n 19,\n 19,\n 19,\n 20,\n 20,\n 20,\n 21,\n 21,\n 21,\n 22,\n 22,\n 22,\n 23,\n 23,\n 23,\n 24,\n 24,\n 24,\n 25,\n 25,\n 25,\n 26,\n 26,\n 26,\n 27,\n 27,\n 27,\n 28,\n 28,\n 28,\n 29,\n 29,\n 29,\n 30,\n 30,\n 30,\n 31,\n 31,\n 31,\n 32,\n 32,\n 32,\n 33,\n 33,\n 33,\n 34,\n 34,\n 34,\n 35,\n 35,\n 35,\n 36,\n 36,\n 36,\n 37,\n 37,\n 37,\n 38,\n 38,\n 38,\n 39,\n 39,\n 39,\n 40,\n 40,\n 40,\n 41,\n 41,\n 41,\n 42,\n 42,\n 42,\n 43,\n 43,\n 43,\n 44,\n 44,\n 44,\n 45,\n 45,\n 45,\n 46,\n 46,\n 46,\n 47,\n 47,\n 47,\n 48,\n 48,\n 48,\n 49,\n 49,\n 49,\n 50,\n 50,\n 50,\n 51,\n 51,\n 51,\n 52,\n 52,\n 52,\n 53,\n 53,\n 53,\n 54,\n 54,\n 54,\n 55,\n 55,\n 55,\n 56,\n 56,\n 56,\n 57,\n 57,\n 57,\n 58,\n 58,\n 58,\n 59,\n 59,\n 59,\n 60,\n 60,\n 60,\n 61,\n 61,\n 61,\n None]"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens.word_ids()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T17:04:24.855004Z",
     "start_time": "2024-01-01T17:04:24.851108Z"
    }
   },
   "id": "efa7ed4a934b24dd"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0, 0),\n (1, 8),\n (8, 9),\n (9, 15),\n (15, 16),\n (16, 20),\n (20, 27),\n (27, 28),\n (28, 32),\n (32, 40),\n (40, 41),\n (41, 45),\n (45, 52),\n (52, 53),\n (53, 57),\n (57, 65),\n (65, 66),\n (66, 70),\n (70, 77),\n (77, 78),\n (78, 82),\n (82, 90),\n (90, 91),\n (91, 95),\n (95, 102),\n (102, 103),\n (103, 107),\n (107, 115),\n (115, 116),\n (116, 120),\n (120, 127),\n (127, 128),\n (128, 132),\n (132, 140),\n (140, 141),\n (141, 145),\n (145, 152),\n (152, 153),\n (153, 157),\n (157, 165),\n (165, 166),\n (166, 170),\n (170, 177),\n (177, 178),\n (178, 182),\n (182, 190),\n (190, 191),\n (191, 195),\n (195, 202),\n (202, 203),\n (203, 207),\n (207, 215),\n (215, 216),\n (216, 220),\n (220, 227),\n (227, 228),\n (228, 232),\n (232, 240),\n (240, 241),\n (241, 245),\n (245, 252),\n (252, 253),\n (253, 257),\n (257, 265),\n (265, 266),\n (266, 270),\n (270, 277),\n (277, 278),\n (278, 282),\n (282, 290),\n (290, 291),\n (291, 295),\n (295, 302),\n (302, 303),\n (303, 307),\n (307, 315),\n (315, 316),\n (316, 320),\n (320, 327),\n (327, 328),\n (328, 332),\n (332, 340),\n (340, 341),\n (341, 345),\n (345, 352),\n (352, 353),\n (353, 357),\n (357, 365),\n (365, 366),\n (366, 370),\n (370, 377),\n (377, 378),\n (378, 382),\n (382, 390),\n (390, 391),\n (391, 395),\n (395, 402),\n (402, 403),\n (403, 407),\n (407, 415),\n (415, 416),\n (416, 420),\n (420, 427),\n (427, 428),\n (428, 432),\n (432, 440),\n (440, 441),\n (441, 445),\n (445, 452),\n (452, 453),\n (453, 457),\n (457, 465),\n (465, 466),\n (466, 470),\n (470, 477),\n (477, 478),\n (478, 482),\n (482, 490),\n (490, 491),\n (491, 495),\n (495, 502),\n (502, 503),\n (503, 507),\n (507, 515),\n (515, 516),\n (516, 520),\n (520, 527),\n (527, 528),\n (528, 532),\n (532, 540),\n (540, 541),\n (541, 545),\n (545, 552),\n (552, 553),\n (553, 557),\n (557, 565),\n (565, 566),\n (566, 570),\n (570, 577),\n (577, 578),\n (578, 582),\n (582, 590),\n (590, 591),\n (591, 595),\n (595, 602),\n (602, 603),\n (603, 607),\n (607, 615),\n (615, 616),\n (616, 620),\n (620, 627),\n (627, 628),\n (628, 632),\n (632, 640),\n (640, 641),\n (641, 645),\n (645, 652),\n (652, 653),\n (653, 657),\n (657, 665),\n (665, 666),\n (666, 670),\n (670, 677),\n (677, 678),\n (678, 682),\n (682, 690),\n (690, 691),\n (691, 695),\n (695, 702),\n (702, 703),\n (703, 707),\n (707, 715),\n (715, 716),\n (716, 720),\n (720, 727),\n (727, 728),\n (728, 732),\n (732, 740),\n (740, 741),\n (741, 745),\n (745, 752),\n (752, 753),\n (753, 757),\n (757, 765),\n (765, 766),\n (0, 0)]"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens.offset_mapping"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T17:08:40.464074Z",
     "start_time": "2024-01-01T17:08:40.457874Z"
    }
   },
   "id": "47fb596c6e0d19f8"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "'▁Pre'"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._convert_id_to_token(3810)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T17:02:08.127008Z",
     "start_time": "2024-01-01T17:02:08.117376Z"
    }
   },
   "id": "1b6a2fd8b6d27b6"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-100, -100,   99,   98,   97,   96])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "|\"\"\" Experiment for ELECTRA get discriminator input\n",
    "1) flatten logit tensor and label tensor\n",
    "2) get highest logit\n",
    "3) masked select for mlm masking index\n",
    "4) get index of mlm masking index\n",
    "5) index select for discriminator input \n",
    "\"\"\"\n",
    "flat_logit = torch.tensor([99, 98, 97, 96])\n",
    "test = torch.tensor([-100, -100, 1, 2, 3, 4])\n",
    "mlm_mask_idx = torch.where(test != -100)\n",
    "\n",
    "test2 = test.clone()\n",
    "test[mlm_mask_idx] = flat_logit\n",
    "test\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:16:45.737064Z",
     "start_time": "2023-12-31T08:16:45.728843Z"
    }
   },
   "id": "cb1887885007a163"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 1, 0, 0, 0, 0])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(test, test2).long()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:16:50.262452Z",
     "start_time": "2023-12-31T08:16:50.259370Z"
    }
   },
   "id": "2eb6dd39dea96f1a"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-100, -100],\n        [   1,    2],\n        [   3,    4]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = torch.tensor([1, 2])\n",
    "\n",
    "test = torch.tensor([-100, -100, 1, 2, 3, 4])\n",
    "test.view(-1, a.size(0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:21:25.369486Z",
     "start_time": "2023-12-31T08:21:25.359518Z"
    }
   },
   "id": "d118d5f272197b5a"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Experiment for Huggingface Tokenizer for Building MLM Algorithm \n",
    "meaning of special token in Huggingface Tokenizer is corrspoding to [CLS], [SEP], [MASK], [PAD] ... etc\n",
    "\"\"\"\n",
    "\n",
    "text = 'I am a boy [MASK] [MASK] are a girl [PAD]'\n",
    "tokens = CFG.tokenizer(text)\n",
    "input_ids = [torch.tensor(x) for x in tokens[\"input_ids\"]]\n",
    "special_tokens_mask = CFG.tokenizer.get_special_tokens_mask(input_ids, already_has_special_tokens=True)\n",
    "special_tokens_mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T22:44:13.446160Z",
     "start_time": "2023-12-29T22:44:13.441479Z"
    }
   },
   "id": "7a31d5ebf9b1632a"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n        0.1500, 0.1500, 0.1500])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Experiment for Huggingface Tokenizer for Building MLM Algorithm \n",
    "\"\"\"\n",
    "mlm_probability = 0.15\n",
    "probability_matrix = torch.full(torch.tensor(input_ids).shape, mlm_probability)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T22:44:14.030451Z",
     "start_time": "2023-12-29T22:44:14.025780Z"
    }
   },
   "id": "487442037a73caed"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "masked_fill_() received an invalid combination of arguments - got (list, value=float), but expected one of:\n * (Tensor mask, Tensor value)\n      didn't match because some of the arguments have invalid types: (!list of [int, int, int, int, int, int, int, int, int, int, int, int]!, !value=float!)\n * (Tensor mask, Number value)\n      didn't match because some of the arguments have invalid types: (!list of [int, int, int, int, int, int, int, int, int, int, int, int]!, !value=float!)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\" Experiment for Huggingface Tokenizer for Building MLM Algorithm \u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mprobability_matrix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmasked_fill_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspecial_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m masked_indices \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mbernoulli(probability_matrix)\u001B[38;5;241m.\u001B[39mbool()\n\u001B[1;32m      5\u001B[0m masked_indices\n",
      "\u001B[0;31mTypeError\u001B[0m: masked_fill_() received an invalid combination of arguments - got (list, value=float), but expected one of:\n * (Tensor mask, Tensor value)\n      didn't match because some of the arguments have invalid types: (!list of [int, int, int, int, int, int, int, int, int, int, int, int]!, !value=float!)\n * (Tensor mask, Number value)\n      didn't match because some of the arguments have invalid types: (!list of [int, int, int, int, int, int, int, int, int, int, int, int]!, !value=float!)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Experiment for Huggingface Tokenizer for Building MLM Algorithm \n",
    "\"\"\"\n",
    "probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "masked_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T22:44:15.035358Z",
     "start_time": "2023-12-29T22:44:15.033134Z"
    }
   },
   "id": "96982a7b1365afaa"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43minput_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m~\u001B[39;49m\u001B[43mmasked_indices\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m\n",
      "\u001B[0;31mTypeError\u001B[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "input_ids[~masked_indices] = -100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T22:43:21.978416Z",
     "start_time": "2023-12-29T22:43:21.966992Z"
    }
   },
   "id": "5cf33010e2468866"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def hf_load_dataset(cfg: CFG) -> DatasetDict:\n",
    "    \"\"\" Load dataset from Huggingface Datasets\n",
    "    Notes:\n",
    "        This function is temporary just fit-able for Wikipedia dataset\n",
    "    References:\n",
    "        https://github.com/huggingface/datasets/blob/main/src/datasets/load.py#2247\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(cfg.hf_dataset, cfg.language)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def hf_split_dataset(cfg: CFG, dataset: Dataset) -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\" Split dataset from Huggingface Datasets with huggingface method \"train_test_split\"\n",
    "    Args:\n",
    "        cfg: configuration.CFG, needed to load split ratio, seed value\n",
    "        dataset: Huggingface Datasets object, dataset from Huggingface Datasets\n",
    "    Notes:\n",
    "        This function is temporary just fit-able for Wikipedia dataset & MLM Task\n",
    "    \"\"\"\n",
    "    dataset = dataset.train_test_split(cfg.split_ratio, seed=cfg.seed)\n",
    "    train, valid = dataset['train'], dataset['test']\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def chunking(sequences: Dict, cfg: CFG = CFG) -> List[str]:\n",
    "    \"\"\" Chunking sentence to token using pretrained tokenizer\n",
    "    Args:\n",
    "        cfg: configuration.CFG, needed to load pretrained tokenizer\n",
    "        sequences: list, sentence to chunking\n",
    "    References:\n",
    "        https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling\n",
    "    \"\"\"\n",
    "    return cfg.tokenizer([\" \".join(x) for x in sequences['text']])\n",
    "\n",
    "\n",
    "def group_texts(sequences: Dict, cfg: CFG = CFG) -> Dict:\n",
    "    \"\"\" Dealing Problem: some of data instances are longer than the maximum input length for the model,\n",
    "    This function is ONLY used to HF Dataset Object\n",
    "    1) Concatenate all texts\n",
    "    2) We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    3) customize this part to your needs\n",
    "    4) Split by chunks of max_len\n",
    "    \"\"\"\n",
    "    concatenated_sequences = {k: sum(sequences[k], []) for k in sequences.keys()}\n",
    "    total_length = len(concatenated_sequences[list(sequences.keys())[0]])\n",
    "    if total_length >= cfg.max_seq:\n",
    "        total_length = (total_length // cfg.max_seq) * cfg.max_seq\n",
    "    result = {\n",
    "        k: [t[i: i + cfg.max_seq] for i in range(0, total_length, cfg.max_seq)]\n",
    "        for k, t in concatenated_sequences.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def apply_preprocess(dataset: Dataset, function: Callable, batched: bool = True, num_proc: int = 4, remove_columns: any = None) -> Dataset:\n",
    "    \"\"\" Apply preprocessing to text data, which is using huggingface dataset method \"map()\"\n",
    "    for pretrained training (MLM, CLM)\n",
    "    Args:\n",
    "        dataset: Huggingface Datasets object, dataset from Huggingface Datasets\n",
    "        function: Callable, function that you want to apply\n",
    "        batched: bool, default True, if you want to apply function to batched data, set True\n",
    "        num_proc: int, default 4, number of process for multiprocessing\n",
    "        remove_columns: any, default None, if you want to remove some columns, set column name\n",
    "    References:\n",
    "        https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling\n",
    "    \"\"\"\n",
    "    mapped_dataset = dataset.map(\n",
    "        function,\n",
    "        batched=batched,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=remove_columns,\n",
    "    )\n",
    "    return mapped_dataset\n",
    "\n",
    "\n",
    "def load_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\" Load data_folder from csv file like as train.csv, test.csv, val.csv\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def no_char(text):\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]$\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def no_multi_spaces(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "\n",
    "def underscore_to_space(text: str):\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(source):\n",
    "    \"\"\" Remove all the special characters\n",
    "    \"\"\"\n",
    "    source = re.sub(r'\\W', ' ', str(source))\n",
    "    source = re.sub(r'^b\\s+', '', source)\n",
    "    source = source.lower()\n",
    "    return source\n",
    "\n",
    "\n",
    "def cleaning_words(text: str) -> str:\n",
    "    \"\"\" Apply all of cleaning process to text data\n",
    "    \"\"\"\n",
    "    tmp_text = underscore_to_space(text)\n",
    "    tmp_text = no_char(tmp_text)\n",
    "    tmp_text = preprocess_text(tmp_text)\n",
    "    tmp_text = no_multi_spaces(tmp_text)\n",
    "    return tmp_text\n",
    "\n",
    "\n",
    "def split_token(inputs: str):\n",
    "    \"\"\" Convert malform list to Python List Object & elementwise type casting\n",
    "    \"\"\"\n",
    "    inputs = cleaning_words(inputs)\n",
    "    tmp = inputs.split()\n",
    "    result = list(map(int, tmp))\n",
    "    return result\n",
    "\n",
    "\n",
    "def split_list(inputs: List, max_length: int) -> List[List]:\n",
    "    \"\"\" Split List into sub shorter list, which is longer than max_length\n",
    "    \"\"\"\n",
    "    result = [inputs[i:i + max_length] for i in range(0, len(inputs), max_length)]\n",
    "    return result\n",
    "\n",
    "\n",
    "def flatten_sublist(inputs: List[List], max_length: int = 512) -> List[List]:\n",
    "    \"\"\" Flatten Nested List to 1D-List \"\"\"\n",
    "    result = []\n",
    "    for instance in tqdm(inputs):\n",
    "        tmp = split_token(instance)\n",
    "        if len(tmp) > max_length:\n",
    "            tmp = split_list(tmp, max_length)\n",
    "            for i in range(len(tmp)):\n",
    "                result.append(tmp[i])\n",
    "        else:\n",
    "            result.append(tmp)\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess4tokenizer(input_ids: List, token_type_ids: List, attention_mask: List):\n",
    "    for i, inputs in tqdm(enumerate(input_ids)):\n",
    "        if inputs[0] != 1:\n",
    "            inputs.insert(0, 1)\n",
    "            token_type_ids[i].insert(0, 0)\n",
    "            attention_mask[i].insert(0, 1)\n",
    "        if inputs[-1] != 2:\n",
    "            inputs.append(2)\n",
    "            token_type_ids[i].append(0)\n",
    "            attention_mask[i].append(1)\n",
    "    return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "\n",
    "def cut_instance(input_ids: List, token_type_ids: List, attention_mask: List, min_length: int = 256):\n",
    "    n_input_ids, n_token_type_ids, n_attention_mask = [], [], []\n",
    "    for i, inputs in tqdm(enumerate(input_ids)):\n",
    "        if len(inputs) >= min_length:\n",
    "            n_input_ids.append(inputs)\n",
    "            n_token_type_ids.append(token_type_ids[i])\n",
    "            n_attention_mask.append(attention_mask[i])\n",
    "    return n_input_ids, n_token_type_ids, n_attention_mask\n",
    "\n",
    "\n",
    "def save_pkl(input_dict: Any, filename: str) -> None:\n",
    "    with open(f'{filename}.pkl', 'wb') as file:\n",
    "        pickle.dump(input_dict, file)\n",
    "\n",
    "\n",
    "def load_pkl(filepath: str) -> Any:\n",
    "    \"\"\"  Load pickle file\n",
    "    Examples:\n",
    "        filepath = './dataset_class/data_folder/train'\n",
    "    \"\"\"\n",
    "    with open(f'{filepath}.pkl', 'rb') as file:\n",
    "        output = pickle.load(file)\n",
    "    return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T22:41:54.863067Z",
     "start_time": "2023-12-29T22:41:54.859512Z"
    }
   },
   "id": "a4b602dc2a95d746",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hf_load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m1) Load Dataset, Tokenizer\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m2) Split Dataset, preprocess dataset for MLM Task\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mhf_load_dataset\u001B[49m(CFG)\n\u001B[1;32m      6\u001B[0m _, sub_ds \u001B[38;5;241m=\u001B[39m hf_split_dataset(CFG, ds[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      7\u001B[0m train, valid \u001B[38;5;241m=\u001B[39m hf_split_dataset(CFG, sub_ds)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'hf_load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1) Load Dataset, Tokenizer\n",
    "2) Split Dataset, preprocess dataset for MLM Task\n",
    "\"\"\"\n",
    "ds = hf_load_dataset(CFG)\n",
    "_, sub_ds = hf_split_dataset(CFG, ds['train'])\n",
    "train, valid = hf_split_dataset(CFG, sub_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T05:58:37.078524Z",
     "start_time": "2023-12-28T05:58:36.985803Z"
    }
   },
   "id": "3dd1380b24923e54",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/1025250 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2864e98a41a54796be60b508bf91d442"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/256313 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f839d04ba1ae47a2ac881fcf1aef0589"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Apply preprocessing to dataset \"\"\"\n",
    "\n",
    "chunked_train = apply_preprocess(\n",
    "    train,\n",
    "    chunking,\n",
    "    remove_columns=train.column_names\n",
    ")\n",
    "\n",
    "chunked_valid = apply_preprocess(\n",
    "    valid,\n",
    "    chunking,\n",
    "    remove_columns=valid.column_names\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T08:04:26.000451213Z",
     "start_time": "2023-12-26T07:11:17.408675415Z"
    }
   },
   "id": "34e27b69bc4e33a7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=8):   0%|          | 0/1025250 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "323d0bc5d14543a7ad5b34458b4b9ecc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36miflatmap_unordered\u001B[0;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[1;32m    639\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 640\u001B[0;31m                     \u001B[0;32myield\u001B[0m \u001B[0mqueue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    641\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mEmpty\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<string>\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, *args, **kwds)\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/managers.py\u001B[0m in \u001B[0;36m_callmethod\u001B[0;34m(self, methodname, args, kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m         \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmethodname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 810\u001B[0;31m         \u001B[0mkind\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    811\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36mrecv\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    252\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_readable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 253\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    254\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_ForkingPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetbuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36m_recv_bytes\u001B[0;34m(self, maxsize)\u001B[0m\n\u001B[1;32m    416\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmaxsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 417\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    418\u001B[0m         \u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munpack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"!i\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36m_recv\u001B[0;34m(self, size, read)\u001B[0m\n\u001B[1;32m    381\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0mremaining\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 382\u001B[0;31m             \u001B[0mchunk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mremaining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    383\u001B[0m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTimeoutError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_8506/2765937612.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;34m\"\"\" Grouping text data to fit the maximum input length for the model \"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m grouped_train = apply_preprocess(\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mchunked_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mgroup_texts\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_8506/3833045376.py\u001B[0m in \u001B[0;36mapply_preprocess\u001B[0;34m(dataset, function, batched, num_proc, remove_columns)\u001B[0m\n\u001B[1;32m     65\u001B[0m         \u001B[0mhttps\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m//\u001B[0m\u001B[0mhuggingface\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mco\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mdocs\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mtransformers\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mmain\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mtasks\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mmasked_language_modeling\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m     \"\"\"\n\u001B[0;32m---> 67\u001B[0;31m     mapped_dataset = dataset.map(\n\u001B[0m\u001B[1;32m     68\u001B[0m         \u001B[0mfunction\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m         \u001B[0mbatched\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatched\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    590\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"Dataset\"\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"self\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    591\u001B[0m         \u001B[0;31m# apply actual function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 592\u001B[0;31m         \u001B[0mout\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"DatasetDict\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    593\u001B[0m         \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    594\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    555\u001B[0m         }\n\u001B[1;32m    556\u001B[0m         \u001B[0;31m# apply actual function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 557\u001B[0;31m         \u001B[0mout\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"DatasetDict\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    558\u001B[0m         \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    559\u001B[0m         \u001B[0;31m# re-apply format to the output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mmap\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3183\u001B[0m                         \u001B[0mdesc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdesc\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m\"Map\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34mf\" (num_proc={num_proc})\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3184\u001B[0m                     ) as pbar:\n\u001B[0;32m-> 3185\u001B[0;31m                         for rank, done, content in iflatmap_unordered(\n\u001B[0m\u001B[1;32m   3186\u001B[0m                             \u001B[0mpool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_map_single\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs_iterable\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwargs_per_job\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3187\u001B[0m                         ):\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36miflatmap_unordered\u001B[0;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[1;32m    652\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mpool_changed\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    653\u001B[0m                 \u001B[0;31m# we get the result in case there's an error to raise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 654\u001B[0;31m                 \u001B[0;34m[\u001B[0m\u001B[0masync_result\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0masync_result\u001B[0m \u001B[0;32min\u001B[0m \u001B[0masync_results\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    652\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mpool_changed\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    653\u001B[0m                 \u001B[0;31m# we get the result in case there's an error to raise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 654\u001B[0;31m                 \u001B[0;34m[\u001B[0m\u001B[0masync_result\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0masync_result\u001B[0m \u001B[0;32min\u001B[0m \u001B[0masync_results\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/pool.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    765\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    766\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mready\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 767\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    768\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_success\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    769\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTimeoutError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Grouping text data to fit the maximum input length for the model \"\"\"\n",
    "\n",
    "grouped_train = apply_preprocess(\n",
    "    chunked_train,\n",
    "    group_texts,\n",
    "    num_proc=8,\n",
    ")\n",
    "\n",
    "grouped_valid = apply_preprocess(\n",
    "    chunked_train,\n",
    "    group_texts,\n",
    "    num_proc=8,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T08:13:11.370459308Z",
     "start_time": "2023-12-26T08:10:05.225477717Z"
    }
   },
   "id": "23d7abc06fd50bb1",
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
