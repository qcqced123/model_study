{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-27T17:26:24.809615210Z",
     "start_time": "2023-12-27T17:26:24.807359561Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T17:17:44.211462473Z",
     "start_time": "2023-12-27T17:17:44.167525751Z"
    }
   },
   "id": "b2f87627eba03135",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3232, -0.5501,  0.7347, -0.0715, -1.0714],\n",
      "        [-0.5967, -1.8648, -0.3850,  1.4584,  0.6718],\n",
      "        [-1.4820,  0.8257, -0.3200, -1.1902, -0.5884]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[0.7218, 0.0408, 0.1474, 0.0658, 0.0242],\n        [0.0720, 0.0203, 0.0890, 0.5625, 0.2562],\n        [0.0555, 0.5575, 0.1773, 0.0743, 0.1355]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "inputs = torch.randn(3, 5)\n",
    "print(inputs)\n",
    "F.softmax(inputs, dim=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T17:27:16.965785857Z",
     "start_time": "2023-12-27T17:27:16.961848056Z"
    }
   },
   "id": "7ffd479d34a6fa42",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def hf_load_dataset(cfg: configuration.CFG) -> DatasetDict:\n",
    "    \"\"\" Load dataset from Huggingface Datasets\n",
    "    Notes:\n",
    "        This function is temporary just fit-able for Wikipedia dataset\n",
    "    References:\n",
    "        https://github.com/huggingface/datasets/blob/main/src/datasets/load.py#2247\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(cfg.hf_dataset, cfg.language)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def hf_split_dataset(cfg: configuration.CFG, dataset: Dataset) -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\" Split dataset from Huggingface Datasets with huggingface method \"train_test_split\"\n",
    "    Args:\n",
    "        cfg: configuration.CFG, needed to load split ratio, seed value\n",
    "        dataset: Huggingface Datasets object, dataset from Huggingface Datasets\n",
    "    Notes:\n",
    "        This function is temporary just fit-able for Wikipedia dataset & MLM Task\n",
    "    \"\"\"\n",
    "    dataset = dataset.train_test_split(cfg.split_ratio, seed=cfg.seed)\n",
    "    train, valid = dataset['train'], dataset['test']\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def chunking(sequences: Dict, cfg: configuration.CFG = CFG) -> List[str]:\n",
    "    \"\"\" Chunking sentence to token using pretrained tokenizer\n",
    "    Args:\n",
    "        cfg: configuration.CFG, needed to load pretrained tokenizer\n",
    "        sequences: list, sentence to chunking\n",
    "    References:\n",
    "        https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling\n",
    "    \"\"\"\n",
    "    return cfg.tokenizer([\" \".join(x) for x in sequences['text']])\n",
    "\n",
    "\n",
    "def group_texts(sequences: Dict, cfg: configuration.CFG = CFG) -> Dict:\n",
    "    \"\"\" Dealing Problem: some of data instances are longer than the maximum input length for the model,\n",
    "    This function is ONLY used to HF Dataset Object\n",
    "    1) Concatenate all texts\n",
    "    2) We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    3) customize this part to your needs\n",
    "    4) Split by chunks of max_len\n",
    "    \"\"\"\n",
    "    concatenated_sequences = {k: sum(sequences[k], []) for k in sequences.keys()}\n",
    "    total_length = len(concatenated_sequences[list(sequences.keys())[0]])\n",
    "    if total_length >= cfg.max_seq:\n",
    "        total_length = (total_length // cfg.max_seq) * cfg.max_seq\n",
    "    result = {\n",
    "        k: [t[i: i + cfg.max_seq] for i in range(0, total_length, cfg.max_seq)]\n",
    "        for k, t in concatenated_sequences.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def apply_preprocess(dataset: Dataset, function: Callable, batched: bool = True, num_proc: int = 4, remove_columns: any = None) -> Dataset:\n",
    "    \"\"\" Apply preprocessing to text data, which is using huggingface dataset method \"map()\"\n",
    "    for pretrained training (MLM, CLM)\n",
    "    Args:\n",
    "        dataset: Huggingface Datasets object, dataset from Huggingface Datasets\n",
    "        function: Callable, function that you want to apply\n",
    "        batched: bool, default True, if you want to apply function to batched data, set True\n",
    "        num_proc: int, default 4, number of process for multiprocessing\n",
    "        remove_columns: any, default None, if you want to remove some columns, set column name\n",
    "    References:\n",
    "        https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling\n",
    "    \"\"\"\n",
    "    mapped_dataset = dataset.map(\n",
    "        function,\n",
    "        batched=batched,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=remove_columns,\n",
    "    )\n",
    "    return mapped_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T08:04:58.036532980Z",
     "start_time": "2023-12-26T08:04:57.992520874Z"
    }
   },
   "id": "a4b602dc2a95d746",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26503453cee54dcfa86d57492d551de2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1) Load Dataset, Tokenizer\n",
    "2) Split Dataset, preprocess dataset for MLM Task\n",
    "\"\"\"\n",
    "ds = hf_load_dataset(CFG)\n",
    "_, sub_ds = hf_split_dataset(CFG, ds['train'])\n",
    "train, valid = hf_split_dataset(CFG, sub_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T07:11:17.406020370Z",
     "start_time": "2023-12-26T07:11:06.631553955Z"
    }
   },
   "id": "3dd1380b24923e54",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/1025250 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2864e98a41a54796be60b508bf91d442"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/256313 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f839d04ba1ae47a2ac881fcf1aef0589"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Apply preprocessing to dataset \"\"\"\n",
    "\n",
    "chunked_train = apply_preprocess(\n",
    "    train,\n",
    "    chunking,\n",
    "    remove_columns=train.column_names\n",
    ")\n",
    "\n",
    "chunked_valid = apply_preprocess(\n",
    "    valid,\n",
    "    chunking,\n",
    "    remove_columns=valid.column_names\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T08:04:26.000451213Z",
     "start_time": "2023-12-26T07:11:17.408675415Z"
    }
   },
   "id": "34e27b69bc4e33a7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=8):   0%|          | 0/1025250 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "323d0bc5d14543a7ad5b34458b4b9ecc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36miflatmap_unordered\u001B[0;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[1;32m    639\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 640\u001B[0;31m                     \u001B[0;32myield\u001B[0m \u001B[0mqueue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    641\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mEmpty\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<string>\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, *args, **kwds)\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/managers.py\u001B[0m in \u001B[0;36m_callmethod\u001B[0;34m(self, methodname, args, kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m         \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmethodname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 810\u001B[0;31m         \u001B[0mkind\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    811\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36mrecv\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    252\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_readable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 253\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    254\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_ForkingPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetbuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36m_recv_bytes\u001B[0;34m(self, maxsize)\u001B[0m\n\u001B[1;32m    416\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmaxsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 417\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    418\u001B[0m         \u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munpack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"!i\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36m_recv\u001B[0;34m(self, size, read)\u001B[0m\n\u001B[1;32m    381\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0mremaining\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 382\u001B[0;31m             \u001B[0mchunk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mremaining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    383\u001B[0m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTimeoutError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_8506/2765937612.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;34m\"\"\" Grouping text data to fit the maximum input length for the model \"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m grouped_train = apply_preprocess(\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mchunked_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mgroup_texts\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_8506/3833045376.py\u001B[0m in \u001B[0;36mapply_preprocess\u001B[0;34m(dataset, function, batched, num_proc, remove_columns)\u001B[0m\n\u001B[1;32m     65\u001B[0m         \u001B[0mhttps\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m//\u001B[0m\u001B[0mhuggingface\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mco\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mdocs\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mtransformers\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mmain\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mtasks\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mmasked_language_modeling\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m     \"\"\"\n\u001B[0;32m---> 67\u001B[0;31m     mapped_dataset = dataset.map(\n\u001B[0m\u001B[1;32m     68\u001B[0m         \u001B[0mfunction\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m         \u001B[0mbatched\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatched\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    590\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"Dataset\"\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"self\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    591\u001B[0m         \u001B[0;31m# apply actual function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 592\u001B[0;31m         \u001B[0mout\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"DatasetDict\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    593\u001B[0m         \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    594\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    555\u001B[0m         }\n\u001B[1;32m    556\u001B[0m         \u001B[0;31m# apply actual function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 557\u001B[0;31m         \u001B[0mout\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"DatasetDict\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    558\u001B[0m         \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    559\u001B[0m         \u001B[0;31m# re-apply format to the output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mmap\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3183\u001B[0m                         \u001B[0mdesc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdesc\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m\"Map\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34mf\" (num_proc={num_proc})\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3184\u001B[0m                     ) as pbar:\n\u001B[0;32m-> 3185\u001B[0;31m                         for rank, done, content in iflatmap_unordered(\n\u001B[0m\u001B[1;32m   3186\u001B[0m                             \u001B[0mpool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_map_single\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs_iterable\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwargs_per_job\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3187\u001B[0m                         ):\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36miflatmap_unordered\u001B[0;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[1;32m    652\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mpool_changed\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    653\u001B[0m                 \u001B[0;31m# we get the result in case there's an error to raise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 654\u001B[0;31m                 \u001B[0;34m[\u001B[0m\u001B[0masync_result\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0masync_result\u001B[0m \u001B[0;32min\u001B[0m \u001B[0masync_results\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    652\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mpool_changed\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    653\u001B[0m                 \u001B[0;31m# we get the result in case there's an error to raise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 654\u001B[0;31m                 \u001B[0;34m[\u001B[0m\u001B[0masync_result\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0masync_result\u001B[0m \u001B[0;32min\u001B[0m \u001B[0masync_results\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/pool.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    765\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    766\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mready\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 767\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    768\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_success\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    769\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTimeoutError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Grouping text data to fit the maximum input length for the model \"\"\"\n",
    "\n",
    "grouped_train = apply_preprocess(\n",
    "    chunked_train,\n",
    "    group_texts,\n",
    "    num_proc=8,\n",
    ")\n",
    "\n",
    "grouped_valid = apply_preprocess(\n",
    "    chunked_train,\n",
    "    group_texts,\n",
    "    num_proc=8,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T08:13:11.370459308Z",
     "start_time": "2023-12-26T08:10:05.225477717Z"
    }
   },
   "id": "23d7abc06fd50bb1",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [1, 273, 481, 266, 2388, 0, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I am a boy [PAD]'\n",
    "CFG.tokenizer(text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T03:35:28.083989725Z",
     "start_time": "2023-12-27T03:35:28.017690395Z"
    }
   },
   "id": "84c3e612131699b0",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[<function metrics.metric.accuracy(y_true: <built-in function array>, y_pred: <built-in function array>) -> float>,\n <function metrics.metric.recall(y_true: numpy.ndarray, y_pred: numpy.ndarray) -> float>,\n <function metrics.metric.precision(y_true, y_pred) -> float>,\n <function metrics.metric.f_beta(y_true: numpy.ndarray, y_pred: numpy.ndarray, beta: float = 2) -> float>]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_list = ['accuracy', 'recall', 'precision', 'f_beta']\n",
    "metric_module = []\n",
    "for metrics in metric_list:\n",
    "    metric_module.append(getattr(metric, f'{metrics}'))\n",
    "metric_module"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-27T05:12:55.073219348Z",
     "start_time": "2023-12-27T05:12:55.057129849Z"
    }
   },
   "id": "466d597b8335ffd6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1c90dd130d807db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
