{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:38:19.588801Z",
     "start_time": "2024-01-02T18:38:15.942068Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/qcqced/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from typing import Dict, List, Tuple, Callable, Any\n",
    "from configuration import CFG"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Tokenizer Output: ['trained', 'Ġtrain', 'ĠPret', 'rained', 'ĠPret', 'raining', 'ĠPret', 'rained']\n",
      "\n",
      "Bert Tokenizer in HF Output: ['trained', 'train', 'pre', '##train', '##ed', 'pre', '##train', '##ing', 'pre', '##train', '##ed']\n",
      "\n",
      "Sentencepiece Tokenizer (BPE) Output: ['▁trained', '▁train', '▁Pre', 'trained', '▁Pre', 'training', '▁Pre', 'trained']\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Huggingface Tokenizer Experiment \"\"\"\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "bpe_tokenizer = AutoTokenizer.from_pretrained('gpt2')  # roberta, gpt2\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  # bert\n",
    "sentencepiece_tokenizer = CFG.tokenizer  # deberta, T5\n",
    "\n",
    "text = \"trained train Pretrained Pretraining Pretrained\"\n",
    "print(f'BPE Tokenizer Output: {bpe_tokenizer.tokenize(text)}', end=\"\\n\\n\")\n",
    "print(f'Bert Tokenizer in HF Output: {bert_tokenizer.tokenize(text)}', end=\"\\n\\n\")\n",
    "print(f'Sentencepiece Tokenizer (BPE) Output: {sentencepiece_tokenizer.tokenize(text)}', end=\"\\n\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T09:20:38.006323474Z",
     "start_time": "2024-01-02T09:20:36.666640061Z"
    }
   },
   "id": "c7e65f62f7d6a0b4",
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'XLMRobertaTokenizerFast'"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Find tokenizer type in huggingface pretrained tokenizer \"\"\"\n",
    "\n",
    "bpe_tokenizer.__class__.__name__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T09:20:04.546099915Z",
     "start_time": "2024-01-02T09:20:04.521939394Z"
    }
   },
   "id": "524e610523bc7790",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 'spm.model'\n",
    "test[-6:] == '.model'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T08:55:58.847131806Z",
     "start_time": "2024-01-02T08:55:58.838471859Z"
    }
   },
   "id": "497cb9783a5cba8f",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Helper Function \"\"\"\n",
    "\n",
    "def select_alphanumeric_and_non_english(text: str) -> str:\n",
    "    pattern = re.compile(r'[^\\w\\d\\s]|_')\n",
    "    result = pattern.sub('', text)\n",
    "    return result\n",
    "\n",
    "def select_tokens(tokens: List[str]) -> List[str]:\n",
    "    selected_tokens = [token for token in tokens if re.match(r'^[\\w\\d\\s]+$', token) and '_' not in token]\n",
    "    return selected_tokens\n",
    "\n",
    "def select_post_string(token: str) -> str:\n",
    "    pattern = re.compile(r'[^\\w\\d\\s]|_')\n",
    "    flag = False if re.match(pattern, token) else True\n",
    "    return flag\n",
    "\n",
    "def select_src_string(token: str) -> bool:\n",
    "    \"\"\" set flag value for selecting src tokens to mask in sub-word\n",
    "    Args:\n",
    "        token: str, token to check\n",
    "    \"\"\"\n",
    "    flag = False\n",
    "    if tokenizer_type == 'SPM':\n",
    "        flag = True if token.startswith(\"▁\") else False\n",
    "\n",
    "    elif tokenizer_type == 'BPE':\n",
    "        flag = True if token.startswith(\"Ġ\") else False\n",
    "\n",
    "    elif tokenizer_type == 'WORDPIECE':\n",
    "        pattern = re.compile(r'[^\\w\\d\\s]|_')\n",
    "        flag = False if re.match(pattern, token) else True\n",
    "    return flag\n",
    "\n",
    "\n",
    "text = \"_Hello, World! 123 / Example_____ 테스트 文字列\"\n",
    "test = \"나는는\"\n",
    "result = select_alphanumeric_and_non_english(text)\n",
    "print(select_post_string(test))\n",
    "print(select_src_string(test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T11:11:34.876383122Z",
     "start_time": "2024-01-02T11:11:34.872362866Z"
    }
   },
   "id": "b5cfbf1438f64e13"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/qcqced/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/transformers/data/data_collator.py:951: UserWarning: DataCollatorForWholeWordMask is only suitable for BertTokenizer-like tokenizers. Please refer to the documentation for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[64], line 21\u001B[0m\n\u001B[1;32m     18\u001B[0m masked_tokens \u001B[38;5;241m=\u001B[39m DataCollatorForWholeWordMask(tokenizer)([input_tokens])\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Convert masked tokens back to text for visualization\u001B[39;00m\n\u001B[0;32m---> 21\u001B[0m masked_text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mdecode(\u001B[43mmasked_tokens\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# Print results\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInput Text:\u001B[39m\u001B[38;5;124m\"\u001B[39m, input_text)\n",
      "\u001B[0;31mKeyError\u001B[0m: 0"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorForWholeWordMask\n",
    "import torch\n",
    "\n",
    "# Example input text\n",
    "input_text = \"This is an example sentence for Span Masking algorithm. It is important to keep the token length below 100 for demonstration purposes.\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "\n",
    "# Tokenize input text\n",
    "input_tokens = tokenizer.tokenize(input_text)\n",
    "\n",
    "# Whole Word Masking: Create a list of random labels (1 for [MASK], 0 for others)\n",
    "mask_labels = [1 if token.startswith(\"▁\") else 0 for token in input_tokens]\n",
    "\n",
    "# Whole Word Masking: Apply masking to input tokens\n",
    "input_tokens = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "masked_tokens = DataCollatorForWholeWordMask(tokenizer)([input_tokens], mask_labels=mask_labels)\n",
    "\n",
    "# Convert masked tokens back to text for visualization\n",
    "masked_text = tokenizer.decode(masked_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Print results\n",
    "print(\"Input Text:\", input_text)\n",
    "print(\"Masked Text:\", masked_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T16:36:44.754214Z",
     "start_time": "2024-01-01T16:36:42.941551Z"
    }
   },
   "id": "cf7725e28162b34d"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [3], [5, 6], [8, 9], [11, 12], [14, 15], [17, 18], [20, 21], [23, 24], [26, 27], [29, 30], [32, 33], [35, 36], [38, 39], [41, 42], [44, 45], [47, 48], [50, 51], [53, 54], [56, 57], [59, 60], [62, 63], [65, 66], [68, 69], [71, 72], [74, 75], [77, 78], [80, 81], [83, 84, 85], [87, 88], [90, 91], [93, 94], [96, 97], [99, 100], [102, 103], [105, 106], [108, 109], [111, 112], [114, 115], [117, 118], [120, 121], [123, 124], [126, 127], [129, 130], [132, 133], [135, 136], [138, 139], [141, 142], [144, 145], [147, 148], [150, 151], [153, 154], [156, 157], [159, 160], [162, 163], [165, 166], [168, 169], [171, 172], [174, 175], [177, 178], [180, 181], [183, 184]]\n",
      "tensor([[     1,   3266,    300,   2184,    300, 128000, 128000,    302,   3810,\n",
      "          18782,    260,   3810,  16676,    260,   3810,  18782,    260,   3810,\n",
      "          16676,    261,   3810,  18782,    261,   3810,  16676,    261,   3810,\n",
      "          18782,    261,   3810,  16676,    261,   3810,  18782,    261,   3810,\n",
      "          16676,    261,   3810,  18782,    261,   3810,  16676,    300,   3810,\n",
      "         112895,    300,   3810,  16676,    261,   3810,  18782,    261, 128000,\n",
      "          32195,    261,  48120,  93464,    261,   3810,  16676,    261,   3810,\n",
      "          18782,    261, 128000, 128000,    261,   3810,  18782,    261,   3810,\n",
      "          16676,    261,   3810,  18782,    302,   3810,  16676,    302,   3810,\n",
      "          18782,    261,    840,  17108,  16676,    261,   3810,  18782,    261,\n",
      "           3810,  16676,    261, 128000, 128000,    261, 128000, 128000,    261,\n",
      "           3810,  18782,    260, 128000, 128000,    261,   3810,  18782,    261,\n",
      "           3810,  16676,    261,   3810,  18782,    261,   3810,  16676,    261,\n",
      "           3810,  18782,    261,   3810,  16676,    261,   3810,  18782,    261,\n",
      "           3810,  16676,    261,   3810,  18782,    261, 128000, 128000,    261,\n",
      "           3810,  18782,    261,   3810,  16676,    261, 128000, 128000,    261,\n",
      "           3810,  16676,    261,   3810,  18782,    261,   3810,  16676,    261,\n",
      "           3810,  18782,    261, 128000, 128000,    261,  20634, 128000,    261,\n",
      "         128000,  16676,    261,   3810,  18782,    261,   3810,  16676,    261,\n",
      "           3810,  18782,    261,   3810,  16676,    261,   3810,  18782,    261,\n",
      "         128000, 128000,    261,   3810,  18782,      2]])\n",
      "\n",
      "tensor([[ -100,  -100,  -100,  -100,  -100,  3810, 16676,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  3810, 18782,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  3810, 16676,  -100,  3810, 18782,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  3810, 16676,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  3810, 18782,  -100,  3810, 16676,  -100,  -100,\n",
      "          -100,  -100,  3810, 16676,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  3810, 16676,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  3810, 18782,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  3810, 16676,  -100,  3810,\n",
      "         18782,  -100,  3810, 16676,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          3810, 16676,  -100,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# \"\"\" Test for Whole Word Masking \"\"\"\n",
    "# \n",
    "# mlm_probability = 0.15\n",
    "# # input_tokens = \"\"\"\n",
    "# # trained train Pretrained Pretraining Pretrained Pretraining Pretrained Pretraining Pretrained Pretraining Pretrained Pretraining trained train Pretrained Pretraining Pretrained Pretraining Pretrained Pretraining Pretrained Pretraining Pretrained Pretraining trained train Pretrained Pretraining Pretrained Pretraining Pretrained Pretraining Pretrained Pretraining Pretrained Pretraining  \n",
    "# # \"\"\"\n",
    "# input_tokens = \"trained! train! Pretrained? Pretraining. Pretrained. Pretraining. Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained! Pretraining! Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining? Pretrained? Pretraining, /Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining. Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining\"\n",
    "# \n",
    "# \n",
    "# # tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = CFG.tokenizer\n",
    "# input_tokens = tokenizer(\n",
    "#     input_tokens,\n",
    "# )\n",
    "# input_tokens[\"input_ids\"]\n",
    "# tokenizer_type = 'SPM'\n",
    "# \n",
    "# \n",
    "# def select_post_string(token: str) -> bool:\n",
    "#     \"\"\" set flag value for selecting post tokens to mask in sub-word\n",
    "#     Args:\n",
    "#         token: str, token to check\n",
    "#     \"\"\"\n",
    "#     flag = False\n",
    "#     if tokenizer_type == 'SPM':\n",
    "#         pattern = re.compile(r'[^\\w\\d\\s]|_')\n",
    "#         flag = False if re.match(pattern, token[0]) else True\n",
    "# \n",
    "#     elif tokenizer_type == 'BPE':\n",
    "#         flag = False if token.startswith(\"Ġ\") else True\n",
    "# \n",
    "#     elif tokenizer_type == 'WORDPIECE':\n",
    "#         flag = True if token.startswith(\"##\") else False\n",
    "# \n",
    "#     return flag\n",
    "# \n",
    "# def select_src_string(token: str) -> bool:\n",
    "#     \"\"\" set flag value for selecting src tokens to mask in sub-word\n",
    "#     Args:\n",
    "#         token: str, token to check\n",
    "#     \"\"\"\n",
    "#     flag = False\n",
    "#     if tokenizer_type == 'SPM':\n",
    "#         flag = True if token.startswith(\"▁\") else False\n",
    "# \n",
    "#     elif tokenizer_type == 'BPE':\n",
    "#         flag = True if token.startswith(\"Ġ\") else False\n",
    "# \n",
    "#     elif tokenizer_type == 'WORDPIECE':\n",
    "#         pattern = re.compile(r'[^\\w\\d\\s]|_')\n",
    "#         flag = False if re.match(pattern, token) else True\n",
    "#     return flag\n",
    "# \n",
    "# \n",
    "# def get_padding_mask(input_id: Tensor) -> Tensor:\n",
    "#     return torch.zeros(input_id.shape).bool()\n",
    "# \n",
    "# \n",
    "# def _whole_word_mask(\n",
    "#         input_tokens: List[str],\n",
    "#         max_predictions: int = CFG.max_seq\n",
    "# ) -> List[int]:\n",
    "#     \"\"\" \n",
    "#     1) split input_tokens by space into single token\n",
    "#     2) check if token is src token or post token\n",
    "#         - if cand_indexes not empty and token is post token, append index to cand_indexes\n",
    "#         - if token is src token, append index list to cand_indexes\n",
    "#     \"\"\"\n",
    "#     cand_indexes = []\n",
    "#     for i, token in enumerate(input_tokens):\n",
    "#         if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "#             continue\n",
    "#         if len(cand_indexes) >= 1 and select_post_string(token): \n",
    "#             cand_indexes[-1].append(i)\n",
    "#         elif select_src_string(token):\n",
    "#             cand_indexes.append([i])\n",
    "#     print(cand_indexes)  # 여기서부터 변형해서 만들면 되겠다.\n",
    "#     random.shuffle(cand_indexes)  # shuffle cand_indexes list\n",
    "#     num_to_predict = min(max_predictions, max(1, int(round(len(input_tokens) * mlm_probability))))\n",
    "#     masked_lms = []\n",
    "#     covered_indexes = set()\n",
    "#     for index_set in cand_indexes:\n",
    "#         if len(masked_lms) >= num_to_predict:\n",
    "#             break\n",
    "#         if len(masked_lms) + len(index_set) > num_to_predict:\n",
    "#             continue\n",
    "#         is_any_index_covered = False\n",
    "#         for index in index_set:\n",
    "#             if index in covered_indexes:\n",
    "#                 is_any_index_covered = True\n",
    "#                 break\n",
    "#         if is_any_index_covered:\n",
    "#             continue\n",
    "#         for index in index_set:\n",
    "#             covered_indexes.add(index)\n",
    "#             masked_lms.append(index)\n",
    "# \n",
    "#     if len(covered_indexes) != len(masked_lms):\n",
    "#         raise ValueError(\"Length of covered_indexes is not equal to length of masked_lms.\")\n",
    "#     mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\n",
    "#     return mask_labels\n",
    "# \n",
    "# def get_mask_tokens(inputs, mask_labels):\n",
    "#     \"\"\" Prepare masked tokens inputs/labels for masked language modeling(15%):\n",
    "#     80% MASK, 10% random, 10% original. Set 'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref\n",
    "#     \"\"\"\n",
    "#     labels = inputs.clone()\n",
    "#     probability_matrix = mask_labels\n",
    "# \n",
    "#     special_tokens_mask = [\n",
    "#         tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "#     ]\n",
    "#     probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "#     if tokenizer.pad_token is not None:\n",
    "#         padding_mask = labels.eq(tokenizer.pad_token_id)\n",
    "#         probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "# \n",
    "#     masked_indices = probability_matrix.bool()\n",
    "#     labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "# \n",
    "#     # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "#     indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "#     inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "# \n",
    "#     # 10% of the time, we replace masked input tokens with random word\n",
    "#     indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "#     random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "#     inputs[indices_random] = random_words[indices_random]\n",
    "# \n",
    "#     # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "#     return inputs, labels\n",
    "# \n",
    "# def testing(batched):\n",
    "#     \"\"\" Masking for MLM with whole-word tokenizing \"\"\"\n",
    "#     batched = batched[\"input_ids\"]\n",
    "#     input_ids = [torch.tensor(batched)]\n",
    "#     padding_mask = [get_padding_mask(x) for x in input_ids]\n",
    "#     padding_mask = pad_sequence(padding_mask, batch_first=True, padding_value=True)\n",
    "#     input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "# \n",
    "#     mask_labels = []\n",
    "#     ref_tokens = []\n",
    "#     for input_id in batched:\n",
    "#         token = tokenizer._convert_id_to_token(input_id)\n",
    "#         ref_tokens.append(token)\n",
    "#     mask_labels.append(_whole_word_mask(ref_tokens))\n",
    "# \n",
    "#     mask_labels = [torch.tensor(x) for x in mask_labels]\n",
    "#     mask_labels = pad_sequence(mask_labels, batch_first=True, padding_value=0)\n",
    "#     inputs, labels = get_mask_tokens(\n",
    "#         input_ids,\n",
    "#         mask_labels\n",
    "#     )\n",
    "#     return inputs, labels\n",
    "# \n",
    "# inputs, labels = testing(input_tokens)\n",
    "# print(inputs, end=\"\\n\\n\")\n",
    "# print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T16:31:59.267631Z",
     "start_time": "2024-01-02T16:31:59.262276Z"
    }
   },
   "id": "b68423c9f9b82f9f"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]]\n",
      "27\n",
      "tensor([[     1,   3266,    300, 128000,    300,   3810,  16676,    302,   3810,\n",
      "          18782,    260,   3810,  16676,    260,   3810,  18782,    260,   3810,\n",
      "          16676,    261,   3810,  18782,    261,   3810,  16676,    261,   3810,\n",
      "          18782,    261,   3810,  16676,    261, 128000, 128000, 128000, 128000,\n",
      "         128000, 128000, 128000,  18782,    261,   3810,  16676,    300,   3810,\n",
      "          18782,    300,   3810,  16676,    261,   3810,  18782,    261,   3810,\n",
      "          16676,    261,   3810,  18782,    261,   3810,  16676,    261,   3810,\n",
      "          18782,    261,   3810,  16676,    261,   3810,  18782,    261,   3810,\n",
      "          16676,    261,   3810,  18782,    302,   3810,  16676,    302, 128000,\n",
      "         128000, 128000, 128000,  17108,  16676,    261,   3810,  18782,    261,\n",
      "           3810,  16676,    261,   3810,  18782,    261,   3810,  16676,    261,\n",
      "           3810,  18782,    260,   3810,  16676,    261,   3810,  18782,    261,\n",
      "           3810,  16676,    261,   3810,  18782,    261,   3810,  16676,    261,\n",
      "           3810,  18782,    261, 128000, 128000,    261, 128000, 128000, 128000,\n",
      "           3810,  16676,    261,   3810,  18782,    261, 128000, 128000, 128000,\n",
      "         128000, 128000, 128000,   3810,  16676,    261,   3810,  18782,    261,\n",
      "           3810,  16676,    261,   3810,  18782,    261,   3810,  16676,    261,\n",
      "           3810,  18782,    261,   3810,  16676,    261,   3810,  18782,    261,\n",
      "           3810,  16676,    261,   3810,  18782,    261,   3810,  16676,    261,\n",
      "           3810,  18782,    261,   3810,  16676,    261,   3810,  18782,    261,\n",
      "         128000, 128000, 128000, 128000,  18782,      2]])\n",
      "\n",
      "tensor([[ -100,  -100,  -100,  2184,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  3810, 18782,   261,  3810, 16676,   261,  3810,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          3810, 18782,   261,   840,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          3810, 16676,  -100,  3810, 18782,   261,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  3810, 16676,   261,  3810, 18782,   261,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          3810, 16676,   261,  3810,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test for Span Masking Algorithm \"\"\"\n",
    "\n",
    "input_tokens = \"trained! train! Pretrained? Pretraining. Pretrained. Pretraining. Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained! Pretraining! Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining? Pretrained? Pretraining, /Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining. Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining\"\n",
    "\n",
    "mlm_probability = 0.15\n",
    "masking_budget = 0.15\n",
    "span_probability = 0.2\n",
    "max_span_length = 10\n",
    "\n",
    "tokenizer = CFG.tokenizer\n",
    "input_tokens = tokenizer(\n",
    "    input_tokens,\n",
    ")\n",
    "input_tokens[\"input_ids\"]\n",
    "tokenizer_type = 'SPM'\n",
    "\n",
    "\n",
    "def random_non_negative_integer(max_value: int):\n",
    "    return random.randint(0, max_value)\n",
    "\n",
    "\n",
    "def select_post_string(token: str) -> bool:\n",
    "    \"\"\" set flag value for selecting post tokens to mask in sub-word\n",
    "    Args:\n",
    "        token: str, token to check\n",
    "    \"\"\"\n",
    "    flag = False\n",
    "    if tokenizer_type == 'SPM':\n",
    "        pattern = re.compile(r'[^\\w\\d\\s]|_')\n",
    "        flag = False if re.match(pattern, token[0]) else True\n",
    "\n",
    "    elif tokenizer_type == 'BPE':\n",
    "        flag = False if token.startswith(\"Ġ\") else True\n",
    "\n",
    "    elif tokenizer_type == 'WORDPIECE':\n",
    "        flag = True if token.startswith(\"##\") else False\n",
    "\n",
    "    return flag\n",
    "\n",
    "def select_src_string(token: str) -> bool:\n",
    "    \"\"\" set flag value for selecting src tokens to mask in sub-word\n",
    "    Args:\n",
    "        token: str, token to check\n",
    "    \"\"\"\n",
    "    flag = False\n",
    "    if tokenizer_type == 'SPM':\n",
    "        flag = True if token.startswith(\"▁\") else False\n",
    "\n",
    "    elif tokenizer_type == 'BPE':\n",
    "        flag = True if token.startswith(\"Ġ\") else False\n",
    "\n",
    "    elif tokenizer_type == 'WORDPIECE':\n",
    "        pattern = re.compile(r'[^\\w\\d\\s]|_')\n",
    "        flag = False if re.match(pattern, token) else True\n",
    "    return flag\n",
    "\n",
    "\n",
    "def get_padding_mask(input_id: Tensor) -> Tensor:\n",
    "    return torch.zeros(input_id.shape).bool()\n",
    "\n",
    "\n",
    "def _whole_word_mask(\n",
    "        input_tokens: List[str],\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    0) apply Whole Word Masking Algorithm for make gathering original token index in natural language \n",
    "    1) calculate number of convert into masking tokens with masking budget*len(input_tokens)\n",
    "    2) define span length of this iteration\n",
    "        - span length follow geometric distribution\n",
    "        - span length is limited by max_span_length\n",
    "    \"\"\"\n",
    "    cand_indexes = []\n",
    "    for i, token in enumerate(input_tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if len(cand_indexes) >= 1 and select_post_string(token): \n",
    "            cand_indexes[-1].append(i)\n",
    "        elif select_src_string(token):\n",
    "            cand_indexes.append([i])\n",
    "    \n",
    "    l = len(input_tokens)\n",
    "    src_l = len(cand_indexes)\n",
    "    num_convert_tokens = int(masking_budget * l)  # 27\n",
    "    budget = num_convert_tokens  # int is immutable object, so not need to copy manually \n",
    "    masked_lms = []\n",
    "    covered_indexes = set()\n",
    "    while budget:\n",
    "        span_length = max(1, min(10, int(torch.distributions.Geometric(probs=span_probability).sample())))\n",
    "        src_index = random_non_negative_integer(src_l-1)\n",
    "        if span_length > budget:\n",
    "            if budget < 5:  # 남은 예산이 너무 적은 경우 수많은 Iteration 발생을 막기 위해서 스팬 길이를 budget으로 설정\n",
    "                span_length = budget\n",
    "            else:\n",
    "                continue \n",
    "        if cand_indexes[src_index][0] + span_length > l-1:  # 스팬의 마지막 토큰의 인덱스가 시퀀스 범위를 벗어나는 경우\n",
    "            continue\n",
    "        if len(cand_indexes[src_index]) > span_length:  # 처음부터 형태소를 마스킹하게 되는 경우\n",
    "            continue\n",
    "        span_token_index = cand_indexes[src_index][0]  # init span token index: src\n",
    "        while span_length:    \n",
    "            if span_length == 0:\n",
    "                break\n",
    "            if span_token_index in covered_indexes:  # 이미 마스킹 된 index 만나면 끝내고, 다음 순회 시작\n",
    "                break\n",
    "            else:  # 스팬 길이가 처음 선택 되었던 시작 토큰 인덱스가 해당되는 리스트 길이를 넘는 경우, 이후 선택되는 토큰은 wwm 위배 가능성\n",
    "                covered_indexes.add(span_token_index)\n",
    "                masked_lms.append(span_token_index)\n",
    "                span_length -= 1\n",
    "                budget -= 1\n",
    "                span_token_index += 1\n",
    "                continue\n",
    "\n",
    "    if len(covered_indexes) != len(masked_lms):\n",
    "        raise ValueError(\"Length of covered_indexes is not equal to length of masked_lms.\")\n",
    "    mask_labels = [1 if i in covered_indexes else 0 for i in range(len(input_tokens))]\n",
    "    return mask_labels\n",
    "\n",
    "def get_mask_tokens(inputs, mask_labels):\n",
    "    \"\"\" All of masking tokens are replaced by tokenizer.mask_token ([MASK]) unlikely BERT \n",
    "    \"\"\"\n",
    "    labels = inputs.clone()\n",
    "    probability_matrix = mask_labels\n",
    "    \n",
    "    special_tokens_mask = [\n",
    "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "    ]\n",
    "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "    if tokenizer.pad_token is not None:\n",
    "        padding_mask = labels.eq(tokenizer.pad_token_id)\n",
    "        probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
    "        \n",
    "    masked_indices = probability_matrix.bool()\n",
    "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "    inputs[masked_indices] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
    "    return inputs, labels\n",
    "\n",
    "def testing(batched):\n",
    "    \"\"\" Masking for MLM with whole-word tokenizing \"\"\"\n",
    "    batched = batched[\"input_ids\"]\n",
    "    input_ids = [torch.tensor(batched)]\n",
    "    padding_mask = [get_padding_mask(x) for x in input_ids]\n",
    "    padding_mask = pad_sequence(padding_mask, batch_first=True, padding_value=True)\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "    mask_labels = []\n",
    "    ref_tokens = []\n",
    "    for input_id in batched:\n",
    "        token = tokenizer._convert_id_to_token(input_id)\n",
    "        ref_tokens.append(token)\n",
    "    mask_labels.append(_whole_word_mask(ref_tokens))\n",
    "    mask_labels = [torch.tensor(x) for x in mask_labels]\n",
    "    mask_labels = pad_sequence(mask_labels, batch_first=True, padding_value=0)\n",
    "    inputs, labels = get_mask_tokens(\n",
    "        input_ids,\n",
    "        mask_labels\n",
    "    )\n",
    "    return inputs, labels\n",
    "\n",
    "inputs, labels = testing(input_tokens)\n",
    "print(inputs, end=\"\\n\\n\")\n",
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:58:02.823714Z",
     "start_time": "2024-01-02T18:58:02.808195Z"
    }
   },
   "id": "4230c7917521186e"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "'▁train'"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._convert_id_to_token(2184)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T18:56:48.425430Z",
     "start_time": "2024-01-02T18:56:48.419647Z"
    }
   },
   "id": "58c372f3fcdb49ca"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'vocab_file': 'spm.model', 'tokenizer_file': 'tokenizer.json'}"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_files_names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T08:43:19.828506155Z",
     "start_time": "2024-01-02T08:43:19.772276081Z"
    }
   },
   "id": "27ff48a2990d9c45",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-100, -100,   99,   98,   97,   96])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Experiment for ELECTRA get discriminator input\n",
    "1) flatten logit tensor and label tensor\n",
    "2) get highest logit\n",
    "3) masked select for mlm masking index\n",
    "4) get index of mlm masking index\n",
    "5) index select for discriminator input \n",
    "\"\"\"\n",
    "flat_logit = torch.tensor([99, 98, 97, 96])\n",
    "test = torch.tensor([-100, -100, 1, 2, 3, 4])\n",
    "mlm_mask_idx = torch.where(test != -100)\n",
    "\n",
    "test2 = test.clone()\n",
    "test[mlm_mask_idx] = flat_logit\n",
    "test\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:16:45.737064Z",
     "start_time": "2023-12-31T08:16:45.728843Z"
    }
   },
   "id": "cb1887885007a163"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 1, 0, 0, 0, 0])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eq(test, test2).long()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:16:50.262452Z",
     "start_time": "2023-12-31T08:16:50.259370Z"
    }
   },
   "id": "2eb6dd39dea96f1a"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-100, -100],\n        [   1,    2],\n        [   3,    4]])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = torch.tensor([1, 2])\n",
    "\n",
    "test = torch.tensor([-100, -100, 1, 2, 3, 4])\n",
    "test.view(-1, a.size(0))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T08:21:25.369486Z",
     "start_time": "2023-12-31T08:21:25.359518Z"
    }
   },
   "id": "d118d5f272197b5a"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1]"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Experiment for Huggingface Tokenizer for Building MLM Algorithm \n",
    "meaning of special token in Huggingface Tokenizer is corrspoding to [CLS], [SEP], [MASK], [PAD] ... etc\n",
    "\"\"\"\n",
    "\n",
    "text = 'I am a boy [MASK] [MASK] are a girl [PAD]'\n",
    "tokens = CFG.tokenizer(text)\n",
    "input_ids = [torch.tensor(x) for x in tokens[\"input_ids\"]]\n",
    "special_tokens_mask = CFG.tokenizer.get_special_tokens_mask(input_ids, already_has_special_tokens=True)\n",
    "special_tokens_mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T22:44:13.446160Z",
     "start_time": "2023-12-29T22:44:13.441479Z"
    }
   },
   "id": "7a31d5ebf9b1632a"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n        0.1500, 0.1500, 0.1500])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Experiment for Huggingface Tokenizer for Building MLM Algorithm \n",
    "\"\"\"\n",
    "mlm_probability = 0.15\n",
    "probability_matrix = torch.full(torch.tensor(input_ids).shape, mlm_probability)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T22:44:14.030451Z",
     "start_time": "2023-12-29T22:44:14.025780Z"
    }
   },
   "id": "487442037a73caed"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "masked_fill_() received an invalid combination of arguments - got (list, value=float), but expected one of:\n * (Tensor mask, Tensor value)\n      didn't match because some of the arguments have invalid types: (!list of [int, int, int, int, int, int, int, int, int, int, int, int]!, !value=float!)\n * (Tensor mask, Number value)\n      didn't match because some of the arguments have invalid types: (!list of [int, int, int, int, int, int, int, int, int, int, int, int]!, !value=float!)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\" Experiment for Huggingface Tokenizer for Building MLM Algorithm \u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mprobability_matrix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmasked_fill_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspecial_tokens_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m masked_indices \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mbernoulli(probability_matrix)\u001B[38;5;241m.\u001B[39mbool()\n\u001B[1;32m      5\u001B[0m masked_indices\n",
      "\u001B[0;31mTypeError\u001B[0m: masked_fill_() received an invalid combination of arguments - got (list, value=float), but expected one of:\n * (Tensor mask, Tensor value)\n      didn't match because some of the arguments have invalid types: (!list of [int, int, int, int, int, int, int, int, int, int, int, int]!, !value=float!)\n * (Tensor mask, Number value)\n      didn't match because some of the arguments have invalid types: (!list of [int, int, int, int, int, int, int, int, int, int, int, int]!, !value=float!)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Experiment for Huggingface Tokenizer for Building MLM Algorithm \n",
    "\"\"\"\n",
    "probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "masked_indices"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T22:44:15.035358Z",
     "start_time": "2023-12-29T22:44:15.033134Z"
    }
   },
   "id": "96982a7b1365afaa"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43minput_ids\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m~\u001B[39;49m\u001B[43mmasked_indices\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m100\u001B[39m\n",
      "\u001B[0;31mTypeError\u001B[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "input_ids[~masked_indices] = -100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T22:43:21.978416Z",
     "start_time": "2023-12-29T22:43:21.966992Z"
    }
   },
   "id": "5cf33010e2468866"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def hf_load_dataset(cfg: CFG) -> DatasetDict:\n",
    "    \"\"\" Load dataset from Huggingface Datasets\n",
    "    Notes:\n",
    "        This function is temporary just fit-able for Wikipedia dataset\n",
    "    References:\n",
    "        https://github.com/huggingface/datasets/blob/main/src/datasets/load.py#2247\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(cfg.hf_dataset, cfg.language)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def hf_split_dataset(cfg: CFG, dataset: Dataset) -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\" Split dataset from Huggingface Datasets with huggingface method \"train_test_split\"\n",
    "    Args:\n",
    "        cfg: configuration.CFG, needed to load split ratio, seed value\n",
    "        dataset: Huggingface Datasets object, dataset from Huggingface Datasets\n",
    "    Notes:\n",
    "        This function is temporary just fit-able for Wikipedia dataset & MLM Task\n",
    "    \"\"\"\n",
    "    dataset = dataset.train_test_split(cfg.split_ratio, seed=cfg.seed)\n",
    "    train, valid = dataset['train'], dataset['test']\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def chunking(sequences: Dict, cfg: CFG = CFG) -> List[str]:\n",
    "    \"\"\" Chunking sentence to token using pretrained tokenizer\n",
    "    Args:\n",
    "        cfg: configuration.CFG, needed to load pretrained tokenizer\n",
    "        sequences: list, sentence to chunking\n",
    "    References:\n",
    "        https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling\n",
    "    \"\"\"\n",
    "    return cfg.tokenizer([\" \".join(x) for x in sequences['text']])\n",
    "\n",
    "\n",
    "def group_texts(sequences: Dict, cfg: CFG = CFG) -> Dict:\n",
    "    \"\"\" Dealing Problem: some of data instances are longer than the maximum input length for the model,\n",
    "    This function is ONLY used to HF Dataset Object\n",
    "    1) Concatenate all texts\n",
    "    2) We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    3) customize this part to your needs\n",
    "    4) Split by chunks of max_len\n",
    "    \"\"\"\n",
    "    concatenated_sequences = {k: sum(sequences[k], []) for k in sequences.keys()}\n",
    "    total_length = len(concatenated_sequences[list(sequences.keys())[0]])\n",
    "    if total_length >= cfg.max_seq:\n",
    "        total_length = (total_length // cfg.max_seq) * cfg.max_seq\n",
    "    result = {\n",
    "        k: [t[i: i + cfg.max_seq] for i in range(0, total_length, cfg.max_seq)]\n",
    "        for k, t in concatenated_sequences.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def apply_preprocess(dataset: Dataset, function: Callable, batched: bool = True, num_proc: int = 4, remove_columns: any = None) -> Dataset:\n",
    "    \"\"\" Apply preprocessing to text data, which is using huggingface dataset method \"map()\"\n",
    "    for pretrained training (MLM, CLM)\n",
    "    Args:\n",
    "        dataset: Huggingface Datasets object, dataset from Huggingface Datasets\n",
    "        function: Callable, function that you want to apply\n",
    "        batched: bool, default True, if you want to apply function to batched data, set True\n",
    "        num_proc: int, default 4, number of process for multiprocessing\n",
    "        remove_columns: any, default None, if you want to remove some columns, set column name\n",
    "    References:\n",
    "        https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling\n",
    "    \"\"\"\n",
    "    mapped_dataset = dataset.map(\n",
    "        function,\n",
    "        batched=batched,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=remove_columns,\n",
    "    )\n",
    "    return mapped_dataset\n",
    "\n",
    "\n",
    "def load_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\" Load data_folder from csv file like as train.csv, test.csv, val.csv\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def no_char(text):\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]$\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def no_multi_spaces(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "\n",
    "def underscore_to_space(text: str):\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(source):\n",
    "    \"\"\" Remove all the special characters\n",
    "    \"\"\"\n",
    "    source = re.sub(r'\\W', ' ', str(source))\n",
    "    source = re.sub(r'^b\\s+', '', source)\n",
    "    source = source.lower()\n",
    "    return source\n",
    "\n",
    "\n",
    "def cleaning_words(text: str) -> str:\n",
    "    \"\"\" Apply all of cleaning process to text data\n",
    "    \"\"\"\n",
    "    tmp_text = underscore_to_space(text)\n",
    "    tmp_text = no_char(tmp_text)\n",
    "    tmp_text = preprocess_text(tmp_text)\n",
    "    tmp_text = no_multi_spaces(tmp_text)\n",
    "    return tmp_text\n",
    "\n",
    "\n",
    "def split_token(inputs: str):\n",
    "    \"\"\" Convert malform list to Python List Object & elementwise type casting\n",
    "    \"\"\"\n",
    "    inputs = cleaning_words(inputs)\n",
    "    tmp = inputs.split()\n",
    "    result = list(map(int, tmp))\n",
    "    return result\n",
    "\n",
    "\n",
    "def split_list(inputs: List, max_length: int) -> List[List]:\n",
    "    \"\"\" Split List into sub shorter list, which is longer than max_length\n",
    "    \"\"\"\n",
    "    result = [inputs[i:i + max_length] for i in range(0, len(inputs), max_length)]\n",
    "    return result\n",
    "\n",
    "\n",
    "def flatten_sublist(inputs: List[List], max_length: int = 512) -> List[List]:\n",
    "    \"\"\" Flatten Nested List to 1D-List \"\"\"\n",
    "    result = []\n",
    "    for instance in tqdm(inputs):\n",
    "        tmp = split_token(instance)\n",
    "        if len(tmp) > max_length:\n",
    "            tmp = split_list(tmp, max_length)\n",
    "            for i in range(len(tmp)):\n",
    "                result.append(tmp[i])\n",
    "        else:\n",
    "            result.append(tmp)\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess4tokenizer(input_ids: List, token_type_ids: List, attention_mask: List):\n",
    "    for i, inputs in tqdm(enumerate(input_ids)):\n",
    "        if inputs[0] != 1:\n",
    "            inputs.insert(0, 1)\n",
    "            token_type_ids[i].insert(0, 0)\n",
    "            attention_mask[i].insert(0, 1)\n",
    "        if inputs[-1] != 2:\n",
    "            inputs.append(2)\n",
    "            token_type_ids[i].append(0)\n",
    "            attention_mask[i].append(1)\n",
    "    return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "\n",
    "def cut_instance(input_ids: List, token_type_ids: List, attention_mask: List, min_length: int = 256):\n",
    "    n_input_ids, n_token_type_ids, n_attention_mask = [], [], []\n",
    "    for i, inputs in tqdm(enumerate(input_ids)):\n",
    "        if len(inputs) >= min_length:\n",
    "            n_input_ids.append(inputs)\n",
    "            n_token_type_ids.append(token_type_ids[i])\n",
    "            n_attention_mask.append(attention_mask[i])\n",
    "    return n_input_ids, n_token_type_ids, n_attention_mask\n",
    "\n",
    "\n",
    "def save_pkl(input_dict: Any, filename: str) -> None:\n",
    "    with open(f'{filename}.pkl', 'wb') as file:\n",
    "        pickle.dump(input_dict, file)\n",
    "\n",
    "\n",
    "def load_pkl(filepath: str) -> Any:\n",
    "    \"\"\"  Load pickle file\n",
    "    Examples:\n",
    "        filepath = './dataset_class/data_folder/train'\n",
    "    \"\"\"\n",
    "    with open(f'{filepath}.pkl', 'rb') as file:\n",
    "        output = pickle.load(file)\n",
    "    return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T09:31:18.201421201Z",
     "start_time": "2024-01-02T09:31:18.194621731Z"
    }
   },
   "id": "a4b602dc2a95d746",
   "execution_count": 99
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hf_load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m1) Load Dataset, Tokenizer\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m2) Split Dataset, preprocess dataset for MLM Task\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mhf_load_dataset\u001B[49m(CFG)\n\u001B[1;32m      6\u001B[0m _, sub_ds \u001B[38;5;241m=\u001B[39m hf_split_dataset(CFG, ds[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      7\u001B[0m train, valid \u001B[38;5;241m=\u001B[39m hf_split_dataset(CFG, sub_ds)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'hf_load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1) Load Dataset, Tokenizer\n",
    "2) Split Dataset, preprocess dataset for MLM Task\n",
    "\"\"\"\n",
    "ds = hf_load_dataset(CFG)\n",
    "_, sub_ds = hf_split_dataset(CFG, ds['train'])\n",
    "train, valid = hf_split_dataset(CFG, sub_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T05:58:37.078524Z",
     "start_time": "2023-12-28T05:58:36.985803Z"
    }
   },
   "id": "3dd1380b24923e54",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/1025250 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2864e98a41a54796be60b508bf91d442"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/256313 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f839d04ba1ae47a2ac881fcf1aef0589"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Apply preprocessing to dataset \"\"\"\n",
    "\n",
    "chunked_train = apply_preprocess(\n",
    "    train,\n",
    "    chunking,\n",
    "    remove_columns=train.column_names\n",
    ")\n",
    "\n",
    "chunked_valid = apply_preprocess(\n",
    "    valid,\n",
    "    chunking,\n",
    "    remove_columns=valid.column_names\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T08:04:26.000451213Z",
     "start_time": "2023-12-26T07:11:17.408675415Z"
    }
   },
   "id": "34e27b69bc4e33a7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=8):   0%|          | 0/1025250 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "323d0bc5d14543a7ad5b34458b4b9ecc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36miflatmap_unordered\u001B[0;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[1;32m    639\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 640\u001B[0;31m                     \u001B[0;32myield\u001B[0m \u001B[0mqueue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    641\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mEmpty\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<string>\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, *args, **kwds)\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/managers.py\u001B[0m in \u001B[0;36m_callmethod\u001B[0;34m(self, methodname, args, kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m         \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmethodname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 810\u001B[0;31m         \u001B[0mkind\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    811\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36mrecv\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    252\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_readable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 253\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    254\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_ForkingPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetbuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36m_recv_bytes\u001B[0;34m(self, maxsize)\u001B[0m\n\u001B[1;32m    416\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmaxsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 417\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    418\u001B[0m         \u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munpack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"!i\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36m_recv\u001B[0;34m(self, size, read)\u001B[0m\n\u001B[1;32m    381\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0mremaining\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 382\u001B[0;31m             \u001B[0mchunk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mremaining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    383\u001B[0m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTimeoutError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_8506/2765937612.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;34m\"\"\" Grouping text data to fit the maximum input length for the model \"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m grouped_train = apply_preprocess(\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mchunked_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mgroup_texts\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_8506/3833045376.py\u001B[0m in \u001B[0;36mapply_preprocess\u001B[0;34m(dataset, function, batched, num_proc, remove_columns)\u001B[0m\n\u001B[1;32m     65\u001B[0m         \u001B[0mhttps\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m//\u001B[0m\u001B[0mhuggingface\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mco\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mdocs\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mtransformers\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mmain\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mtasks\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mmasked_language_modeling\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m     \"\"\"\n\u001B[0;32m---> 67\u001B[0;31m     mapped_dataset = dataset.map(\n\u001B[0m\u001B[1;32m     68\u001B[0m         \u001B[0mfunction\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m         \u001B[0mbatched\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatched\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    590\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"Dataset\"\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"self\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    591\u001B[0m         \u001B[0;31m# apply actual function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 592\u001B[0;31m         \u001B[0mout\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"DatasetDict\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    593\u001B[0m         \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    594\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    555\u001B[0m         }\n\u001B[1;32m    556\u001B[0m         \u001B[0;31m# apply actual function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 557\u001B[0;31m         \u001B[0mout\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"DatasetDict\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    558\u001B[0m         \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    559\u001B[0m         \u001B[0;31m# re-apply format to the output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mmap\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3183\u001B[0m                         \u001B[0mdesc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdesc\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m\"Map\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34mf\" (num_proc={num_proc})\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3184\u001B[0m                     ) as pbar:\n\u001B[0;32m-> 3185\u001B[0;31m                         for rank, done, content in iflatmap_unordered(\n\u001B[0m\u001B[1;32m   3186\u001B[0m                             \u001B[0mpool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_map_single\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs_iterable\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwargs_per_job\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3187\u001B[0m                         ):\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36miflatmap_unordered\u001B[0;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[1;32m    652\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mpool_changed\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    653\u001B[0m                 \u001B[0;31m# we get the result in case there's an error to raise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 654\u001B[0;31m                 \u001B[0;34m[\u001B[0m\u001B[0masync_result\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0masync_result\u001B[0m \u001B[0;32min\u001B[0m \u001B[0masync_results\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    652\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mpool_changed\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    653\u001B[0m                 \u001B[0;31m# we get the result in case there's an error to raise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 654\u001B[0;31m                 \u001B[0;34m[\u001B[0m\u001B[0masync_result\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0masync_result\u001B[0m \u001B[0;32min\u001B[0m \u001B[0masync_results\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/pool.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    765\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    766\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mready\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 767\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    768\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_success\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    769\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTimeoutError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Grouping text data to fit the maximum input length for the model \"\"\"\n",
    "\n",
    "grouped_train = apply_preprocess(\n",
    "    chunked_train,\n",
    "    group_texts,\n",
    "    num_proc=8,\n",
    ")\n",
    "\n",
    "grouped_valid = apply_preprocess(\n",
    "    chunked_train,\n",
    "    group_texts,\n",
    "    num_proc=8,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T08:13:11.370459308Z",
     "start_time": "2023-12-26T08:10:05.225477717Z"
    }
   },
   "id": "23d7abc06fd50bb1",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "'trained train pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining pretrained pretraining'"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"trained, train, Pretrained, Pretraining. Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining. Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining, Pretrained, Pretraining\"\n",
    "\n",
    "cleaning_words(input_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T09:31:20.604222646Z",
     "start_time": "2024-01-02T09:31:20.600308365Z"
    }
   },
   "id": "768529dd22d186c8",
   "execution_count": 100
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f6ec65fceb9f62eb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
