{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-29T01:03:17.802723Z",
     "start_time": "2023-12-29T01:03:17.797472Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from typing import Dict, List, Tuple, Callable, Any\n",
    "from configuration import CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "71b38229a2199c41"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension specified as 0 but tensor has no dimensions",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m tokens \u001B[38;5;241m=\u001B[39m CFG\u001B[38;5;241m.\u001B[39mtokenizer(text)\n\u001B[1;32m      7\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m [torch\u001B[38;5;241m.\u001B[39mtensor(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m tokens[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[0;32m----> 8\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m \u001B[43mpad_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m special_tokens_mask \u001B[38;5;241m=\u001B[39m CFG\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mget_special_tokens_mask(input_ids, already_has_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     11\u001B[0m special_tokens_mask\n",
      "File \u001B[0;32m~/Desktop/SAMSUNG/venv/lib/python3.9/site-packages/torch/nn/utils/rnn.py:399\u001B[0m, in \u001B[0;36mpad_sequence\u001B[0;34m(sequences, batch_first, padding_value)\u001B[0m\n\u001B[1;32m    395\u001B[0m         sequences \u001B[38;5;241m=\u001B[39m sequences\u001B[38;5;241m.\u001B[39munbind(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    397\u001B[0m \u001B[38;5;66;03m# assuming trailing dimensions and type of all the Tensors\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;66;03m# in sequences are same and fetching those from sequences[0]\u001B[39;00m\n\u001B[0;32m--> 399\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_sequence\u001B[49m\u001B[43m(\u001B[49m\u001B[43msequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_first\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_value\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mIndexError\u001B[0m: Dimension specified as 0 but tensor has no dimensions"
     ]
    }
   ],
   "source": [
    "\"\"\" Experiment for Huggingface Tokenizer for Building MLM Algorithm \n",
    "meaning of special token in Huggingface Tokenizer is corrspoding to [CLS], [SEP], [MASK], [PAD] ... etc\n",
    "\"\"\"\n",
    "\n",
    "text = 'I am a boy [MASK] [MASK] are a girl [PAD]'\n",
    "tokens = CFG.tokenizer(text)\n",
    "input_ids = [torch.tensor(x) for x in tokens[\"input_ids\"]]\n",
    "input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "\n",
    "special_tokens_mask = CFG.tokenizer.get_special_tokens_mask(input_ids, already_has_special_tokens=True)\n",
    "special_tokens_mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T01:04:21.626380Z",
     "start_time": "2023-12-29T01:04:21.613425Z"
    }
   },
   "id": "7a31d5ebf9b1632a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\" Experiment for Huggingface Tokenizer for Building MLM Algorithm \u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m mlm_probability \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.15\u001B[39m\n\u001B[0;32m----> 4\u001B[0m probability_matrix \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfull(\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m, mlm_probability)\n\u001B[1;32m      5\u001B[0m probability_matrix\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\"\"\" Experiment for Huggingface Tokenizer for Building MLM Algorithm \n",
    "\"\"\"\n",
    "mlm_probability = 0.15\n",
    "probability_matrix = torch.full(input_ids.shape, mlm_probability)\n",
    "probability_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T01:02:17.478334Z",
     "start_time": "2023-12-29T01:02:17.473930Z"
    }
   },
   "id": "487442037a73caed"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def hf_load_dataset(cfg: CFG) -> DatasetDict:\n",
    "    \"\"\" Load dataset from Huggingface Datasets\n",
    "    Notes:\n",
    "        This function is temporary just fit-able for Wikipedia dataset\n",
    "    References:\n",
    "        https://github.com/huggingface/datasets/blob/main/src/datasets/load.py#2247\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(cfg.hf_dataset, cfg.language)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def hf_split_dataset(cfg: CFG, dataset: Dataset) -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\" Split dataset from Huggingface Datasets with huggingface method \"train_test_split\"\n",
    "    Args:\n",
    "        cfg: configuration.CFG, needed to load split ratio, seed value\n",
    "        dataset: Huggingface Datasets object, dataset from Huggingface Datasets\n",
    "    Notes:\n",
    "        This function is temporary just fit-able for Wikipedia dataset & MLM Task\n",
    "    \"\"\"\n",
    "    dataset = dataset.train_test_split(cfg.split_ratio, seed=cfg.seed)\n",
    "    train, valid = dataset['train'], dataset['test']\n",
    "    return train, valid\n",
    "\n",
    "\n",
    "def chunking(sequences: Dict, cfg: CFG = CFG) -> List[str]:\n",
    "    \"\"\" Chunking sentence to token using pretrained tokenizer\n",
    "    Args:\n",
    "        cfg: configuration.CFG, needed to load pretrained tokenizer\n",
    "        sequences: list, sentence to chunking\n",
    "    References:\n",
    "        https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling\n",
    "    \"\"\"\n",
    "    return cfg.tokenizer([\" \".join(x) for x in sequences['text']])\n",
    "\n",
    "\n",
    "def group_texts(sequences: Dict, cfg: CFG = CFG) -> Dict:\n",
    "    \"\"\" Dealing Problem: some of data instances are longer than the maximum input length for the model,\n",
    "    This function is ONLY used to HF Dataset Object\n",
    "    1) Concatenate all texts\n",
    "    2) We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    3) customize this part to your needs\n",
    "    4) Split by chunks of max_len\n",
    "    \"\"\"\n",
    "    concatenated_sequences = {k: sum(sequences[k], []) for k in sequences.keys()}\n",
    "    total_length = len(concatenated_sequences[list(sequences.keys())[0]])\n",
    "    if total_length >= cfg.max_seq:\n",
    "        total_length = (total_length // cfg.max_seq) * cfg.max_seq\n",
    "    result = {\n",
    "        k: [t[i: i + cfg.max_seq] for i in range(0, total_length, cfg.max_seq)]\n",
    "        for k, t in concatenated_sequences.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "def apply_preprocess(dataset: Dataset, function: Callable, batched: bool = True, num_proc: int = 4, remove_columns: any = None) -> Dataset:\n",
    "    \"\"\" Apply preprocessing to text data, which is using huggingface dataset method \"map()\"\n",
    "    for pretrained training (MLM, CLM)\n",
    "    Args:\n",
    "        dataset: Huggingface Datasets object, dataset from Huggingface Datasets\n",
    "        function: Callable, function that you want to apply\n",
    "        batched: bool, default True, if you want to apply function to batched data, set True\n",
    "        num_proc: int, default 4, number of process for multiprocessing\n",
    "        remove_columns: any, default None, if you want to remove some columns, set column name\n",
    "    References:\n",
    "        https://huggingface.co/docs/transformers/main/tasks/masked_language_modeling\n",
    "    \"\"\"\n",
    "    mapped_dataset = dataset.map(\n",
    "        function,\n",
    "        batched=batched,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=remove_columns,\n",
    "    )\n",
    "    return mapped_dataset\n",
    "\n",
    "\n",
    "def load_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\" Load data_folder from csv file like as train.csv, test.csv, val.csv\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def no_char(text):\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]$\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def no_multi_spaces(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "\n",
    "def underscore_to_space(text: str):\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(source):\n",
    "    \"\"\" Remove all the special characters\n",
    "    \"\"\"\n",
    "    source = re.sub(r'\\W', ' ', str(source))\n",
    "    source = re.sub(r'^b\\s+', '', source)\n",
    "    source = source.lower()\n",
    "    return source\n",
    "\n",
    "\n",
    "def cleaning_words(text: str) -> str:\n",
    "    \"\"\" Apply all of cleaning process to text data\n",
    "    \"\"\"\n",
    "    tmp_text = underscore_to_space(text)\n",
    "    tmp_text = no_char(tmp_text)\n",
    "    tmp_text = preprocess_text(tmp_text)\n",
    "    tmp_text = no_multi_spaces(tmp_text)\n",
    "    return tmp_text\n",
    "\n",
    "\n",
    "def split_token(inputs: str):\n",
    "    \"\"\" Convert malform list to Python List Object & elementwise type casting\n",
    "    \"\"\"\n",
    "    inputs = cleaning_words(inputs)\n",
    "    tmp = inputs.split()\n",
    "    result = list(map(int, tmp))\n",
    "    return result\n",
    "\n",
    "\n",
    "def split_list(inputs: List, max_length: int) -> List[List]:\n",
    "    \"\"\" Split List into sub shorter list, which is longer than max_length\n",
    "    \"\"\"\n",
    "    result = [inputs[i:i + max_length] for i in range(0, len(inputs), max_length)]\n",
    "    return result\n",
    "\n",
    "\n",
    "def flatten_sublist(inputs: List[List], max_length: int = 512) -> List[List]:\n",
    "    \"\"\" Flatten Nested List to 1D-List \"\"\"\n",
    "    result = []\n",
    "    for instance in tqdm(inputs):\n",
    "        tmp = split_token(instance)\n",
    "        if len(tmp) > max_length:\n",
    "            tmp = split_list(tmp, max_length)\n",
    "            for i in range(len(tmp)):\n",
    "                result.append(tmp[i])\n",
    "        else:\n",
    "            result.append(tmp)\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess4tokenizer(input_ids: List, token_type_ids: List, attention_mask: List):\n",
    "    for i, inputs in tqdm(enumerate(input_ids)):\n",
    "        if inputs[0] != 1:\n",
    "            inputs.insert(0, 1)\n",
    "            token_type_ids[i].insert(0, 0)\n",
    "            attention_mask[i].insert(0, 1)\n",
    "        if inputs[-1] != 2:\n",
    "            inputs.append(2)\n",
    "            token_type_ids[i].append(0)\n",
    "            attention_mask[i].append(1)\n",
    "    return input_ids, token_type_ids, attention_mask\n",
    "\n",
    "\n",
    "def cut_instance(input_ids: List, token_type_ids: List, attention_mask: List, min_length: int = 256):\n",
    "    n_input_ids, n_token_type_ids, n_attention_mask = [], [], []\n",
    "    for i, inputs in tqdm(enumerate(input_ids)):\n",
    "        if len(inputs) >= min_length:\n",
    "            n_input_ids.append(inputs)\n",
    "            n_token_type_ids.append(token_type_ids[i])\n",
    "            n_attention_mask.append(attention_mask[i])\n",
    "    return n_input_ids, n_token_type_ids, n_attention_mask\n",
    "\n",
    "\n",
    "def save_pkl(input_dict: Any, filename: str) -> None:\n",
    "    with open(f'{filename}.pkl', 'wb') as file:\n",
    "        pickle.dump(input_dict, file)\n",
    "\n",
    "\n",
    "def load_pkl(filepath: str) -> Any:\n",
    "    \"\"\"  Load pickle file\n",
    "    Examples:\n",
    "        filepath = './dataset_class/data_folder/train'\n",
    "    \"\"\"\n",
    "    with open(f'{filepath}.pkl', 'rb') as file:\n",
    "        output = pickle.load(file)\n",
    "    return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T06:01:36.551257Z",
     "start_time": "2023-12-28T06:01:36.548943Z"
    }
   },
   "id": "a4b602dc2a95d746",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hf_load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;124;03m1) Load Dataset, Tokenizer\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m2) Split Dataset, preprocess dataset for MLM Task\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m----> 5\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mhf_load_dataset\u001B[49m(CFG)\n\u001B[1;32m      6\u001B[0m _, sub_ds \u001B[38;5;241m=\u001B[39m hf_split_dataset(CFG, ds[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m      7\u001B[0m train, valid \u001B[38;5;241m=\u001B[39m hf_split_dataset(CFG, sub_ds)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'hf_load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1) Load Dataset, Tokenizer\n",
    "2) Split Dataset, preprocess dataset for MLM Task\n",
    "\"\"\"\n",
    "ds = hf_load_dataset(CFG)\n",
    "_, sub_ds = hf_split_dataset(CFG, ds['train'])\n",
    "train, valid = hf_split_dataset(CFG, sub_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T05:58:37.078524Z",
     "start_time": "2023-12-28T05:58:36.985803Z"
    }
   },
   "id": "3dd1380b24923e54",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/1025250 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2864e98a41a54796be60b508bf91d442"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/256313 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f839d04ba1ae47a2ac881fcf1aef0589"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Apply preprocessing to dataset \"\"\"\n",
    "\n",
    "chunked_train = apply_preprocess(\n",
    "    train,\n",
    "    chunking,\n",
    "    remove_columns=train.column_names\n",
    ")\n",
    "\n",
    "chunked_valid = apply_preprocess(\n",
    "    valid,\n",
    "    chunking,\n",
    "    remove_columns=valid.column_names\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T08:04:26.000451213Z",
     "start_time": "2023-12-26T07:11:17.408675415Z"
    }
   },
   "id": "34e27b69bc4e33a7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map (num_proc=8):   0%|          | 0/1025250 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "323d0bc5d14543a7ad5b34458b4b9ecc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TimeoutError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36miflatmap_unordered\u001B[0;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[1;32m    639\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 640\u001B[0;31m                     \u001B[0;32myield\u001B[0m \u001B[0mqueue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    641\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mEmpty\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<string>\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, *args, **kwds)\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/managers.py\u001B[0m in \u001B[0;36m_callmethod\u001B[0;34m(self, methodname, args, kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m         \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_id\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmethodname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 810\u001B[0;31m         \u001B[0mkind\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    811\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36mrecv\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    252\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_check_readable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 253\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    254\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_ForkingPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetbuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36m_recv_bytes\u001B[0;34m(self, maxsize)\u001B[0m\n\u001B[1;32m    416\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmaxsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 417\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    418\u001B[0m         \u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munpack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"!i\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/connection.py\u001B[0m in \u001B[0;36m_recv\u001B[0;34m(self, size, read)\u001B[0m\n\u001B[1;32m    381\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0mremaining\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 382\u001B[0;31m             \u001B[0mchunk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mremaining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    383\u001B[0m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mTimeoutError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_8506/2765937612.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;34m\"\"\" Grouping text data to fit the maximum input length for the model \"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m grouped_train = apply_preprocess(\n\u001B[0m\u001B[1;32m      4\u001B[0m     \u001B[0mchunked_train\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mgroup_texts\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_8506/3833045376.py\u001B[0m in \u001B[0;36mapply_preprocess\u001B[0;34m(dataset, function, batched, num_proc, remove_columns)\u001B[0m\n\u001B[1;32m     65\u001B[0m         \u001B[0mhttps\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m//\u001B[0m\u001B[0mhuggingface\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mco\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mdocs\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mtransformers\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mmain\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mtasks\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mmasked_language_modeling\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     66\u001B[0m     \"\"\"\n\u001B[0;32m---> 67\u001B[0;31m     mapped_dataset = dataset.map(\n\u001B[0m\u001B[1;32m     68\u001B[0m         \u001B[0mfunction\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m         \u001B[0mbatched\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatched\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    590\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"Dataset\"\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"self\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    591\u001B[0m         \u001B[0;31m# apply actual function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 592\u001B[0;31m         \u001B[0mout\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"DatasetDict\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    593\u001B[0m         \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    594\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mdataset\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    555\u001B[0m         }\n\u001B[1;32m    556\u001B[0m         \u001B[0;31m# apply actual function\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 557\u001B[0;31m         \u001B[0mout\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mUnion\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"DatasetDict\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    558\u001B[0m         \u001B[0mdatasets\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"Dataset\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdict\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    559\u001B[0m         \u001B[0;31m# re-apply format to the output\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001B[0m in \u001B[0;36mmap\u001B[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[1;32m   3183\u001B[0m                         \u001B[0mdesc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdesc\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m\"Map\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34mf\" (num_proc={num_proc})\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3184\u001B[0m                     ) as pbar:\n\u001B[0;32m-> 3185\u001B[0;31m                         for rank, done, content in iflatmap_unordered(\n\u001B[0m\u001B[1;32m   3186\u001B[0m                             \u001B[0mpool\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_map_single\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs_iterable\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mkwargs_per_job\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3187\u001B[0m                         ):\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36miflatmap_unordered\u001B[0;34m(pool, func, kwargs_iterable)\u001B[0m\n\u001B[1;32m    652\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mpool_changed\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    653\u001B[0m                 \u001B[0;31m# we get the result in case there's an error to raise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 654\u001B[0;31m                 \u001B[0;34m[\u001B[0m\u001B[0masync_result\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0masync_result\u001B[0m \u001B[0;32min\u001B[0m \u001B[0masync_results\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/datasets/utils/py_utils.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    652\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mpool_changed\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    653\u001B[0m                 \u001B[0;31m# we get the result in case there's an error to raise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 654\u001B[0;31m                 \u001B[0;34m[\u001B[0m\u001B[0masync_result\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.05\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0masync_result\u001B[0m \u001B[0;32min\u001B[0m \u001B[0masync_results\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/multiprocess/pool.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    765\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    766\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mready\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 767\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    768\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_success\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    769\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTimeoutError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" Grouping text data to fit the maximum input length for the model \"\"\"\n",
    "\n",
    "grouped_train = apply_preprocess(\n",
    "    chunked_train,\n",
    "    group_texts,\n",
    "    num_proc=8,\n",
    ")\n",
    "\n",
    "grouped_valid = apply_preprocess(\n",
    "    chunked_train,\n",
    "    group_texts,\n",
    "    num_proc=8,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T08:13:11.370459308Z",
     "start_time": "2023-12-26T08:10:05.225477717Z"
    }
   },
   "id": "23d7abc06fd50bb1",
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
