{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-11T09:45:52.311133Z",
     "start_time": "2024-04-11T09:45:52.307680Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc, os, warnings, re, json, pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import get_dataset_config_names, load_dataset\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers import AutoModelForQuestionAnswering,BitsAndBytesConfig\n",
    "from peft import PeftType, TaskType\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, replace_lora_weights_loftq\n",
    "from peft import PromptEncoderConfig, PromptEncoder\n",
    "\n",
    "import streamlit\n",
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from typing import Any, Dict\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"LRU_CACHE_CAPACITY\"] = \"4096\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.8, max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='mps')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" set default device to mps \"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T09:45:54.578124Z",
     "start_time": "2024-04-11T09:45:54.574547Z"
    }
   },
   "id": "e25afd3d562cf9e2",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "19c368fa996abbeb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Connect to Elastic Search Server \"\"\"\n",
    "\n",
    "try:\n",
    "    es = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    basic_auth=(\"elastic\", \"IJ_+q4tOV1vCl-Yt51U1\"),\n",
    "    ca_certs=\"/Users/qcqced/Desktop/ElasticSearch/elasticsearch-8.12.2/config/certs/http_ca.crt\"\n",
    "    )\n",
    "    print(es.ping())\n",
    "\n",
    "except ConnectionError as e:\n",
    "    print(\"Connection Error:\", e)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T09:45:56.334916Z",
     "start_time": "2024-04-11T09:45:56.324990Z"
    }
   },
   "id": "f8b2f6093f34d9c2",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" helper function for loading the dataset  \"\"\"\n",
    "\n",
    "def load_pkl(filepath: str) -> Any:\n",
    "    \"\"\" Load pickle file\n",
    "\n",
    "    Examples:\n",
    "        filepath = './dataset_class/data_folder/train.pkl'\n",
    "    \"\"\"\n",
    "    with open(f'{filepath}', 'rb') as file:\n",
    "        output = pickle.load(file)\n",
    "    return output\n",
    "\n",
    "def load_json(filepath: str) -> Any:\n",
    "    \"\"\" Load json file\n",
    "\n",
    "    Examples:\n",
    "        filepath = './dataset_class/data_folder/train.json'\n",
    "    \"\"\"\n",
    "    with open(f'{filepath}', 'r') as file:\n",
    "        output = json.load(file)\n",
    "    return output\n",
    "\n",
    "\n",
    "def load_parquet(filepath: str) -> Dict:\n",
    "    \"\"\" Load parquet file\n",
    "\n",
    "    Examples:\n",
    "        filepath = './dataset_class/data_folder/train.parquet'\n",
    "    \"\"\"\n",
    "    output = pd.read_parquet(filepath).to_dict()\n",
    "    return output\n",
    "\n",
    "\n",
    "def load_csv(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\" Load csv file\n",
    "\n",
    "    Examples:\n",
    "        filepath = './dataset_class/data_folder/train.csv'\n",
    "    \"\"\"\n",
    "    output = pd.read_csv(filepath).to_dict()\n",
    "    return output\n",
    "\n",
    "\n",
    "def load_all_types_dataset(path: str) -> Dict:\n",
    "    \"\"\" Load all pickle files from folder\n",
    "\n",
    "    Args:\n",
    "        path: path in your local directory\n",
    "\n",
    "    Examples:\n",
    "        load_all_types_dataset('./data_folder/squad2/train.json')\n",
    "        load_all_types_dataset('./data_folder/yahoo_qa/test.csv')\n",
    "        load_all_types_dataset('./data_folder/yelp_review/train_0.parquet')\n",
    "\n",
    "    All of file types are supported: json, csv, parquet, pkl\n",
    "    And Then, they are converted to dict type in python\n",
    "    \"\"\"\n",
    "    file_types = path.split('.')[-1]\n",
    "    if file_types == 'pkl': output = load_pkl(path)\n",
    "    elif file_types == 'json': output = load_json(path)\n",
    "    elif file_types == 'parquet': output = load_parquet(path)\n",
    "    elif file_types == 'csv': output = load_csv(path)\n",
    "    return output\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T09:46:19.777686Z",
     "start_time": "2024-04-11T09:46:19.773748Z"
    }
   },
   "id": "911006f5a02da422",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" Helper function for cleansing & normalizing the text \"\"\"\n",
    "\n",
    "def no_char(text):\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\^[a-zA-Z]\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+[a-zA-Z]$\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def no_multi_spaces(text):\n",
    "    return re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "\n",
    "def underscore_to_space(text: str):\n",
    "    text = text.replace(\"_\", \" \")\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess_text(source):\n",
    "    \"\"\" Remove all the special characters\n",
    "    \"\"\"\n",
    "    source = re.sub(r'\\W', ' ', str(source))\n",
    "    source = re.sub(r'^b\\s+', '', source)\n",
    "    source = source.lower()\n",
    "    return source\n",
    "\n",
    "\n",
    "def cleaning_words(text: str) -> str:\n",
    "    \"\"\" Apply all of cleaning process to text data\n",
    "    \"\"\"\n",
    "    tmp_text = underscore_to_space(text)\n",
    "    tmp_text = no_char(tmp_text)\n",
    "    tmp_text = preprocess_text(tmp_text)\n",
    "    tmp_text = no_multi_spaces(tmp_text)\n",
    "    return tmp_text"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T09:46:19.782564Z",
     "start_time": "2024-04-11T09:46:19.778549Z"
    }
   },
   "id": "d41ca3aa38624d47",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "          id  gender masterCategory subCategory articleType baseColour  \\\n0      16149   Women        Apparel     Topwear     Tshirts      White   \n1       9484  Unisex    Accessories        Bags   Backpacks      Black   \n2      19101   Women        Apparel     Topwear      Kurtas       Blue   \n3       8556     Men        Apparel     Topwear     Tshirts     Orange   \n4      16975     Men       Footwear      Sandal     Sandals       Grey   \n...      ...     ...            ...         ...         ...        ...   \n22031  19333   Women        Apparel     Topwear     Jackets  Navy Blue   \n22032  28646   Women        Apparel     Topwear     Tshirts       Grey   \n22033   2850     Men        Apparel     Topwear     Tshirts       Grey   \n22034  34219     Men        Apparel     Topwear     Tshirts  Sea Green   \n22035   3982     Men        Apparel     Topwear     Tshirts       Grey   \n\n       season    year   usage  \\\n0        Fall  2011.0  Casual   \n1        Fall  2011.0  Casual   \n2        Fall  2011.0  Ethnic   \n3        Fall  2011.0  Casual   \n4        Fall  2011.0  Casual   \n...       ...     ...     ...   \n22031    Fall  2011.0  Casual   \n22032  Summer  2012.0  Sports   \n22033  Summer  2011.0  Casual   \n22034  Summer  2013.0  Casual   \n22035  Summer  2011.0  Casual   \n\n                                      productDisplayName  \\\n0              Tokyo Talkies Women Printed White T-shirt   \n1            Puma Unisex Ferrari Replica Black Backpacks   \n2                Mother Earth Women Solid Sea Blue Kurta   \n3      Probase Men Speed Ski the Best performer Orang...   \n4                             Puma Men Rover Grey Sandal   \n...                                                  ...   \n22031  United Colors of Benetton Women Solid Navy Blu...   \n22032                     Nike Women Bleach Grey T-shirt   \n22033                 Mr.Men Men's Mr.Funny Grey T-shirt   \n22034                     Proline Sea Green Polo T-shirt   \n22035                   Mr.Men Men's Wassup Grey T-shirt   \n\n                                                   image  \n0      {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n1      {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n2      {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n3      {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n4      {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n...                                                  ...  \n22031  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n22032  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n22033  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n22034  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n22035  {'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...  \n\n[44072 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>gender</th>\n      <th>masterCategory</th>\n      <th>subCategory</th>\n      <th>articleType</th>\n      <th>baseColour</th>\n      <th>season</th>\n      <th>year</th>\n      <th>usage</th>\n      <th>productDisplayName</th>\n      <th>image</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16149</td>\n      <td>Women</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Tshirts</td>\n      <td>White</td>\n      <td>Fall</td>\n      <td>2011.0</td>\n      <td>Casual</td>\n      <td>Tokyo Talkies Women Printed White T-shirt</td>\n      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9484</td>\n      <td>Unisex</td>\n      <td>Accessories</td>\n      <td>Bags</td>\n      <td>Backpacks</td>\n      <td>Black</td>\n      <td>Fall</td>\n      <td>2011.0</td>\n      <td>Casual</td>\n      <td>Puma Unisex Ferrari Replica Black Backpacks</td>\n      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19101</td>\n      <td>Women</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Kurtas</td>\n      <td>Blue</td>\n      <td>Fall</td>\n      <td>2011.0</td>\n      <td>Ethnic</td>\n      <td>Mother Earth Women Solid Sea Blue Kurta</td>\n      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8556</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Tshirts</td>\n      <td>Orange</td>\n      <td>Fall</td>\n      <td>2011.0</td>\n      <td>Casual</td>\n      <td>Probase Men Speed Ski the Best performer Orang...</td>\n      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16975</td>\n      <td>Men</td>\n      <td>Footwear</td>\n      <td>Sandal</td>\n      <td>Sandals</td>\n      <td>Grey</td>\n      <td>Fall</td>\n      <td>2011.0</td>\n      <td>Casual</td>\n      <td>Puma Men Rover Grey Sandal</td>\n      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>22031</th>\n      <td>19333</td>\n      <td>Women</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Jackets</td>\n      <td>Navy Blue</td>\n      <td>Fall</td>\n      <td>2011.0</td>\n      <td>Casual</td>\n      <td>United Colors of Benetton Women Solid Navy Blu...</td>\n      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n    </tr>\n    <tr>\n      <th>22032</th>\n      <td>28646</td>\n      <td>Women</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Tshirts</td>\n      <td>Grey</td>\n      <td>Summer</td>\n      <td>2012.0</td>\n      <td>Sports</td>\n      <td>Nike Women Bleach Grey T-shirt</td>\n      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n    </tr>\n    <tr>\n      <th>22033</th>\n      <td>2850</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Tshirts</td>\n      <td>Grey</td>\n      <td>Summer</td>\n      <td>2011.0</td>\n      <td>Casual</td>\n      <td>Mr.Men Men's Mr.Funny Grey T-shirt</td>\n      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n    </tr>\n    <tr>\n      <th>22034</th>\n      <td>34219</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Tshirts</td>\n      <td>Sea Green</td>\n      <td>Summer</td>\n      <td>2013.0</td>\n      <td>Casual</td>\n      <td>Proline Sea Green Polo T-shirt</td>\n      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n    </tr>\n    <tr>\n      <th>22035</th>\n      <td>3982</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Tshirts</td>\n      <td>Grey</td>\n      <td>Summer</td>\n      <td>2011.0</td>\n      <td>Casual</td>\n      <td>Mr.Men Men's Wassup Grey T-shirt</td>\n      <td>{'bytes': b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x...</td>\n    </tr>\n  </tbody>\n</table>\n<p>44072 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Merge Two DataFrames \"\"\"\n",
    "\n",
    "df1 = pd.DataFrame.from_dict(load_all_types_dataset('./product_search_tutorial/myntra1.parquet'))\n",
    "df2 = pd.DataFrame.from_dict(load_all_types_dataset('./product_search_tutorial/myntra2.parquet'))\n",
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T09:56:17.677111Z",
     "start_time": "2024-04-11T09:56:16.866464Z"
    }
   },
   "id": "a4e1c8b1579b1214",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" Dict Object for Index Mapping in elasticsearch \"\"\"\n",
    "indexName = \"all_products\"\n",
    "\n",
    "indexMapping = {\n",
    "    \"properties\":{\n",
    "        \"ProductID\":{\n",
    "            \"type\":\"long\"\n",
    "        },\n",
    "        \"ProductName\":{\n",
    "            \"type\":\"text\"\n",
    "        },\n",
    "        \"ProductBrand\":{\n",
    "            \"type\":\"text\"\n",
    "        },\n",
    "        \"Gender\":{\n",
    "            \"type\":\"text\"\n",
    "        },\n",
    "        \"Price (INR)\":{\n",
    "            \"type\":\"long\"\n",
    "        },\n",
    "        \"NumImages\":{\n",
    "            \"type\":\"long\"\n",
    "        },\n",
    "        \"Description\":{\n",
    "            \"type\":\"text\"\n",
    "        },\n",
    "        \"PrimaryColor\":{\n",
    "            \"type\":\"text\"\n",
    "        },\n",
    "        \"DescriptionVector\":{\n",
    "            \"type\":\"dense_vector\",\n",
    "            \"dims\": 768,\n",
    "            \"index\":True,\n",
    "            \"similarity\": \"cosine\"\n",
    "        }\n",
    "\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T09:59:57.851266Z",
     "start_time": "2024-04-11T09:59:57.846990Z"
    }
   },
   "id": "58ae5247b487be67",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "620d14e9e4fb45d3980a602f80d5c3ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6df96cce214a44c6bb62a19aacc0358c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb719aed95584d64ac6e8f5511602e59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fe73218b95a45319e2aa7c39c9bab20"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f7592d4f4354c8698e9da100e4166ca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05115a65dbfd4ce4b6c3954e3ea0f9de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Fine-Tune with product data\n",
    "1) load pretrained tokenizer, model\n",
    "\"\"\"\n",
    "max_len = 512\n",
    "model_name = 'sentence-transformers/paraphrase-MiniLM-L6-v2'   # UPKLAB 거기꺼 안쓰네\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model_cfg = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    config=model_cfg\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T09:46:19.766297Z",
     "start_time": "2024-04-11T09:46:06.710661Z"
    }
   },
   "id": "4fcba2985295a8a5",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def tokenizing(text: str, padding: bool or str = 'max_length') -> Any:\n",
    "    \"\"\" Preprocess text for LLM Input, for common batch system\n",
    "\n",
    "    Args:\n",
    "        cfg: configuration.CFG, needed to load tokenizer from Huggingface AutoTokenizer\n",
    "        text: text from dataframe or any other dataset, please pass str type\n",
    "        padding: padding options, default 'max_length', if you want use smart batching, init this param to False\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=max_len,\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False,  # later, we will add ourselves\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v)\n",
    "    return inputs\n",
    "\n",
    "class ProductSearchDataset(Dataset):\n",
    "    \"\"\" Custom Dataset for Pretraining Task in NLP, such as MLM, CLM, ... etc\n",
    "    \"\"\"\n",
    "    def __init__(self, inputs: pd.DataFrame) -> None:\n",
    "        self.inputs = inputs\n",
    "        self.ids = self.inputs.id\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item: int) -> Dict[str, Tensor]:\n",
    "        prompts = '' + tokenizer.cls_token\n",
    "        for col in self.inputs.columns:\n",
    "            prompts += f\"{self.inputs[col][item]}\" + tokenizer.sep_token\n",
    "        \n",
    "        batch_prompt = tokenizing(prompts, padding=False)\n",
    "        for k, v in batch_prompt.items():\n",
    "            batch_prompt[k] = torch.as_tensor(v)\n",
    "        return batch_prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2935573813145ba5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" Model class for Fine-Tuning Pretrained Model for Semantic Search \"\"\"\n",
    "\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    \"\"\" Module for pure mean pooling \"\"\"\n",
    "    def __init__(self, auto_cfg):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(last_hidden_state: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)  # if lower than threshold, replace value to threshold (parameter min)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "\n",
    "class SemanticSearchModel(nn.Module):\n",
    "    \"\"\" Model for Semantic Search\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SemanticSearchModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.mean_pooling = MeanPooling(model_cfg)\n",
    "    \n",
    "    def forward(self, inputs: Dict[str, Tensor]) -> Tensor:\n",
    "        hidden_states = self.model(**inputs).last_hidden_state\n",
    "        h = self.mean_pooling(hidden_states, inputs['attention_mask'])\n",
    "        return h"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T09:48:07.015570Z",
     "start_time": "2024-04-11T09:48:07.008396Z"
    }
   },
   "id": "58b4c90981c338ff",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" Just project the text to model hidden state dimension \"\"\"\n",
    "\n",
    "def search(input_query: str):\n",
    "    h = model.encode(input_query)\n",
    "    query = {\n",
    "        \"field\": \"DescriptionVector\",\n",
    "        \"query_vector\": h,\n",
    "        \"k\": 30,\n",
    "        \"num_candidates\": 500\n",
    "    }\n",
    "    \n",
    "    candidates = es.knn_search(\n",
    "        index=\"all_products\",\n",
    "        knn=query,\n",
    "        source=['\"ProductName', 'Description'] \n",
    "    )\n",
    "    results = candidates['hits']['hits']\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    streamlit.title(\"Search Fashion Products\")\n",
    "    query = streamlit.text_input(\"Enter your query here\")\n",
    "    if streamlit.button(\"Search\"):\n",
    "        if query:\n",
    "            results = search(query)\n",
    "            streamlit.subheader(\"Search Results\")\n",
    "            for result in results:\n",
    "                with streamlit.container():\n",
    "                    if '_source' in result:\n",
    "                        try:\n",
    "                            streamlit.header(f\"{result['_source']['ProductName']}\")\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                        try:\n",
    "                            streamlit.write(result['_source']['Description'])\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                    \n",
    "                    streamlit.divider()\n",
    "main()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T09:55:55.796024Z",
     "start_time": "2024-04-11T09:55:55.789785Z"
    }
   },
   "id": "362442d49425bc28",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e2d21772a0e07ecb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
