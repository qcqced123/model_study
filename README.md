# ğŸ”¬ ML/DL Experiment Application


### ğŸ—‚ï¸ Application Structure
This Project implement with Pytorch. The overview structure is as follows.
```
Experiment Application
â”‚
â”œâ”€â”€ experiment
â”‚	â”œâ”€â”€ losses
â”‚	â”‚	â”œâ”€â”€ loss.py
â”‚	â”‚	â””â”€â”€ distance_metric.py  # for metric learning
â”‚	â”œâ”€â”€ metrics
â”‚	â”‚	â””â”€â”€ metric.py
â”‚	â”œâ”€â”€ models
â”‚	â”‚	â”œâ”€â”€ attention
â”‚	â”‚	â”‚    â”œâ”€â”€ transformer.py
â”‚	â”‚	â”‚    â”œâ”€â”€ vit.py  # vision transformer
â”‚	â”‚	â”‚    â”œâ”€â”€ deberta.py
â”‚	â”‚	â”‚    â”œâ”€â”€ electra.py
â”‚	â”‚	â”‚    â””â”€â”€ gpt2.py
â”‚	â”‚	â”œâ”€â”€ recurrent
â”‚	â”‚	â”‚    â”œâ”€â”€ rnn.py
â”‚	â”‚	â”‚    â”œâ”€â”€ lstm.py  
â”‚	â”‚	â”‚    â””â”€â”€ gru.py
â”‚	â”‚	â”œâ”€â”€ convolution
â”‚	â”‚	â””â”€â”€ probability
â”‚	â”œâ”€â”€ pooling
â”‚	â”‚
â”‚	â”œâ”€â”€ tokenizer
â”‚	â”‚
â”‚	â””â”€â”€ tuner
â”‚		â”œâ”€â”€ mlm.py
â”‚		â”œâ”€â”€ clm.py
â”‚		â”œâ”€â”€ rtd.py
â”‚		â”œâ”€â”€ sbo.py
â”‚	    â””â”€â”€ p_tuning.py
â”‚
â”œâ”€â”€ dataset_class
â”‚	â”œâ”€â”€ data_folder  # input your dataset
â”‚	â”œâ”€â”€ dataclass.py
â”‚   â””â”€â”€ preprocessing.py
â”‚  
â”œâ”€â”€ model
â”‚	â”œâ”€â”€ abstract_task.py
â”‚   â”œâ”€â”€ model.py
â”‚   â””â”€â”€ model_utils.py
â”‚
â”œâ”€â”€ trainer
â”‚   â”œâ”€â”€ train_loop.py
â”‚   â”œâ”€â”€ trainer.py
â”‚   â””â”€â”€ trainer_utils.py
â”‚
â””â”€â”€ Experiment 
    â””â”€â”€  í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ê³¼ì • ë° ê²°ê³¼ ì†Œê°œ**
```

### Attention
- **[Transformer] Attention Is All You Need (ì™„ë£Œ, [ë¦¬ë·°](https://qcqced123.github.io/nlp/transformer))**

- [Longformer] Longformer: The Long-Document Transformer (ì˜ˆì •)

- [Reformer] Reformer: The Efficient Transformer (ì˜ˆì •)

- [ELECTRA] Pre-training Text Encoders as Discriminators Rather Than Generators (ì˜ˆì •)

- [SpanBERT] SpanBERT: Improving Pre-training by Representing and Predicting Spans (ì˜ˆì •)

- [DistilBERT] DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter

- [Sentence-BERT] Sentence Embeddings using Siamese BERT-Networks

- **[DeBERTa] DeBERTa: Decoding-Enhanced BERT with Disentangled-Attention (ì™„ë£Œ, [ë¦¬ë·°](https://qcqced123.github.io/nlp/deberta))**
- [GPT2] Language Models are Unsupervised Multitask Learners (ì˜ˆì •)

- **[ViT] An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale (ì™„ë£Œ, [ë¦¬ë·°](https://qcqced123.github.io/cv/vit))**

- [SwinTransformer] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows(ì—ì •)

- [CLIP] Learning Transferable Visual Models From Natural Language Supervision(ì—ì •)
 
- [BLIP] BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (ì—ì •)


### Recurrent

- **[RNN] Recurrent Neural Network (ì™„ë£Œ)**
- **[GRU] Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling (ì™„ë£Œ)**
- **[LSTM] Long Short-Term Memory (ì™„ë£Œ)**
- [ELMO] Deep contextualized word representations (ì˜ˆì •)

### Convolution 

- [ConvNext] A ConvNext for the 2020s (ì—ì •)

- [CoAtNet] Marrying Convolution and Attention for All Data Sizes (ì—ì •)
