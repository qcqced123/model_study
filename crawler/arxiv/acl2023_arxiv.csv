entry_id,updated,published,title,authors,summary,comment,journal_ref,doi,primary_category,categories,links,pdf_url,_raw
http://arxiv.org/abs/2305.04533v1,2023-05-08 08:09:00+00:00,2023-05-08 08:09:00+00:00,Prompted LLMs as Chatbot Modules for Long Open-domain Conversation,"[arxiv.Result.Author('Gibbeum Lee'), arxiv.Result.Author('Volker Hartmann'), arxiv.Result.Author('Jongho Park'), arxiv.Result.Author('Dimitris Papailiopoulos'), arxiv.Result.Author('Kangwook Lee')]","In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for
creating high-quality conversational agents without the need for fine-tuning.
Our method utilizes pre-trained large language models (LLMs) as individual
modules for long-term consistency and flexibility, by using techniques such as
few-shot prompting, chain-of-thought (CoT), and external memory. Our human
evaluation results show that MPC is on par with fine-tuned chatbot models in
open-domain conversations, making it an effective solution for creating
consistent and engaging chatbots.","Accepted to the Findings of ACL2023. The camera-ready version with
  additional experimental results will be uploaded",,10.18653/v1/2023.findings-acl.277,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://dx.doi.org/10.18653/v1/2023.findings-acl.277', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2305.04533v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.04533v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.04533v1,"{'id': 'http://arxiv.org/abs/2305.04533v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.04533v1', 'updated': '2023-05-08T08:09:00Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=8, tm_min=9, tm_sec=0, tm_wday=0, tm_yday=128, tm_isdst=0), 'published': '2023-05-08T08:09:00Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=8, tm_min=9, tm_sec=0, tm_wday=0, tm_yday=128, tm_isdst=0), 'title': 'Prompted LLMs as Chatbot Modules for Long Open-domain Conversation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Prompted LLMs as Chatbot Modules for Long Open-domain Conversation'}, 'summary': 'In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for\ncreating high-quality conversational agents without the need for fine-tuning.\nOur method utilizes pre-trained large language models (LLMs) as individual\nmodules for long-term consistency and flexibility, by using techniques such as\nfew-shot prompting, chain-of-thought (CoT), and external memory. Our human\nevaluation results show that MPC is on par with fine-tuned chatbot models in\nopen-domain conversations, making it an effective solution for creating\nconsistent and engaging chatbots.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for\ncreating high-quality conversational agents without the need for fine-tuning.\nOur method utilizes pre-trained large language models (LLMs) as individual\nmodules for long-term consistency and flexibility, by using techniques such as\nfew-shot prompting, chain-of-thought (CoT), and external memory. Our human\nevaluation results show that MPC is on par with fine-tuned chatbot models in\nopen-domain conversations, making it an effective solution for creating\nconsistent and engaging chatbots.'}, 'authors': [{'name': 'Gibbeum Lee'}, {'name': 'Volker Hartmann'}, {'name': 'Jongho Park'}, {'name': 'Dimitris Papailiopoulos'}, {'name': 'Kangwook Lee'}], 'author_detail': {'name': 'Kangwook Lee'}, 'author': 'Kangwook Lee', 'arxiv_doi': '10.18653/v1/2023.findings-acl.277', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.18653/v1/2023.findings-acl.277', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2305.04533v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.04533v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': 'Accepted to the Findings of ACL2023. The camera-ready version with\n  additional experimental results will be uploaded', 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.16896v1,2023-05-26 13:00:58+00:00,2023-05-26 13:00:58+00:00,MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting,"[arxiv.Result.Author('Tatsuro Inaba'), arxiv.Result.Author('Hirokazu Kiyomaru'), arxiv.Result.Author('Fei Cheng'), arxiv.Result.Author('Sadao Kurohashi')]","Large language models (LLMs) have achieved impressive performance on various
reasoning tasks. To further improve the performance, we propose MultiTool-CoT,
a novel framework that leverages chain-of-thought (CoT) prompting to
incorporate multiple external tools, such as a calculator and a knowledge
retriever, during the reasoning process. We apply MultiTool-CoT to the Task 2
dataset of NumGLUE, which requires both numerical reasoning and domain-specific
knowledge. The experiments show that our method significantly outperforms
strong baselines and achieves state-of-the-art performance.","ACL2023. Our code is available at
  https://github.com/InabaTatsuro/MultiTool-CoT",,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.16896v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.16896v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.16896v1,"{'id': 'http://arxiv.org/abs/2305.16896v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.16896v1', 'updated': '2023-05-26T13:00:58Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=13, tm_min=0, tm_sec=58, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2023-05-26T13:00:58Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=13, tm_min=0, tm_sec=58, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of\n  Thought Prompting', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of\n  Thought Prompting'}, 'summary': 'Large language models (LLMs) have achieved impressive performance on various\nreasoning tasks. To further improve the performance, we propose MultiTool-CoT,\na novel framework that leverages chain-of-thought (CoT) prompting to\nincorporate multiple external tools, such as a calculator and a knowledge\nretriever, during the reasoning process. We apply MultiTool-CoT to the Task 2\ndataset of NumGLUE, which requires both numerical reasoning and domain-specific\nknowledge. The experiments show that our method significantly outperforms\nstrong baselines and achieves state-of-the-art performance.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Large language models (LLMs) have achieved impressive performance on various\nreasoning tasks. To further improve the performance, we propose MultiTool-CoT,\na novel framework that leverages chain-of-thought (CoT) prompting to\nincorporate multiple external tools, such as a calculator and a knowledge\nretriever, during the reasoning process. We apply MultiTool-CoT to the Task 2\ndataset of NumGLUE, which requires both numerical reasoning and domain-specific\nknowledge. The experiments show that our method significantly outperforms\nstrong baselines and achieves state-of-the-art performance.'}, 'authors': [{'name': 'Tatsuro Inaba'}, {'name': 'Hirokazu Kiyomaru'}, {'name': 'Fei Cheng'}, {'name': 'Sadao Kurohashi'}], 'author_detail': {'name': 'Sadao Kurohashi'}, 'author': 'Sadao Kurohashi', 'arxiv_comment': 'ACL2023. Our code is available at\n  https://github.com/InabaTatsuro/MultiTool-CoT', 'links': [{'href': 'http://arxiv.org/abs/2305.16896v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.16896v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.17750v1,2023-05-28 15:14:54+00:00,2023-05-28 15:14:54+00:00,Reliable and Interpretable Drift Detection in Streams of Short Texts,"[arxiv.Result.Author('Ella Rabinovich'), arxiv.Result.Author('Matan Vetzler'), arxiv.Result.Author('Samuel Ackerman'), arxiv.Result.Author('Ateret Anaby-Tavor')]","Data drift is the change in model input data that is one of the key factors
leading to machine learning models performance degradation over time.
Monitoring drift helps detecting these issues and preventing their harmful
consequences. Meaningful drift interpretation is a fundamental step towards
effective re-training of the model. In this study we propose an end-to-end
framework for reliable model-agnostic change-point detection and interpretation
in large task-oriented dialog systems, proven effective in multiple customer
deployments. We evaluate our approach and demonstrate its benefits with a novel
variant of intent classification training dataset, simulating customer requests
to a dialog system. We make the data publicly available.",ACL2023 industry track (9 pages),,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.17750v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.17750v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.17750v1,"{'id': 'http://arxiv.org/abs/2305.17750v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.17750v1', 'updated': '2023-05-28T15:14:54Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=28, tm_hour=15, tm_min=14, tm_sec=54, tm_wday=6, tm_yday=148, tm_isdst=0), 'published': '2023-05-28T15:14:54Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=28, tm_hour=15, tm_min=14, tm_sec=54, tm_wday=6, tm_yday=148, tm_isdst=0), 'title': 'Reliable and Interpretable Drift Detection in Streams of Short Texts', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Reliable and Interpretable Drift Detection in Streams of Short Texts'}, 'summary': 'Data drift is the change in model input data that is one of the key factors\nleading to machine learning models performance degradation over time.\nMonitoring drift helps detecting these issues and preventing their harmful\nconsequences. Meaningful drift interpretation is a fundamental step towards\neffective re-training of the model. In this study we propose an end-to-end\nframework for reliable model-agnostic change-point detection and interpretation\nin large task-oriented dialog systems, proven effective in multiple customer\ndeployments. We evaluate our approach and demonstrate its benefits with a novel\nvariant of intent classification training dataset, simulating customer requests\nto a dialog system. We make the data publicly available.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Data drift is the change in model input data that is one of the key factors\nleading to machine learning models performance degradation over time.\nMonitoring drift helps detecting these issues and preventing their harmful\nconsequences. Meaningful drift interpretation is a fundamental step towards\neffective re-training of the model. In this study we propose an end-to-end\nframework for reliable model-agnostic change-point detection and interpretation\nin large task-oriented dialog systems, proven effective in multiple customer\ndeployments. We evaluate our approach and demonstrate its benefits with a novel\nvariant of intent classification training dataset, simulating customer requests\nto a dialog system. We make the data publicly available.'}, 'authors': [{'name': 'Ella Rabinovich'}, {'name': 'Matan Vetzler'}, {'name': 'Samuel Ackerman'}, {'name': 'Ateret Anaby-Tavor'}], 'author_detail': {'name': 'Ateret Anaby-Tavor'}, 'author': 'Ateret Anaby-Tavor', 'arxiv_comment': 'ACL2023 industry track (9 pages)', 'links': [{'href': 'http://arxiv.org/abs/2305.17750v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.17750v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.05561v1,2023-06-08 21:06:19+00:00,2023-06-08 21:06:19+00:00,Privacy- and Utility-Preserving NLP with Anonymized Data: A case study of Pseudonymization,"[arxiv.Result.Author('Oleksandr Yermilov'), arxiv.Result.Author('Vipul Raheja'), arxiv.Result.Author('Artem Chernodub')]","This work investigates the effectiveness of different pseudonymization
techniques, ranging from rule-based substitutions to using pre-trained Large
Language Models (LLMs), on a variety of datasets and models used for two widely
used NLP tasks: text classification and summarization. Our work provides
crucial insights into the gaps between original and anonymized data (focusing
on the pseudonymization technique) and model quality and fosters future
research into higher-quality anonymization techniques to better balance the
trade-offs between data protection and utility preservation. We make our code,
pseudonymized datasets, and downstream models publicly available",10 pages. Accepted for TrustNLP workshop at ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.05561v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.05561v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.05561v1,"{'id': 'http://arxiv.org/abs/2306.05561v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.05561v1', 'updated': '2023-06-08T21:06:19Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=21, tm_min=6, tm_sec=19, tm_wday=3, tm_yday=159, tm_isdst=0), 'published': '2023-06-08T21:06:19Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=21, tm_min=6, tm_sec=19, tm_wday=3, tm_yday=159, tm_isdst=0), 'title': 'Privacy- and Utility-Preserving NLP with Anonymized Data: A case study\n  of Pseudonymization', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Privacy- and Utility-Preserving NLP with Anonymized Data: A case study\n  of Pseudonymization'}, 'summary': 'This work investigates the effectiveness of different pseudonymization\ntechniques, ranging from rule-based substitutions to using pre-trained Large\nLanguage Models (LLMs), on a variety of datasets and models used for two widely\nused NLP tasks: text classification and summarization. Our work provides\ncrucial insights into the gaps between original and anonymized data (focusing\non the pseudonymization technique) and model quality and fosters future\nresearch into higher-quality anonymization techniques to better balance the\ntrade-offs between data protection and utility preservation. We make our code,\npseudonymized datasets, and downstream models publicly available', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'This work investigates the effectiveness of different pseudonymization\ntechniques, ranging from rule-based substitutions to using pre-trained Large\nLanguage Models (LLMs), on a variety of datasets and models used for two widely\nused NLP tasks: text classification and summarization. Our work provides\ncrucial insights into the gaps between original and anonymized data (focusing\non the pseudonymization technique) and model quality and fosters future\nresearch into higher-quality anonymization techniques to better balance the\ntrade-offs between data protection and utility preservation. We make our code,\npseudonymized datasets, and downstream models publicly available'}, 'authors': [{'name': 'Oleksandr Yermilov'}, {'name': 'Vipul Raheja'}, {'name': 'Artem Chernodub'}], 'author_detail': {'name': 'Artem Chernodub'}, 'author': 'Artem Chernodub', 'arxiv_comment': '10 pages. Accepted for TrustNLP workshop at ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.05561v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.05561v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.10098v1,2023-06-16 17:49:34+00:00,2023-06-16 17:49:34+00:00,Differentiable Instruction Optimization for Cross-Task Generalization,"[arxiv.Result.Author('Masaru Isonuma'), arxiv.Result.Author('Junichiro Mori'), arxiv.Result.Author('Ichiro Sakata')]","Instruction tuning has been attracting much attention to achieve
generalization ability across a wide variety of tasks. Although various types
of instructions have been manually created for instruction tuning, it is still
unclear what kind of instruction is optimal to obtain cross-task generalization
ability. This work presents instruction optimization, which optimizes training
instructions with respect to generalization ability. Rather than manually
tuning instructions, we introduce learnable instructions and optimize them with
gradient descent by leveraging bilevel optimization. Experimental results show
that the learned instruction enhances the diversity of instructions and
improves the generalization ability compared to using only manually created
instructions.","14pages, 6 figures, accepted for Findings of ACL2023",,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2306.10098v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.10098v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.10098v1,"{'id': 'http://arxiv.org/abs/2306.10098v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.10098v1', 'updated': '2023-06-16T17:49:34Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=16, tm_hour=17, tm_min=49, tm_sec=34, tm_wday=4, tm_yday=167, tm_isdst=0), 'published': '2023-06-16T17:49:34Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=16, tm_hour=17, tm_min=49, tm_sec=34, tm_wday=4, tm_yday=167, tm_isdst=0), 'title': 'Differentiable Instruction Optimization for Cross-Task Generalization', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Differentiable Instruction Optimization for Cross-Task Generalization'}, 'summary': 'Instruction tuning has been attracting much attention to achieve\ngeneralization ability across a wide variety of tasks. Although various types\nof instructions have been manually created for instruction tuning, it is still\nunclear what kind of instruction is optimal to obtain cross-task generalization\nability. This work presents instruction optimization, which optimizes training\ninstructions with respect to generalization ability. Rather than manually\ntuning instructions, we introduce learnable instructions and optimize them with\ngradient descent by leveraging bilevel optimization. Experimental results show\nthat the learned instruction enhances the diversity of instructions and\nimproves the generalization ability compared to using only manually created\ninstructions.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Instruction tuning has been attracting much attention to achieve\ngeneralization ability across a wide variety of tasks. Although various types\nof instructions have been manually created for instruction tuning, it is still\nunclear what kind of instruction is optimal to obtain cross-task generalization\nability. This work presents instruction optimization, which optimizes training\ninstructions with respect to generalization ability. Rather than manually\ntuning instructions, we introduce learnable instructions and optimize them with\ngradient descent by leveraging bilevel optimization. Experimental results show\nthat the learned instruction enhances the diversity of instructions and\nimproves the generalization ability compared to using only manually created\ninstructions.'}, 'authors': [{'name': 'Masaru Isonuma'}, {'name': 'Junichiro Mori'}, {'name': 'Ichiro Sakata'}], 'author_detail': {'name': 'Ichiro Sakata'}, 'author': 'Ichiro Sakata', 'arxiv_comment': '14pages, 6 figures, accepted for Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.10098v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.10098v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2109.07446v2,2023-06-27 17:10:50+00:00,2021-09-15 17:29:30+00:00,"When Does Translation Require Context? A Data-driven, Multilingual Exploration","[arxiv.Result.Author('Patrick Fernandes'), arxiv.Result.Author('Kayo Yin'), arxiv.Result.Author('Emmy Liu'), arxiv.Result.Author('André F. T. Martins'), arxiv.Result.Author('Graham Neubig')]","Although proper handling of discourse significantly contributes to the
quality of machine translation (MT), these improvements are not adequately
measured in common translation quality metrics. Recent works in context-aware
MT attempt to target a small set of discourse phenomena during evaluation,
however not in a fully systematic way. In this paper, we develop the
Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that
identify and evaluate model performance on discourse phenomena in any given
dataset. The choice of phenomena is inspired by a novel methodology to
systematically identify translations requiring context. We confirm the
difficulty of previously studied phenomena while uncovering others that were
previously unaddressed. We find that common context-aware MT models make only
marginal improvements over context-agnostic models, which suggests these models
do not handle these ambiguities effectively. We release code and data for 14
language pairs to encourage the MT community to focus on accurately capturing
discourse phenomena.",Accepted at ACL2023,,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2109.07446v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2109.07446v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2109.07446v2,"{'id': 'http://arxiv.org/abs/2109.07446v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2109.07446v2', 'updated': '2023-06-27T17:10:50Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=27, tm_hour=17, tm_min=10, tm_sec=50, tm_wday=1, tm_yday=178, tm_isdst=0), 'published': '2021-09-15T17:29:30Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=9, tm_mday=15, tm_hour=17, tm_min=29, tm_sec=30, tm_wday=2, tm_yday=258, tm_isdst=0), 'title': 'When Does Translation Require Context? A Data-driven, Multilingual\n  Exploration', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'When Does Translation Require Context? A Data-driven, Multilingual\n  Exploration'}, 'summary': 'Although proper handling of discourse significantly contributes to the\nquality of machine translation (MT), these improvements are not adequately\nmeasured in common translation quality metrics. Recent works in context-aware\nMT attempt to target a small set of discourse phenomena during evaluation,\nhowever not in a fully systematic way. In this paper, we develop the\nMultilingual Discourse-Aware (MuDA) benchmark, a series of taggers that\nidentify and evaluate model performance on discourse phenomena in any given\ndataset. The choice of phenomena is inspired by a novel methodology to\nsystematically identify translations requiring context. We confirm the\ndifficulty of previously studied phenomena while uncovering others that were\npreviously unaddressed. We find that common context-aware MT models make only\nmarginal improvements over context-agnostic models, which suggests these models\ndo not handle these ambiguities effectively. We release code and data for 14\nlanguage pairs to encourage the MT community to focus on accurately capturing\ndiscourse phenomena.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Although proper handling of discourse significantly contributes to the\nquality of machine translation (MT), these improvements are not adequately\nmeasured in common translation quality metrics. Recent works in context-aware\nMT attempt to target a small set of discourse phenomena during evaluation,\nhowever not in a fully systematic way. In this paper, we develop the\nMultilingual Discourse-Aware (MuDA) benchmark, a series of taggers that\nidentify and evaluate model performance on discourse phenomena in any given\ndataset. The choice of phenomena is inspired by a novel methodology to\nsystematically identify translations requiring context. We confirm the\ndifficulty of previously studied phenomena while uncovering others that were\npreviously unaddressed. We find that common context-aware MT models make only\nmarginal improvements over context-agnostic models, which suggests these models\ndo not handle these ambiguities effectively. We release code and data for 14\nlanguage pairs to encourage the MT community to focus on accurately capturing\ndiscourse phenomena.'}, 'authors': [{'name': 'Patrick Fernandes'}, {'name': 'Kayo Yin'}, {'name': 'Emmy Liu'}, {'name': 'André F. T. Martins'}, {'name': 'Graham Neubig'}], 'author_detail': {'name': 'Graham Neubig'}, 'author': 'Graham Neubig', 'arxiv_comment': 'Accepted at ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2109.07446v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.07446v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2201.12926v2,2023-07-05 17:59:33+00:00,2022-01-30 21:44:46+00:00,Compositionality as Lexical Symmetry,"[arxiv.Result.Author('Ekin Akyürek'), arxiv.Result.Author('Jacob Andreas')]","In tasks like semantic parsing, instruction following, and question
answering, standard deep networks fail to generalize compositionally from small
datasets. Many existing approaches overcome this limitation with model
architectures that enforce a compositional process of sentence interpretation.
In this paper, we present a domain-general and model-agnostic formulation of
compositionality as a constraint on symmetries of data distributions rather
than models. Informally, we prove that whenever a task can be solved by a
compositional model, there is a corresponding data augmentation scheme -- a
procedure for transforming examples into other well formed examples -- that
imparts compositional inductive bias on any model trained to solve the same
task. We describe a procedure called LEXSYM that discovers these
transformations automatically, then applies them to training data for ordinary
neural sequence models. Unlike existing compositional data augmentation
procedures, LEXSYM can be deployed agnostically across text, structured data,
and even images. It matches or surpasses state-of-the-art, task-specific models
on COGS semantic parsing, SCAN and ALCHEMY instruction following, and
CLEVR-COGENT visual question answering datasets.",ACL2023 Final Version,,,cs.CL,"['cs.CL', 'cs.CV', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2201.12926v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2201.12926v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2201.12926v2,"{'id': 'http://arxiv.org/abs/2201.12926v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2201.12926v2', 'updated': '2023-07-05T17:59:33Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=5, tm_hour=17, tm_min=59, tm_sec=33, tm_wday=2, tm_yday=186, tm_isdst=0), 'published': '2022-01-30T21:44:46Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=1, tm_mday=30, tm_hour=21, tm_min=44, tm_sec=46, tm_wday=6, tm_yday=30, tm_isdst=0), 'title': 'Compositionality as Lexical Symmetry', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Compositionality as Lexical Symmetry'}, 'summary': 'In tasks like semantic parsing, instruction following, and question\nanswering, standard deep networks fail to generalize compositionally from small\ndatasets. Many existing approaches overcome this limitation with model\narchitectures that enforce a compositional process of sentence interpretation.\nIn this paper, we present a domain-general and model-agnostic formulation of\ncompositionality as a constraint on symmetries of data distributions rather\nthan models. Informally, we prove that whenever a task can be solved by a\ncompositional model, there is a corresponding data augmentation scheme -- a\nprocedure for transforming examples into other well formed examples -- that\nimparts compositional inductive bias on any model trained to solve the same\ntask. We describe a procedure called LEXSYM that discovers these\ntransformations automatically, then applies them to training data for ordinary\nneural sequence models. Unlike existing compositional data augmentation\nprocedures, LEXSYM can be deployed agnostically across text, structured data,\nand even images. It matches or surpasses state-of-the-art, task-specific models\non COGS semantic parsing, SCAN and ALCHEMY instruction following, and\nCLEVR-COGENT visual question answering datasets.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'In tasks like semantic parsing, instruction following, and question\nanswering, standard deep networks fail to generalize compositionally from small\ndatasets. Many existing approaches overcome this limitation with model\narchitectures that enforce a compositional process of sentence interpretation.\nIn this paper, we present a domain-general and model-agnostic formulation of\ncompositionality as a constraint on symmetries of data distributions rather\nthan models. Informally, we prove that whenever a task can be solved by a\ncompositional model, there is a corresponding data augmentation scheme -- a\nprocedure for transforming examples into other well formed examples -- that\nimparts compositional inductive bias on any model trained to solve the same\ntask. We describe a procedure called LEXSYM that discovers these\ntransformations automatically, then applies them to training data for ordinary\nneural sequence models. Unlike existing compositional data augmentation\nprocedures, LEXSYM can be deployed agnostically across text, structured data,\nand even images. It matches or surpasses state-of-the-art, task-specific models\non COGS semantic parsing, SCAN and ALCHEMY instruction following, and\nCLEVR-COGENT visual question answering datasets.'}, 'authors': [{'name': 'Ekin Akyürek'}, {'name': 'Jacob Andreas'}], 'author_detail': {'name': 'Jacob Andreas'}, 'author': 'Jacob Andreas', 'arxiv_comment': 'ACL2023 Final Version', 'links': [{'href': 'http://arxiv.org/abs/2201.12926v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2201.12926v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.07648v5,2023-05-25 00:33:47+00:00,2022-03-15 05:07:04+00:00,Contrastive Learning of Sociopragmatic Meaning in Social Media,"[arxiv.Result.Author('Chiyu Zhang'), arxiv.Result.Author('Muhammad Abdul-Mageed'), arxiv.Result.Author('Ganesh Jawahar')]","Recent progress in representation and contrastive learning in NLP has not
widely considered the class of \textit{sociopragmatic meaning} (i.e., meaning
in interaction within different language communities). To bridge this gap, we
propose a novel framework for learning task-agnostic representations
transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate
speech, humor, sarcasm). Our framework outperforms other contrastive learning
frameworks for both in-domain and out-of-domain data, across both the general
and few-shot settings. For example, compared to two popular pre-trained
language models, our method obtains an improvement of $11.66$ average $F_1$ on
$16$ datasets when fine-tuned on only $20$ training samples per dataset.Our
code is available at: https://github.com/UBC-NLP/infodcl",Final camera-ready version for ACL2023,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2203.07648v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.07648v5', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.07648v5,"{'id': 'http://arxiv.org/abs/2203.07648v5', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.07648v5', 'updated': '2023-05-25T00:33:47Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=25, tm_hour=0, tm_min=33, tm_sec=47, tm_wday=3, tm_yday=145, tm_isdst=0), 'published': '2022-03-15T05:07:04Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=15, tm_hour=5, tm_min=7, tm_sec=4, tm_wday=1, tm_yday=74, tm_isdst=0), 'title': 'Contrastive Learning of Sociopragmatic Meaning in Social Media', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Contrastive Learning of Sociopragmatic Meaning in Social Media'}, 'summary': 'Recent progress in representation and contrastive learning in NLP has not\nwidely considered the class of \\textit{sociopragmatic meaning} (i.e., meaning\nin interaction within different language communities). To bridge this gap, we\npropose a novel framework for learning task-agnostic representations\ntransferable to a wide range of sociopragmatic tasks (e.g., emotion, hate\nspeech, humor, sarcasm). Our framework outperforms other contrastive learning\nframeworks for both in-domain and out-of-domain data, across both the general\nand few-shot settings. For example, compared to two popular pre-trained\nlanguage models, our method obtains an improvement of $11.66$ average $F_1$ on\n$16$ datasets when fine-tuned on only $20$ training samples per dataset.Our\ncode is available at: https://github.com/UBC-NLP/infodcl', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Recent progress in representation and contrastive learning in NLP has not\nwidely considered the class of \\textit{sociopragmatic meaning} (i.e., meaning\nin interaction within different language communities). To bridge this gap, we\npropose a novel framework for learning task-agnostic representations\ntransferable to a wide range of sociopragmatic tasks (e.g., emotion, hate\nspeech, humor, sarcasm). Our framework outperforms other contrastive learning\nframeworks for both in-domain and out-of-domain data, across both the general\nand few-shot settings. For example, compared to two popular pre-trained\nlanguage models, our method obtains an improvement of $11.66$ average $F_1$ on\n$16$ datasets when fine-tuned on only $20$ training samples per dataset.Our\ncode is available at: https://github.com/UBC-NLP/infodcl'}, 'authors': [{'name': 'Chiyu Zhang'}, {'name': 'Muhammad Abdul-Mageed'}, {'name': 'Ganesh Jawahar'}], 'author_detail': {'name': 'Ganesh Jawahar'}, 'author': 'Ganesh Jawahar', 'arxiv_comment': 'Final camera-ready version for ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2203.07648v5', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.07648v5', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.14207v2,2023-05-03 09:09:22+00:00,2022-03-27 04:41:55+00:00,Text Adversarial Purification as Defense against Adversarial Attacks,"[arxiv.Result.Author('Linyang Li'), arxiv.Result.Author('Demin Song'), arxiv.Result.Author('Xipeng Qiu')]","Adversarial purification is a successful defense mechanism against
adversarial attacks without requiring knowledge of the form of the incoming
attack. Generally, adversarial purification aims to remove the adversarial
perturbations therefore can make correct predictions based on the recovered
clean samples. Despite the success of adversarial purification in the computer
vision field that incorporates generative models such as energy-based models
and diffusion models, using purification as a defense strategy against textual
adversarial attacks is rarely explored. In this work, we introduce a novel
adversarial purification method that focuses on defending against textual
adversarial attacks. With the help of language models, we can inject noise by
masking input texts and reconstructing the masked texts based on the masked
language models. In this way, we construct an adversarial purification process
for textual models against the most widely used word-substitution adversarial
attacks. We test our proposed adversarial purification method on several strong
adversarial attack methods including Textfooler and BERT-Attack and
experimental results indicate that the purification algorithm can successfully
defend against strong word-substitution attacks.",Accepted by ACL2023 main conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.14207v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.14207v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.14207v2,"{'id': 'http://arxiv.org/abs/2203.14207v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.14207v2', 'updated': '2023-05-03T09:09:22Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=3, tm_hour=9, tm_min=9, tm_sec=22, tm_wday=2, tm_yday=123, tm_isdst=0), 'published': '2022-03-27T04:41:55Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=27, tm_hour=4, tm_min=41, tm_sec=55, tm_wday=6, tm_yday=86, tm_isdst=0), 'title': 'Text Adversarial Purification as Defense against Adversarial Attacks', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Text Adversarial Purification as Defense against Adversarial Attacks'}, 'summary': 'Adversarial purification is a successful defense mechanism against\nadversarial attacks without requiring knowledge of the form of the incoming\nattack. Generally, adversarial purification aims to remove the adversarial\nperturbations therefore can make correct predictions based on the recovered\nclean samples. Despite the success of adversarial purification in the computer\nvision field that incorporates generative models such as energy-based models\nand diffusion models, using purification as a defense strategy against textual\nadversarial attacks is rarely explored. In this work, we introduce a novel\nadversarial purification method that focuses on defending against textual\nadversarial attacks. With the help of language models, we can inject noise by\nmasking input texts and reconstructing the masked texts based on the masked\nlanguage models. In this way, we construct an adversarial purification process\nfor textual models against the most widely used word-substitution adversarial\nattacks. We test our proposed adversarial purification method on several strong\nadversarial attack methods including Textfooler and BERT-Attack and\nexperimental results indicate that the purification algorithm can successfully\ndefend against strong word-substitution attacks.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Adversarial purification is a successful defense mechanism against\nadversarial attacks without requiring knowledge of the form of the incoming\nattack. Generally, adversarial purification aims to remove the adversarial\nperturbations therefore can make correct predictions based on the recovered\nclean samples. Despite the success of adversarial purification in the computer\nvision field that incorporates generative models such as energy-based models\nand diffusion models, using purification as a defense strategy against textual\nadversarial attacks is rarely explored. In this work, we introduce a novel\nadversarial purification method that focuses on defending against textual\nadversarial attacks. With the help of language models, we can inject noise by\nmasking input texts and reconstructing the masked texts based on the masked\nlanguage models. In this way, we construct an adversarial purification process\nfor textual models against the most widely used word-substitution adversarial\nattacks. We test our proposed adversarial purification method on several strong\nadversarial attack methods including Textfooler and BERT-Attack and\nexperimental results indicate that the purification algorithm can successfully\ndefend against strong word-substitution attacks.'}, 'authors': [{'name': 'Linyang Li'}, {'name': 'Demin Song'}, {'name': 'Xipeng Qiu'}], 'author_detail': {'name': 'Xipeng Qiu'}, 'author': 'Xipeng Qiu', 'arxiv_comment': 'Accepted by ACL2023 main conference', 'links': [{'href': 'http://arxiv.org/abs/2203.14207v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.14207v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2205.12585v2,2023-05-26 08:31:50+00:00,2022-05-25 08:57:46+00:00,TAGPRIME: A Unified Framework for Relational Structure Extraction,"[arxiv.Result.Author('I-Hung Hsu'), arxiv.Result.Author('Kuan-Hao Huang'), arxiv.Result.Author('Shuning Zhang'), arxiv.Result.Author('Wenxin Cheng'), arxiv.Result.Author('Premkumar Natarajan'), arxiv.Result.Author('Kai-Wei Chang'), arxiv.Result.Author('Nanyun Peng')]","Many tasks in natural language processing require the extraction of
relationship information for a given condition, such as event argument
extraction, relation extraction, and task-oriented semantic parsing. Recent
works usually propose sophisticated models for each task independently and pay
less attention to the commonality of these tasks and to have a unified
framework for all the tasks. In this work, we propose to take a unified view of
all these tasks and introduce TAGPRIME to address relational structure
extraction problems. TAGPRIME is a sequence tagging model that appends priming
words about the information of the given condition (such as an event trigger)
to the input text. With the self-attention mechanism in pre-trained language
models, the priming words make the output contextualized representations
contain more information about the given condition, and hence become more
suitable for extracting specific relationships for the condition. Extensive
experiments and analyses on three different tasks that cover ten datasets
across five different languages demonstrate the generality and effectiveness of
TAGPRIME.","Paper accepted by ACL2023 as a main conference paper. The first two
  authors contribute equally",,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2205.12585v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.12585v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2205.12585v2,"{'id': 'http://arxiv.org/abs/2205.12585v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2205.12585v2', 'updated': '2023-05-26T08:31:50Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=8, tm_min=31, tm_sec=50, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2022-05-25T08:57:46Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=25, tm_hour=8, tm_min=57, tm_sec=46, tm_wday=2, tm_yday=145, tm_isdst=0), 'title': 'TAGPRIME: A Unified Framework for Relational Structure Extraction', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'TAGPRIME: A Unified Framework for Relational Structure Extraction'}, 'summary': 'Many tasks in natural language processing require the extraction of\nrelationship information for a given condition, such as event argument\nextraction, relation extraction, and task-oriented semantic parsing. Recent\nworks usually propose sophisticated models for each task independently and pay\nless attention to the commonality of these tasks and to have a unified\nframework for all the tasks. In this work, we propose to take a unified view of\nall these tasks and introduce TAGPRIME to address relational structure\nextraction problems. TAGPRIME is a sequence tagging model that appends priming\nwords about the information of the given condition (such as an event trigger)\nto the input text. With the self-attention mechanism in pre-trained language\nmodels, the priming words make the output contextualized representations\ncontain more information about the given condition, and hence become more\nsuitable for extracting specific relationships for the condition. Extensive\nexperiments and analyses on three different tasks that cover ten datasets\nacross five different languages demonstrate the generality and effectiveness of\nTAGPRIME.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Many tasks in natural language processing require the extraction of\nrelationship information for a given condition, such as event argument\nextraction, relation extraction, and task-oriented semantic parsing. Recent\nworks usually propose sophisticated models for each task independently and pay\nless attention to the commonality of these tasks and to have a unified\nframework for all the tasks. In this work, we propose to take a unified view of\nall these tasks and introduce TAGPRIME to address relational structure\nextraction problems. TAGPRIME is a sequence tagging model that appends priming\nwords about the information of the given condition (such as an event trigger)\nto the input text. With the self-attention mechanism in pre-trained language\nmodels, the priming words make the output contextualized representations\ncontain more information about the given condition, and hence become more\nsuitable for extracting specific relationships for the condition. Extensive\nexperiments and analyses on three different tasks that cover ten datasets\nacross five different languages demonstrate the generality and effectiveness of\nTAGPRIME.'}, 'authors': [{'name': 'I-Hung Hsu'}, {'name': 'Kuan-Hao Huang'}, {'name': 'Shuning Zhang'}, {'name': 'Wenxin Cheng'}, {'name': 'Premkumar Natarajan'}, {'name': 'Kai-Wei Chang'}, {'name': 'Nanyun Peng'}], 'author_detail': {'name': 'Nanyun Peng'}, 'author': 'Nanyun Peng', 'arxiv_comment': 'Paper accepted by ACL2023 as a main conference paper. The first two\n  authors contribute equally', 'links': [{'href': 'http://arxiv.org/abs/2205.12585v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.12585v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2208.09137v2,2023-07-09 09:34:39+00:00,2022-08-19 03:33:45+00:00,GreenKGC: A Lightweight Knowledge Graph Completion Method,"[arxiv.Result.Author('Yun-Cheng Wang'), arxiv.Result.Author('Xiou Ge'), arxiv.Result.Author('Bin Wang'), arxiv.Result.Author('C. -C. Jay Kuo')]","Knowledge graph completion (KGC) aims to discover missing relationships
between entities in knowledge graphs (KGs). Most prior KGC work focuses on
learning embeddings for entities and relations through a simple scoring
function. Yet, a higher-dimensional embedding space is usually required for a
better reasoning capability, which leads to a larger model size and hinders
applicability to real-world problems (e.g., large-scale KGs or mobile/edge
computing). A lightweight modularized KGC solution, called GreenKGC, is
proposed in this work to address this issue. GreenKGC consists of three
modules: representation learning, feature pruning, and decision learning, to
extract discriminant KG features and make accurate predictions on missing
relationships using classifiers and negative sampling. Experimental results
demonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in
most datasets. In addition, low-dimensional GreenKGC can achieve competitive or
even better performance against high-dimensional models with a much smaller
model size.",Accepted to ACL2023,,,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://arxiv.org/abs/2208.09137v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2208.09137v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2208.09137v2,"{'id': 'http://arxiv.org/abs/2208.09137v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2208.09137v2', 'updated': '2023-07-09T09:34:39Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=9, tm_hour=9, tm_min=34, tm_sec=39, tm_wday=6, tm_yday=190, tm_isdst=0), 'published': '2022-08-19T03:33:45Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=8, tm_mday=19, tm_hour=3, tm_min=33, tm_sec=45, tm_wday=4, tm_yday=231, tm_isdst=0), 'title': 'GreenKGC: A Lightweight Knowledge Graph Completion Method', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'GreenKGC: A Lightweight Knowledge Graph Completion Method'}, 'summary': 'Knowledge graph completion (KGC) aims to discover missing relationships\nbetween entities in knowledge graphs (KGs). Most prior KGC work focuses on\nlearning embeddings for entities and relations through a simple scoring\nfunction. Yet, a higher-dimensional embedding space is usually required for a\nbetter reasoning capability, which leads to a larger model size and hinders\napplicability to real-world problems (e.g., large-scale KGs or mobile/edge\ncomputing). A lightweight modularized KGC solution, called GreenKGC, is\nproposed in this work to address this issue. GreenKGC consists of three\nmodules: representation learning, feature pruning, and decision learning, to\nextract discriminant KG features and make accurate predictions on missing\nrelationships using classifiers and negative sampling. Experimental results\ndemonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in\nmost datasets. In addition, low-dimensional GreenKGC can achieve competitive or\neven better performance against high-dimensional models with a much smaller\nmodel size.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Knowledge graph completion (KGC) aims to discover missing relationships\nbetween entities in knowledge graphs (KGs). Most prior KGC work focuses on\nlearning embeddings for entities and relations through a simple scoring\nfunction. Yet, a higher-dimensional embedding space is usually required for a\nbetter reasoning capability, which leads to a larger model size and hinders\napplicability to real-world problems (e.g., large-scale KGs or mobile/edge\ncomputing). A lightweight modularized KGC solution, called GreenKGC, is\nproposed in this work to address this issue. GreenKGC consists of three\nmodules: representation learning, feature pruning, and decision learning, to\nextract discriminant KG features and make accurate predictions on missing\nrelationships using classifiers and negative sampling. Experimental results\ndemonstrate that, in low dimensions, GreenKGC can outperform SOTA methods in\nmost datasets. In addition, low-dimensional GreenKGC can achieve competitive or\neven better performance against high-dimensional models with a much smaller\nmodel size.'}, 'authors': [{'name': 'Yun-Cheng Wang'}, {'name': 'Xiou Ge'}, {'name': 'Bin Wang'}, {'name': 'C. -C. Jay Kuo'}], 'author_detail': {'name': 'C. -C. Jay Kuo'}, 'author': 'C. -C. Jay Kuo', 'arxiv_comment': 'Accepted to ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2208.09137v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2208.09137v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2209.15206v3,2023-05-16 02:45:51+00:00,2022-09-30 03:28:19+00:00,What Makes Pre-trained Language Models Better Zero-shot Learners?,"[arxiv.Result.Author('Jinghui Lu'), arxiv.Result.Author('Dongsheng Zhu'), arxiv.Result.Author('Weidong Han'), arxiv.Result.Author('Rui Zhao'), arxiv.Result.Author('Brian Mac Namee'), arxiv.Result.Author('Fei Tan')]","Current methods for prompt learning in zeroshot scenarios widely rely on a
development set with sufficient human-annotated data to select the
best-performing prompt template a posteriori. This is not ideal because in a
realworld zero-shot scenario of practical relevance, no labelled data is
available. Thus, we propose a simple yet effective method for screening
reasonable prompt templates in zero-shot text classification: Perplexity
Selection (Perplection). We hypothesize that language discrepancy can be used
to measure the efficacy of prompt templates, and thereby develop a
substantiated perplexity-based scheme allowing for forecasting the performance
of prompt templates in advance. Experiments show that our method leads to
improved prediction performance in a realistic zero-shot setting, eliminating
the need for any labelled examples.",Accepted to ACL2023 main conference,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2209.15206v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2209.15206v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2209.15206v3,"{'id': 'http://arxiv.org/abs/2209.15206v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2209.15206v3', 'updated': '2023-05-16T02:45:51Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=16, tm_hour=2, tm_min=45, tm_sec=51, tm_wday=1, tm_yday=136, tm_isdst=0), 'published': '2022-09-30T03:28:19Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=9, tm_mday=30, tm_hour=3, tm_min=28, tm_sec=19, tm_wday=4, tm_yday=273, tm_isdst=0), 'title': 'What Makes Pre-trained Language Models Better Zero-shot Learners?', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'What Makes Pre-trained Language Models Better Zero-shot Learners?'}, 'summary': 'Current methods for prompt learning in zeroshot scenarios widely rely on a\ndevelopment set with sufficient human-annotated data to select the\nbest-performing prompt template a posteriori. This is not ideal because in a\nrealworld zero-shot scenario of practical relevance, no labelled data is\navailable. Thus, we propose a simple yet effective method for screening\nreasonable prompt templates in zero-shot text classification: Perplexity\nSelection (Perplection). We hypothesize that language discrepancy can be used\nto measure the efficacy of prompt templates, and thereby develop a\nsubstantiated perplexity-based scheme allowing for forecasting the performance\nof prompt templates in advance. Experiments show that our method leads to\nimproved prediction performance in a realistic zero-shot setting, eliminating\nthe need for any labelled examples.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Current methods for prompt learning in zeroshot scenarios widely rely on a\ndevelopment set with sufficient human-annotated data to select the\nbest-performing prompt template a posteriori. This is not ideal because in a\nrealworld zero-shot scenario of practical relevance, no labelled data is\navailable. Thus, we propose a simple yet effective method for screening\nreasonable prompt templates in zero-shot text classification: Perplexity\nSelection (Perplection). We hypothesize that language discrepancy can be used\nto measure the efficacy of prompt templates, and thereby develop a\nsubstantiated perplexity-based scheme allowing for forecasting the performance\nof prompt templates in advance. Experiments show that our method leads to\nimproved prediction performance in a realistic zero-shot setting, eliminating\nthe need for any labelled examples.'}, 'authors': [{'name': 'Jinghui Lu'}, {'name': 'Dongsheng Zhu'}, {'name': 'Weidong Han'}, {'name': 'Rui Zhao'}, {'name': 'Brian Mac Namee'}, {'name': 'Fei Tan'}], 'author_detail': {'name': 'Fei Tan'}, 'author': 'Fei Tan', 'arxiv_comment': 'Accepted to ACL2023 main conference', 'links': [{'href': 'http://arxiv.org/abs/2209.15206v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2209.15206v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2210.08917v2,2023-07-10 06:21:55+00:00,2022-10-17 10:14:22+00:00,Mars: Modeling Context & State Representations with Contrastive Learning for End-to-End Task-Oriented Dialog,"[arxiv.Result.Author('Haipeng Sun'), arxiv.Result.Author('Junwei Bao'), arxiv.Result.Author('Youzheng Wu'), arxiv.Result.Author('Xiaodong He')]","Traditional end-to-end task-oriented dialog systems first convert dialog
context into belief state and action state before generating the system
response. The system response performance is significantly affected by the
quality of the belief state and action state. We first explore what dialog
context representation is beneficial to improving the quality of the belief
state and action state, which further enhances the generated response quality.
To tackle our exploration, we propose Mars, an end-to-end task-oriented dialog
system with two contrastive learning strategies to model the relationship
between dialog context and belief/action state representations. Empirical
results show dialog context representations, which are more different from
semantic state representations, are more conducive to multi-turn task-oriented
dialog. Moreover, our proposed Mars achieves state-of-the-art performance on
the MultiWOZ 2.0, CamRest676, and CrossWOZ.",Findings of ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2210.08917v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2210.08917v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2210.08917v2,"{'id': 'http://arxiv.org/abs/2210.08917v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2210.08917v2', 'updated': '2023-07-10T06:21:55Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=10, tm_hour=6, tm_min=21, tm_sec=55, tm_wday=0, tm_yday=191, tm_isdst=0), 'published': '2022-10-17T10:14:22Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=10, tm_mday=17, tm_hour=10, tm_min=14, tm_sec=22, tm_wday=0, tm_yday=290, tm_isdst=0), 'title': 'Mars: Modeling Context & State Representations with Contrastive Learning\n  for End-to-End Task-Oriented Dialog', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Mars: Modeling Context & State Representations with Contrastive Learning\n  for End-to-End Task-Oriented Dialog'}, 'summary': 'Traditional end-to-end task-oriented dialog systems first convert dialog\ncontext into belief state and action state before generating the system\nresponse. The system response performance is significantly affected by the\nquality of the belief state and action state. We first explore what dialog\ncontext representation is beneficial to improving the quality of the belief\nstate and action state, which further enhances the generated response quality.\nTo tackle our exploration, we propose Mars, an end-to-end task-oriented dialog\nsystem with two contrastive learning strategies to model the relationship\nbetween dialog context and belief/action state representations. Empirical\nresults show dialog context representations, which are more different from\nsemantic state representations, are more conducive to multi-turn task-oriented\ndialog. Moreover, our proposed Mars achieves state-of-the-art performance on\nthe MultiWOZ 2.0, CamRest676, and CrossWOZ.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Traditional end-to-end task-oriented dialog systems first convert dialog\ncontext into belief state and action state before generating the system\nresponse. The system response performance is significantly affected by the\nquality of the belief state and action state. We first explore what dialog\ncontext representation is beneficial to improving the quality of the belief\nstate and action state, which further enhances the generated response quality.\nTo tackle our exploration, we propose Mars, an end-to-end task-oriented dialog\nsystem with two contrastive learning strategies to model the relationship\nbetween dialog context and belief/action state representations. Empirical\nresults show dialog context representations, which are more different from\nsemantic state representations, are more conducive to multi-turn task-oriented\ndialog. Moreover, our proposed Mars achieves state-of-the-art performance on\nthe MultiWOZ 2.0, CamRest676, and CrossWOZ.'}, 'authors': [{'name': 'Haipeng Sun'}, {'name': 'Junwei Bao'}, {'name': 'Youzheng Wu'}, {'name': 'Xiaodong He'}], 'author_detail': {'name': 'Xiaodong He'}, 'author': 'Xiaodong He', 'arxiv_comment': 'Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2210.08917v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.08917v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2212.08307v2,2023-05-24 10:02:31+00:00,2022-12-16 07:11:18+00:00,Controllable Text Generation via Probability Density Estimation in the Latent Space,"[arxiv.Result.Author('Yuxuan Gu'), arxiv.Result.Author('Xiaocheng Feng'), arxiv.Result.Author('Sicheng Ma'), arxiv.Result.Author('Lingyuan Zhang'), arxiv.Result.Author('Heng Gong'), arxiv.Result.Author('Weihong Zhong'), arxiv.Result.Author('Bing Qin')]","Previous work on controllable text generation has explored the idea of
control from the latent space, such as optimizing a representation with
attribute-related classifiers or sampling a representation from relevant
discrete samples. However, they are not effective enough in modeling both the
latent space and the control, leaving controlled text with low quality and
diversity. In this work, we propose a novel control framework using probability
density estimation in the latent space. Our method utilizes an invertible
transformation function, the Normalizing Flow, that maps the complex
distributions in the latent space to simple Gaussian distributions in the prior
space. Thus, we can perform sophisticated and flexible control in the prior
space and feed the control effects back into the latent space owing to the
one-one-mapping property of invertible transformations. Experiments on
single-attribute controls and multi-attribute control reveal that our method
outperforms several strong baselines on attribute relevance and text quality
and achieves the SOTA. Further analysis of control strength adjustment
demonstrates the flexibility of our control strategy.","25 pages, 9 figures, Accepted to ACL2023",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2212.08307v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.08307v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2212.08307v2,"{'id': 'http://arxiv.org/abs/2212.08307v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2212.08307v2', 'updated': '2023-05-24T10:02:31Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=24, tm_hour=10, tm_min=2, tm_sec=31, tm_wday=2, tm_yday=144, tm_isdst=0), 'published': '2022-12-16T07:11:18Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=12, tm_mday=16, tm_hour=7, tm_min=11, tm_sec=18, tm_wday=4, tm_yday=350, tm_isdst=0), 'title': 'Controllable Text Generation via Probability Density Estimation in the\n  Latent Space', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Controllable Text Generation via Probability Density Estimation in the\n  Latent Space'}, 'summary': 'Previous work on controllable text generation has explored the idea of\ncontrol from the latent space, such as optimizing a representation with\nattribute-related classifiers or sampling a representation from relevant\ndiscrete samples. However, they are not effective enough in modeling both the\nlatent space and the control, leaving controlled text with low quality and\ndiversity. In this work, we propose a novel control framework using probability\ndensity estimation in the latent space. Our method utilizes an invertible\ntransformation function, the Normalizing Flow, that maps the complex\ndistributions in the latent space to simple Gaussian distributions in the prior\nspace. Thus, we can perform sophisticated and flexible control in the prior\nspace and feed the control effects back into the latent space owing to the\none-one-mapping property of invertible transformations. Experiments on\nsingle-attribute controls and multi-attribute control reveal that our method\noutperforms several strong baselines on attribute relevance and text quality\nand achieves the SOTA. Further analysis of control strength adjustment\ndemonstrates the flexibility of our control strategy.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Previous work on controllable text generation has explored the idea of\ncontrol from the latent space, such as optimizing a representation with\nattribute-related classifiers or sampling a representation from relevant\ndiscrete samples. However, they are not effective enough in modeling both the\nlatent space and the control, leaving controlled text with low quality and\ndiversity. In this work, we propose a novel control framework using probability\ndensity estimation in the latent space. Our method utilizes an invertible\ntransformation function, the Normalizing Flow, that maps the complex\ndistributions in the latent space to simple Gaussian distributions in the prior\nspace. Thus, we can perform sophisticated and flexible control in the prior\nspace and feed the control effects back into the latent space owing to the\none-one-mapping property of invertible transformations. Experiments on\nsingle-attribute controls and multi-attribute control reveal that our method\noutperforms several strong baselines on attribute relevance and text quality\nand achieves the SOTA. Further analysis of control strength adjustment\ndemonstrates the flexibility of our control strategy.'}, 'authors': [{'name': 'Yuxuan Gu'}, {'name': 'Xiaocheng Feng'}, {'name': 'Sicheng Ma'}, {'name': 'Lingyuan Zhang'}, {'name': 'Heng Gong'}, {'name': 'Weihong Zhong'}, {'name': 'Bing Qin'}], 'author_detail': {'name': 'Bing Qin'}, 'author': 'Bing Qin', 'arxiv_comment': '25 pages, 9 figures, Accepted to ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2212.08307v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2212.08307v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2212.09086v4,2023-05-16 11:29:24+00:00,2022-12-18 13:36:07+00:00,PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism,"[arxiv.Result.Author('Yongkang Liu'), arxiv.Result.Author('Shi Feng'), arxiv.Result.Author('Daling Wang'), arxiv.Result.Author('Yifei Zhang'), arxiv.Result.Author('Hinrich Schütze')]","We investigate response generation for multi-turn dialogue in
generative-based chatbots. Existing generative models based on RNNs (Recurrent
Neural Networks) usually employ the last hidden state to summarize the
sequences, which makes models unable to capture the subtle variability observed
in different dialogues and cannot distinguish the differences between dialogues
that are similar in composition. In this paper, we propose a Pseudo-Variational
Gated Recurrent Unit (PVGRU) component without posterior knowledge through
introducing a recurrent summarizing variable into the GRU, which can aggregate
the accumulated distribution variations of subsequences. PVGRU can perceive the
subtle semantic variability through summarizing variables that are optimized by
the devised distribution consistency and reconstruction objectives. In
addition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model
based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve
the diversity and relevance of responses on two benchmark datasets.",ACL2023 main conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2212.09086v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.09086v4', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2212.09086v4,"{'id': 'http://arxiv.org/abs/2212.09086v4', 'guidislink': True, 'link': 'http://arxiv.org/abs/2212.09086v4', 'updated': '2023-05-16T11:29:24Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=16, tm_hour=11, tm_min=29, tm_sec=24, tm_wday=1, tm_yday=136, tm_isdst=0), 'published': '2022-12-18T13:36:07Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=12, tm_mday=18, tm_hour=13, tm_min=36, tm_sec=7, tm_wday=6, tm_yday=352, tm_isdst=0), 'title': 'PVGRU: Generating Diverse and Relevant Dialogue Responses via\n  Pseudo-Variational Mechanism', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'PVGRU: Generating Diverse and Relevant Dialogue Responses via\n  Pseudo-Variational Mechanism'}, 'summary': 'We investigate response generation for multi-turn dialogue in\ngenerative-based chatbots. Existing generative models based on RNNs (Recurrent\nNeural Networks) usually employ the last hidden state to summarize the\nsequences, which makes models unable to capture the subtle variability observed\nin different dialogues and cannot distinguish the differences between dialogues\nthat are similar in composition. In this paper, we propose a Pseudo-Variational\nGated Recurrent Unit (PVGRU) component without posterior knowledge through\nintroducing a recurrent summarizing variable into the GRU, which can aggregate\nthe accumulated distribution variations of subsequences. PVGRU can perceive the\nsubtle semantic variability through summarizing variables that are optimized by\nthe devised distribution consistency and reconstruction objectives. In\naddition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model\nbased on PVGRU. Experimental results demonstrate that PVGRU can broadly improve\nthe diversity and relevance of responses on two benchmark datasets.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We investigate response generation for multi-turn dialogue in\ngenerative-based chatbots. Existing generative models based on RNNs (Recurrent\nNeural Networks) usually employ the last hidden state to summarize the\nsequences, which makes models unable to capture the subtle variability observed\nin different dialogues and cannot distinguish the differences between dialogues\nthat are similar in composition. In this paper, we propose a Pseudo-Variational\nGated Recurrent Unit (PVGRU) component without posterior knowledge through\nintroducing a recurrent summarizing variable into the GRU, which can aggregate\nthe accumulated distribution variations of subsequences. PVGRU can perceive the\nsubtle semantic variability through summarizing variables that are optimized by\nthe devised distribution consistency and reconstruction objectives. In\naddition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model\nbased on PVGRU. Experimental results demonstrate that PVGRU can broadly improve\nthe diversity and relevance of responses on two benchmark datasets.'}, 'authors': [{'name': 'Yongkang Liu'}, {'name': 'Shi Feng'}, {'name': 'Daling Wang'}, {'name': 'Yifei Zhang'}, {'name': 'Hinrich Schütze'}], 'author_detail': {'name': 'Hinrich Schütze'}, 'author': 'Hinrich Schütze', 'arxiv_comment': 'ACL2023 main conference', 'links': [{'href': 'http://arxiv.org/abs/2212.09086v4', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2212.09086v4', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2212.09305v2,2023-07-07 17:49:31+00:00,2022-12-19 09:02:16+00:00,SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic Mistakes,"[arxiv.Result.Author('Wenda Xu'), arxiv.Result.Author('Xian Qian'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Lei Li'), arxiv.Result.Author('William Yang Wang')]","Is it possible to train a general metric for evaluating text generation
quality without human annotated ratings? Existing learned metrics either
perform unsatisfactorily across text generation tasks or require human ratings
for training on specific tasks. In this paper, we propose SESCORE2, a
self-supervised approach for training a model-based metric for text generation
evaluation. The key concept is to synthesize realistic model mistakes by
perturbing sentences retrieved from a corpus. The primary advantage of the
SESCORE2 is its ease of extension to many other languages while providing
reliable severity estimation. We evaluate SESCORE2 and previous methods on four
text generation tasks across three languages. SESCORE2 outperforms unsupervised
metric PRISM on four text generation evaluation benchmarks, with a Kendall
improvement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised
BLEURT and COMET on multiple text generation tasks. The code and data are
available at https://github.com/xu1998hz/SEScore2.",Accepted at ACL2023 Main Conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2212.09305v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.09305v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2212.09305v2,"{'id': 'http://arxiv.org/abs/2212.09305v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2212.09305v2', 'updated': '2023-07-07T17:49:31Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=7, tm_hour=17, tm_min=49, tm_sec=31, tm_wday=4, tm_yday=188, tm_isdst=0), 'published': '2022-12-19T09:02:16Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=12, tm_mday=19, tm_hour=9, tm_min=2, tm_sec=16, tm_wday=0, tm_yday=353, tm_isdst=0), 'title': 'SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic\n  Mistakes', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'SESCORE2: Learning Text Generation Evaluation via Synthesizing Realistic\n  Mistakes'}, 'summary': 'Is it possible to train a general metric for evaluating text generation\nquality without human annotated ratings? Existing learned metrics either\nperform unsatisfactorily across text generation tasks or require human ratings\nfor training on specific tasks. In this paper, we propose SESCORE2, a\nself-supervised approach for training a model-based metric for text generation\nevaluation. The key concept is to synthesize realistic model mistakes by\nperturbing sentences retrieved from a corpus. The primary advantage of the\nSESCORE2 is its ease of extension to many other languages while providing\nreliable severity estimation. We evaluate SESCORE2 and previous methods on four\ntext generation tasks across three languages. SESCORE2 outperforms unsupervised\nmetric PRISM on four text generation evaluation benchmarks, with a Kendall\nimprovement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised\nBLEURT and COMET on multiple text generation tasks. The code and data are\navailable at https://github.com/xu1998hz/SEScore2.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Is it possible to train a general metric for evaluating text generation\nquality without human annotated ratings? Existing learned metrics either\nperform unsatisfactorily across text generation tasks or require human ratings\nfor training on specific tasks. In this paper, we propose SESCORE2, a\nself-supervised approach for training a model-based metric for text generation\nevaluation. The key concept is to synthesize realistic model mistakes by\nperturbing sentences retrieved from a corpus. The primary advantage of the\nSESCORE2 is its ease of extension to many other languages while providing\nreliable severity estimation. We evaluate SESCORE2 and previous methods on four\ntext generation tasks across three languages. SESCORE2 outperforms unsupervised\nmetric PRISM on four text generation evaluation benchmarks, with a Kendall\nimprovement of 0.078. Surprisingly, SESCORE2 even outperforms the supervised\nBLEURT and COMET on multiple text generation tasks. The code and data are\navailable at https://github.com/xu1998hz/SEScore2.'}, 'authors': [{'name': 'Wenda Xu'}, {'name': 'Xian Qian'}, {'name': 'Mingxuan Wang'}, {'name': 'Lei Li'}, {'name': 'William Yang Wang'}], 'author_detail': {'name': 'William Yang Wang'}, 'author': 'William Yang Wang', 'arxiv_comment': 'Accepted at ACL2023 Main Conference', 'links': [{'href': 'http://arxiv.org/abs/2212.09305v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2212.09305v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2212.09588v2,2023-05-26 11:02:13+00:00,2022-12-19 16:21:05+00:00,Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint Modeling,"[arxiv.Result.Author('Mingzhu Cai'), arxiv.Result.Author('Siqi Bao'), arxiv.Result.Author('Xin Tian'), arxiv.Result.Author('Huang He'), arxiv.Result.Author('Fan Wang'), arxiv.Result.Author('Hua Wu')]","In this paper, we propose an unsupervised query enhanced approach for
knowledge-intensive conversations, namely QKConv. There are three modules in
QKConv: a query generator, an off-the-shelf knowledge selector, and a response
generator. QKConv is optimized through joint training, which produces the
response by exploring multiple candidate queries and leveraging corresponding
selected knowledge. The joint training solely relies on the dialogue context
and target response, getting exempt from extra query annotations or knowledge
provenances. To evaluate the effectiveness of the proposed QKConv, we conduct
experiments on three representative knowledge-intensive conversation datasets:
conversational question-answering, task-oriented dialogue, and
knowledge-grounded conversation. Experimental results reveal that QKConv
performs better than all unsupervised methods across three datasets and
achieves competitive performance compared to supervised methods.",Accepted for publication at ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2212.09588v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.09588v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2212.09588v2,"{'id': 'http://arxiv.org/abs/2212.09588v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2212.09588v2', 'updated': '2023-05-26T11:02:13Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=11, tm_min=2, tm_sec=13, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2022-12-19T16:21:05Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=12, tm_mday=19, tm_hour=16, tm_min=21, tm_sec=5, tm_wday=0, tm_yday=353, tm_isdst=0), 'title': 'Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint\n  Modeling', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Query Enhanced Knowledge-Intensive Conversation via Unsupervised Joint\n  Modeling'}, 'summary': 'In this paper, we propose an unsupervised query enhanced approach for\nknowledge-intensive conversations, namely QKConv. There are three modules in\nQKConv: a query generator, an off-the-shelf knowledge selector, and a response\ngenerator. QKConv is optimized through joint training, which produces the\nresponse by exploring multiple candidate queries and leveraging corresponding\nselected knowledge. The joint training solely relies on the dialogue context\nand target response, getting exempt from extra query annotations or knowledge\nprovenances. To evaluate the effectiveness of the proposed QKConv, we conduct\nexperiments on three representative knowledge-intensive conversation datasets:\nconversational question-answering, task-oriented dialogue, and\nknowledge-grounded conversation. Experimental results reveal that QKConv\nperforms better than all unsupervised methods across three datasets and\nachieves competitive performance compared to supervised methods.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'In this paper, we propose an unsupervised query enhanced approach for\nknowledge-intensive conversations, namely QKConv. There are three modules in\nQKConv: a query generator, an off-the-shelf knowledge selector, and a response\ngenerator. QKConv is optimized through joint training, which produces the\nresponse by exploring multiple candidate queries and leveraging corresponding\nselected knowledge. The joint training solely relies on the dialogue context\nand target response, getting exempt from extra query annotations or knowledge\nprovenances. To evaluate the effectiveness of the proposed QKConv, we conduct\nexperiments on three representative knowledge-intensive conversation datasets:\nconversational question-answering, task-oriented dialogue, and\nknowledge-grounded conversation. Experimental results reveal that QKConv\nperforms better than all unsupervised methods across three datasets and\nachieves competitive performance compared to supervised methods.'}, 'authors': [{'name': 'Mingzhu Cai'}, {'name': 'Siqi Bao'}, {'name': 'Xin Tian'}, {'name': 'Huang He'}, {'name': 'Fan Wang'}, {'name': 'Hua Wu'}], 'author_detail': {'name': 'Hua Wu'}, 'author': 'Hua Wu', 'arxiv_comment': 'Accepted for publication at ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2212.09588v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2212.09588v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2212.09603v2,2023-07-11 05:17:19+00:00,2022-12-19 16:41:19+00:00,Explanation Regeneration via Information Bottleneck,"[arxiv.Result.Author('Qintong Li'), arxiv.Result.Author('Zhiyong Wu'), arxiv.Result.Author('Lingpeng Kong'), arxiv.Result.Author('Wei Bi')]","Explaining the black-box predictions of NLP models naturally and accurately
is an important open problem in natural language generation. These free-text
explanations are expected to contain sufficient and carefully-selected evidence
to form supportive arguments for predictions. Due to the superior generative
capacity of large pretrained language models, recent work built on prompt
engineering enables explanation generation without specific training. However,
explanation generated through single-pass prompting often lacks sufficiency and
conciseness. To address this problem, we develop an information bottleneck
method EIB to produce refined explanations that are sufficient and concise. Our
approach regenerates the free-text explanation by polishing the single-pass
output from the pretrained language model but retaining the information that
supports the contents being explained. Experiments on two out-of-domain tasks
verify the effectiveness of EIB through automatic evaluation and
thoroughly-conducted human evaluation.",Accepted in ACL2023 Findings,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2212.09603v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.09603v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2212.09603v2,"{'id': 'http://arxiv.org/abs/2212.09603v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2212.09603v2', 'updated': '2023-07-11T05:17:19Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=11, tm_hour=5, tm_min=17, tm_sec=19, tm_wday=1, tm_yday=192, tm_isdst=0), 'published': '2022-12-19T16:41:19Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=12, tm_mday=19, tm_hour=16, tm_min=41, tm_sec=19, tm_wday=0, tm_yday=353, tm_isdst=0), 'title': 'Explanation Regeneration via Information Bottleneck', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Explanation Regeneration via Information Bottleneck'}, 'summary': 'Explaining the black-box predictions of NLP models naturally and accurately\nis an important open problem in natural language generation. These free-text\nexplanations are expected to contain sufficient and carefully-selected evidence\nto form supportive arguments for predictions. Due to the superior generative\ncapacity of large pretrained language models, recent work built on prompt\nengineering enables explanation generation without specific training. However,\nexplanation generated through single-pass prompting often lacks sufficiency and\nconciseness. To address this problem, we develop an information bottleneck\nmethod EIB to produce refined explanations that are sufficient and concise. Our\napproach regenerates the free-text explanation by polishing the single-pass\noutput from the pretrained language model but retaining the information that\nsupports the contents being explained. Experiments on two out-of-domain tasks\nverify the effectiveness of EIB through automatic evaluation and\nthoroughly-conducted human evaluation.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Explaining the black-box predictions of NLP models naturally and accurately\nis an important open problem in natural language generation. These free-text\nexplanations are expected to contain sufficient and carefully-selected evidence\nto form supportive arguments for predictions. Due to the superior generative\ncapacity of large pretrained language models, recent work built on prompt\nengineering enables explanation generation without specific training. However,\nexplanation generated through single-pass prompting often lacks sufficiency and\nconciseness. To address this problem, we develop an information bottleneck\nmethod EIB to produce refined explanations that are sufficient and concise. Our\napproach regenerates the free-text explanation by polishing the single-pass\noutput from the pretrained language model but retaining the information that\nsupports the contents being explained. Experiments on two out-of-domain tasks\nverify the effectiveness of EIB through automatic evaluation and\nthoroughly-conducted human evaluation.'}, 'authors': [{'name': 'Qintong Li'}, {'name': 'Zhiyong Wu'}, {'name': 'Lingpeng Kong'}, {'name': 'Wei Bi'}], 'author_detail': {'name': 'Wei Bi'}, 'author': 'Wei Bi', 'arxiv_comment': 'Accepted in ACL2023 Findings', 'links': [{'href': 'http://arxiv.org/abs/2212.09603v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2212.09603v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2302.08143v3,2023-11-19 14:31:53+00:00,2023-02-16 08:37:22+00:00,Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?,"[arxiv.Result.Author('Chengwei Qin'), arxiv.Result.Author('Qian Li'), arxiv.Result.Author('Ruochen Zhao'), arxiv.Result.Author('Shafiq Joty')]","Prompt tuning (PT) which only tunes the embeddings of an additional sequence
of tokens per task, keeping the pre-trained language model (PLM) frozen, has
shown remarkable performance in few-shot learning. Despite this, PT has been
shown to rely heavily on good initialization of the prompt embeddings. In this
work, we study meta prompt tuning (MPT) to systematically explore how
meta-learning can help improve (if it can) cross-task generalization in PT
through learning to initialize the prompt embeddings from other relevant tasks.
We empirically analyze a representative set of meta learning algorithms in a
wide range of adaptation settings with different source/target task
configurations on a large set of few-shot tasks. With extensive experiments and
analysis, we demonstrate the effectiveness of MPT. We find the improvement to
be significant particularly on classification tasks. For other kinds of tasks
such as question answering, we observe that while MPT can outperform PT in most
cases, it does not always outperform multi-task learning. We further provide an
in-depth analysis from the perspective of task similarity.",ACL2023,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2302.08143v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2302.08143v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2302.08143v3,"{'id': 'http://arxiv.org/abs/2302.08143v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2302.08143v3', 'updated': '2023-11-19T14:31:53Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=19, tm_hour=14, tm_min=31, tm_sec=53, tm_wday=6, tm_yday=323, tm_isdst=0), 'published': '2023-02-16T08:37:22Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=2, tm_mday=16, tm_hour=8, tm_min=37, tm_sec=22, tm_wday=3, tm_yday=47, tm_isdst=0), 'title': 'Learning to Initialize: Can Meta Learning Improve Cross-task\n  Generalization in Prompt Tuning?', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Learning to Initialize: Can Meta Learning Improve Cross-task\n  Generalization in Prompt Tuning?'}, 'summary': 'Prompt tuning (PT) which only tunes the embeddings of an additional sequence\nof tokens per task, keeping the pre-trained language model (PLM) frozen, has\nshown remarkable performance in few-shot learning. Despite this, PT has been\nshown to rely heavily on good initialization of the prompt embeddings. In this\nwork, we study meta prompt tuning (MPT) to systematically explore how\nmeta-learning can help improve (if it can) cross-task generalization in PT\nthrough learning to initialize the prompt embeddings from other relevant tasks.\nWe empirically analyze a representative set of meta learning algorithms in a\nwide range of adaptation settings with different source/target task\nconfigurations on a large set of few-shot tasks. With extensive experiments and\nanalysis, we demonstrate the effectiveness of MPT. We find the improvement to\nbe significant particularly on classification tasks. For other kinds of tasks\nsuch as question answering, we observe that while MPT can outperform PT in most\ncases, it does not always outperform multi-task learning. We further provide an\nin-depth analysis from the perspective of task similarity.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Prompt tuning (PT) which only tunes the embeddings of an additional sequence\nof tokens per task, keeping the pre-trained language model (PLM) frozen, has\nshown remarkable performance in few-shot learning. Despite this, PT has been\nshown to rely heavily on good initialization of the prompt embeddings. In this\nwork, we study meta prompt tuning (MPT) to systematically explore how\nmeta-learning can help improve (if it can) cross-task generalization in PT\nthrough learning to initialize the prompt embeddings from other relevant tasks.\nWe empirically analyze a representative set of meta learning algorithms in a\nwide range of adaptation settings with different source/target task\nconfigurations on a large set of few-shot tasks. With extensive experiments and\nanalysis, we demonstrate the effectiveness of MPT. We find the improvement to\nbe significant particularly on classification tasks. For other kinds of tasks\nsuch as question answering, we observe that while MPT can outperform PT in most\ncases, it does not always outperform multi-task learning. We further provide an\nin-depth analysis from the perspective of task similarity.'}, 'authors': [{'name': 'Chengwei Qin'}, {'name': 'Qian Li'}, {'name': 'Ruochen Zhao'}, {'name': 'Shafiq Joty'}], 'author_detail': {'name': 'Shafiq Joty'}, 'author': 'Shafiq Joty', 'arxiv_comment': 'ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2302.08143v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2302.08143v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2302.13048v1,2023-02-25 10:20:02+00:00,2023-02-25 10:20:02+00:00,Human-in-the-Loop Schema Induction,"[arxiv.Result.Author('Tianyi Zhang'), arxiv.Result.Author('Isaac Tham'), arxiv.Result.Author('Zhaoyi Hou'), arxiv.Result.Author('Jiaxuan Ren'), arxiv.Result.Author('Liyang Zhou'), arxiv.Result.Author('Hainiu Xu'), arxiv.Result.Author('Li Zhang'), arxiv.Result.Author('Lara J. Martin'), arxiv.Result.Author('Rotem Dror'), arxiv.Result.Author('Sha Li'), arxiv.Result.Author('Heng Ji'), arxiv.Result.Author('Martha Palmer'), arxiv.Result.Author('Susan Brown'), arxiv.Result.Author('Reece Suchocki'), arxiv.Result.Author('Chris Callison-Burch')]","Schema induction builds a graph representation explaining how events unfold
in a scenario. Existing approaches have been based on information retrieval
(IR) and information extraction(IE), often with limited human curation. We
demonstrate a human-in-the-loop schema induction system powered by GPT-3. We
first describe the different modules of our system, including prompting to
generate schematic elements, manual edit of those elements, and conversion of
those into a schema graph. By qualitatively comparing our system to previous
ones, we show that our system not only transfers to new domains more easily
than previous approaches, but also reduces efforts of human curation thanks to
our interactive interface.","10 pages, ACL2023 demo track",,10.18653/v1/2023.acl-demo.1,cs.HC,"['cs.HC', 'cs.CL']","[arxiv.Result.Link('http://dx.doi.org/10.18653/v1/2023.acl-demo.1', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2302.13048v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2302.13048v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2302.13048v1,"{'id': 'http://arxiv.org/abs/2302.13048v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2302.13048v1', 'updated': '2023-02-25T10:20:02Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=2, tm_mday=25, tm_hour=10, tm_min=20, tm_sec=2, tm_wday=5, tm_yday=56, tm_isdst=0), 'published': '2023-02-25T10:20:02Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=2, tm_mday=25, tm_hour=10, tm_min=20, tm_sec=2, tm_wday=5, tm_yday=56, tm_isdst=0), 'title': 'Human-in-the-Loop Schema Induction', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Human-in-the-Loop Schema Induction'}, 'summary': 'Schema induction builds a graph representation explaining how events unfold\nin a scenario. Existing approaches have been based on information retrieval\n(IR) and information extraction(IE), often with limited human curation. We\ndemonstrate a human-in-the-loop schema induction system powered by GPT-3. We\nfirst describe the different modules of our system, including prompting to\ngenerate schematic elements, manual edit of those elements, and conversion of\nthose into a schema graph. By qualitatively comparing our system to previous\nones, we show that our system not only transfers to new domains more easily\nthan previous approaches, but also reduces efforts of human curation thanks to\nour interactive interface.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Schema induction builds a graph representation explaining how events unfold\nin a scenario. Existing approaches have been based on information retrieval\n(IR) and information extraction(IE), often with limited human curation. We\ndemonstrate a human-in-the-loop schema induction system powered by GPT-3. We\nfirst describe the different modules of our system, including prompting to\ngenerate schematic elements, manual edit of those elements, and conversion of\nthose into a schema graph. By qualitatively comparing our system to previous\nones, we show that our system not only transfers to new domains more easily\nthan previous approaches, but also reduces efforts of human curation thanks to\nour interactive interface.'}, 'authors': [{'name': 'Tianyi Zhang'}, {'name': 'Isaac Tham'}, {'name': 'Zhaoyi Hou'}, {'name': 'Jiaxuan Ren'}, {'name': 'Liyang Zhou'}, {'name': 'Hainiu Xu'}, {'name': 'Li Zhang'}, {'name': 'Lara J. Martin'}, {'name': 'Rotem Dror'}, {'name': 'Sha Li'}, {'name': 'Heng Ji'}, {'name': 'Martha Palmer'}, {'name': 'Susan Brown'}, {'name': 'Reece Suchocki'}, {'name': 'Chris Callison-Burch'}], 'author_detail': {'name': 'Chris Callison-Burch'}, 'author': 'Chris Callison-Burch', 'arxiv_doi': '10.18653/v1/2023.acl-demo.1', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.18653/v1/2023.acl-demo.1', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2302.13048v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2302.13048v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': '10 pages, ACL2023 demo track', 'arxiv_primary_category': {'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.03117v2,2023-05-22 05:20:04+00:00,2023-05-04 19:31:50+00:00,Are Human Explanations Always Helpful? Towards Objective Evaluation of Human Natural Language Explanations,"[arxiv.Result.Author('Bingsheng Yao'), arxiv.Result.Author('Prithviraj Sen'), arxiv.Result.Author('Lucian Popa'), arxiv.Result.Author('James Hendler'), arxiv.Result.Author('Dakuo Wang')]","Human-annotated labels and explanations are critical for training explainable
NLP models. However, unlike human-annotated labels whose quality is easier to
calibrate (e.g., with a majority vote), human-crafted free-form explanations
can be quite subjective. Before blindly using them as ground truth to train ML
models, a vital question needs to be asked: How do we evaluate a
human-annotated explanation's quality? In this paper, we build on the view that
the quality of a human-annotated explanation can be measured based on its
helpfulness (or impairment) to the ML models' performance for the desired NLP
tasks for which the annotations were collected. In comparison to the commonly
used Simulatability score, we define a new metric that can take into
consideration the helpfulness of an explanation for model performance at both
fine-tuning and inference. With the help of a unified dataset format, we
evaluated the proposed metric on five datasets (e.g., e-SNLI) against two model
architectures (T5 and BART), and the results show that our proposed metric can
objectively evaluate the quality of human-annotated explanations, while
Simulatability falls short.",Accepted to ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.03117v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.03117v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.03117v2,"{'id': 'http://arxiv.org/abs/2305.03117v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.03117v2', 'updated': '2023-05-22T05:20:04Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=22, tm_hour=5, tm_min=20, tm_sec=4, tm_wday=0, tm_yday=142, tm_isdst=0), 'published': '2023-05-04T19:31:50Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=4, tm_hour=19, tm_min=31, tm_sec=50, tm_wday=3, tm_yday=124, tm_isdst=0), 'title': 'Are Human Explanations Always Helpful? Towards Objective Evaluation of\n  Human Natural Language Explanations', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Are Human Explanations Always Helpful? Towards Objective Evaluation of\n  Human Natural Language Explanations'}, 'summary': ""Human-annotated labels and explanations are critical for training explainable\nNLP models. However, unlike human-annotated labels whose quality is easier to\ncalibrate (e.g., with a majority vote), human-crafted free-form explanations\ncan be quite subjective. Before blindly using them as ground truth to train ML\nmodels, a vital question needs to be asked: How do we evaluate a\nhuman-annotated explanation's quality? In this paper, we build on the view that\nthe quality of a human-annotated explanation can be measured based on its\nhelpfulness (or impairment) to the ML models' performance for the desired NLP\ntasks for which the annotations were collected. In comparison to the commonly\nused Simulatability score, we define a new metric that can take into\nconsideration the helpfulness of an explanation for model performance at both\nfine-tuning and inference. With the help of a unified dataset format, we\nevaluated the proposed metric on five datasets (e.g., e-SNLI) against two model\narchitectures (T5 and BART), and the results show that our proposed metric can\nobjectively evaluate the quality of human-annotated explanations, while\nSimulatability falls short."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Human-annotated labels and explanations are critical for training explainable\nNLP models. However, unlike human-annotated labels whose quality is easier to\ncalibrate (e.g., with a majority vote), human-crafted free-form explanations\ncan be quite subjective. Before blindly using them as ground truth to train ML\nmodels, a vital question needs to be asked: How do we evaluate a\nhuman-annotated explanation's quality? In this paper, we build on the view that\nthe quality of a human-annotated explanation can be measured based on its\nhelpfulness (or impairment) to the ML models' performance for the desired NLP\ntasks for which the annotations were collected. In comparison to the commonly\nused Simulatability score, we define a new metric that can take into\nconsideration the helpfulness of an explanation for model performance at both\nfine-tuning and inference. With the help of a unified dataset format, we\nevaluated the proposed metric on five datasets (e.g., e-SNLI) against two model\narchitectures (T5 and BART), and the results show that our proposed metric can\nobjectively evaluate the quality of human-annotated explanations, while\nSimulatability falls short.""}, 'authors': [{'name': 'Bingsheng Yao'}, {'name': 'Prithviraj Sen'}, {'name': 'Lucian Popa'}, {'name': 'James Hendler'}, {'name': 'Dakuo Wang'}], 'author_detail': {'name': 'Dakuo Wang'}, 'author': 'Dakuo Wang', 'arxiv_comment': 'Accepted to ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.03117v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.03117v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.03132v2,2023-05-30 21:26:33+00:00,2023-05-04 20:22:18+00:00,The Role of Global and Local Context in Named Entity Recognition,"[arxiv.Result.Author('Arthur Amalvy'), arxiv.Result.Author('Vincent Labatut'), arxiv.Result.Author('Richard Dufour')]","Pre-trained transformer-based models have recently shown great performance
when applied to Named Entity Recognition (NER). As the complexity of their
self-attention mechanism prevents them from processing long documents at once,
these models are usually applied in a sequential fashion. Such an approach
unfortunately only incorporates local context and prevents leveraging global
document context in long documents such as novels, which might hinder
performance. In this article, we explore the impact of global document context,
and its relationships with local context. We find that correctly retrieving
global document context has a greater impact on performance than only
leveraging local context, prompting for further research on how to better
retrieve that context.",Accepted to ACL2023,"61st Annual Meeting of the Association for Computational
  Linguistics, 2023, p.714-722",10.18653/v1/2023.acl-short.62,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://dx.doi.org/10.18653/v1/2023.acl-short.62', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2305.03132v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.03132v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.03132v2,"{'id': 'http://arxiv.org/abs/2305.03132v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.03132v2', 'updated': '2023-05-30T21:26:33Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=30, tm_hour=21, tm_min=26, tm_sec=33, tm_wday=1, tm_yday=150, tm_isdst=0), 'published': '2023-05-04T20:22:18Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=4, tm_hour=20, tm_min=22, tm_sec=18, tm_wday=3, tm_yday=124, tm_isdst=0), 'title': 'The Role of Global and Local Context in Named Entity Recognition', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The Role of Global and Local Context in Named Entity Recognition'}, 'summary': 'Pre-trained transformer-based models have recently shown great performance\nwhen applied to Named Entity Recognition (NER). As the complexity of their\nself-attention mechanism prevents them from processing long documents at once,\nthese models are usually applied in a sequential fashion. Such an approach\nunfortunately only incorporates local context and prevents leveraging global\ndocument context in long documents such as novels, which might hinder\nperformance. In this article, we explore the impact of global document context,\nand its relationships with local context. We find that correctly retrieving\nglobal document context has a greater impact on performance than only\nleveraging local context, prompting for further research on how to better\nretrieve that context.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Pre-trained transformer-based models have recently shown great performance\nwhen applied to Named Entity Recognition (NER). As the complexity of their\nself-attention mechanism prevents them from processing long documents at once,\nthese models are usually applied in a sequential fashion. Such an approach\nunfortunately only incorporates local context and prevents leveraging global\ndocument context in long documents such as novels, which might hinder\nperformance. In this article, we explore the impact of global document context,\nand its relationships with local context. We find that correctly retrieving\nglobal document context has a greater impact on performance than only\nleveraging local context, prompting for further research on how to better\nretrieve that context.'}, 'authors': [{'name': 'Arthur Amalvy'}, {'name': 'Vincent Labatut'}, {'name': 'Richard Dufour'}], 'author_detail': {'name': 'Richard Dufour'}, 'author': 'Richard Dufour', 'arxiv_doi': '10.18653/v1/2023.acl-short.62', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.18653/v1/2023.acl-short.62', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2305.03132v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.03132v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': 'Accepted to ACL2023', 'arxiv_journal_ref': '61st Annual Meeting of the Association for Computational\n  Linguistics, 2023, p.714-722', 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.04076v2,2023-07-28 06:51:56+00:00,2023-05-06 15:48:24+00:00,SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition,"[arxiv.Result.Author('Shuzheng Si'), arxiv.Result.Author('Zefan Cai'), arxiv.Result.Author('Shuang Zeng'), arxiv.Result.Author('Guoqiang Feng'), arxiv.Result.Author('Jiaxing Lin'), arxiv.Result.Author('Baobao Chang')]","Distantly-Supervised Named Entity Recognition effectively alleviates the
burden of time-consuming and expensive annotation in the supervised setting.
But the context-free matching process and the limited coverage of knowledge
bases introduce inaccurate and incomplete annotation noise respectively.
Previous studies either considered only incomplete annotation noise or
indiscriminately handle two types of noise with the same strategy. In this
paper, we argue that the different causes of two types of noise bring up the
requirement of different strategies in model architecture. Therefore, we
propose the SANTA to handle these two types of noise separately with (1)
Memory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity
problem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate
decision boundary shifting problem caused by incomplete annotation and a
noise-tolerant loss to improve the robustness. Benefiting from our separate
tailored strategies, we confirm in the experiment that the two types of noise
are well mitigated. SANTA also achieves a new state-of-the-art on five public
datasets.",Findings of ACL2023,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.04076v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.04076v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.04076v2,"{'id': 'http://arxiv.org/abs/2305.04076v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.04076v2', 'updated': '2023-07-28T06:51:56Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=28, tm_hour=6, tm_min=51, tm_sec=56, tm_wday=4, tm_yday=209, tm_isdst=0), 'published': '2023-05-06T15:48:24Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=6, tm_hour=15, tm_min=48, tm_sec=24, tm_wday=5, tm_yday=126, tm_isdst=0), 'title': 'SANTA: Separate Strategies for Inaccurate and Incomplete Annotation\n  Noise in Distantly-Supervised Named Entity Recognition', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'SANTA: Separate Strategies for Inaccurate and Incomplete Annotation\n  Noise in Distantly-Supervised Named Entity Recognition'}, 'summary': 'Distantly-Supervised Named Entity Recognition effectively alleviates the\nburden of time-consuming and expensive annotation in the supervised setting.\nBut the context-free matching process and the limited coverage of knowledge\nbases introduce inaccurate and incomplete annotation noise respectively.\nPrevious studies either considered only incomplete annotation noise or\nindiscriminately handle two types of noise with the same strategy. In this\npaper, we argue that the different causes of two types of noise bring up the\nrequirement of different strategies in model architecture. Therefore, we\npropose the SANTA to handle these two types of noise separately with (1)\nMemory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity\nproblem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate\ndecision boundary shifting problem caused by incomplete annotation and a\nnoise-tolerant loss to improve the robustness. Benefiting from our separate\ntailored strategies, we confirm in the experiment that the two types of noise\nare well mitigated. SANTA also achieves a new state-of-the-art on five public\ndatasets.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Distantly-Supervised Named Entity Recognition effectively alleviates the\nburden of time-consuming and expensive annotation in the supervised setting.\nBut the context-free matching process and the limited coverage of knowledge\nbases introduce inaccurate and incomplete annotation noise respectively.\nPrevious studies either considered only incomplete annotation noise or\nindiscriminately handle two types of noise with the same strategy. In this\npaper, we argue that the different causes of two types of noise bring up the\nrequirement of different strategies in model architecture. Therefore, we\npropose the SANTA to handle these two types of noise separately with (1)\nMemory-smoothed Focal Loss and Entity-aware KNN to relieve the entity ambiguity\nproblem caused by inaccurate annotation, and (2) Boundary Mixup to alleviate\ndecision boundary shifting problem caused by incomplete annotation and a\nnoise-tolerant loss to improve the robustness. Benefiting from our separate\ntailored strategies, we confirm in the experiment that the two types of noise\nare well mitigated. SANTA also achieves a new state-of-the-art on five public\ndatasets.'}, 'authors': [{'name': 'Shuzheng Si'}, {'name': 'Zefan Cai'}, {'name': 'Shuang Zeng'}, {'name': 'Guoqiang Feng'}, {'name': 'Jiaxing Lin'}, {'name': 'Baobao Chang'}], 'author_detail': {'name': 'Baobao Chang'}, 'author': 'Baobao Chang', 'arxiv_comment': 'Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.04076v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.04076v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.04474v3,2023-06-22 06:44:57+00:00,2023-05-08 05:53:30+00:00,Vision Language Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation,"[arxiv.Result.Author('Chaoya Jiang'), arxiv.Result.Author('Wei Ye'), arxiv.Result.Author('Haiyang Xu'), arxiv.Result.Author('Miang yan'), arxiv.Result.Author('Shikun Zhang'), arxiv.Result.Author('Jie Zhang'), arxiv.Result.Author('Fei Huang')]","Cross-modal contrastive learning in vision language pretraining (VLP) faces
the challenge of (partial) false negatives. In this paper, we study this
problem from the perspective of Mutual Information (MI) optimization. It is
common sense that InfoNCE loss used in contrastive learning will maximize the
lower bound of MI between anchors and their positives, while we theoretically
prove that MI involving negatives also matters when noises commonly exist.
Guided by a more general lower bound form for optimization, we propose a
contrastive learning strategy regulated by progressively refined cross-modal
similarity, to more accurately optimize MI between an image/text anchor and its
negative texts/images instead of improperly minimizing it. Our method performs
competitively on four downstream cross-modal tasks and systematically balances
the beneficial and harmful effects of (partial) false negative samples under
theoretical guidance.",Accepted by ACL2023,,,cs.CV,"['cs.CV', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.04474v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.04474v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.04474v3,"{'id': 'http://arxiv.org/abs/2305.04474v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.04474v3', 'updated': '2023-06-22T06:44:57Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=22, tm_hour=6, tm_min=44, tm_sec=57, tm_wday=3, tm_yday=173, tm_isdst=0), 'published': '2023-05-08T05:53:30Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=5, tm_min=53, tm_sec=30, tm_wday=0, tm_yday=128, tm_isdst=0), 'title': 'Vision Language Pre-training by Contrastive Learning with Cross-Modal\n  Similarity Regulation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Vision Language Pre-training by Contrastive Learning with Cross-Modal\n  Similarity Regulation'}, 'summary': 'Cross-modal contrastive learning in vision language pretraining (VLP) faces\nthe challenge of (partial) false negatives. In this paper, we study this\nproblem from the perspective of Mutual Information (MI) optimization. It is\ncommon sense that InfoNCE loss used in contrastive learning will maximize the\nlower bound of MI between anchors and their positives, while we theoretically\nprove that MI involving negatives also matters when noises commonly exist.\nGuided by a more general lower bound form for optimization, we propose a\ncontrastive learning strategy regulated by progressively refined cross-modal\nsimilarity, to more accurately optimize MI between an image/text anchor and its\nnegative texts/images instead of improperly minimizing it. Our method performs\ncompetitively on four downstream cross-modal tasks and systematically balances\nthe beneficial and harmful effects of (partial) false negative samples under\ntheoretical guidance.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Cross-modal contrastive learning in vision language pretraining (VLP) faces\nthe challenge of (partial) false negatives. In this paper, we study this\nproblem from the perspective of Mutual Information (MI) optimization. It is\ncommon sense that InfoNCE loss used in contrastive learning will maximize the\nlower bound of MI between anchors and their positives, while we theoretically\nprove that MI involving negatives also matters when noises commonly exist.\nGuided by a more general lower bound form for optimization, we propose a\ncontrastive learning strategy regulated by progressively refined cross-modal\nsimilarity, to more accurately optimize MI between an image/text anchor and its\nnegative texts/images instead of improperly minimizing it. Our method performs\ncompetitively on four downstream cross-modal tasks and systematically balances\nthe beneficial and harmful effects of (partial) false negative samples under\ntheoretical guidance.'}, 'authors': [{'name': 'Chaoya Jiang'}, {'name': 'Wei Ye'}, {'name': 'Haiyang Xu'}, {'name': 'Miang yan'}, {'name': 'Shikun Zhang'}, {'name': 'Jie Zhang'}, {'name': 'Fei Huang'}], 'author_detail': {'name': 'Fei Huang'}, 'author': 'Fei Huang', 'arxiv_comment': 'Accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.04474v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.04474v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.04505v2,2023-06-04 11:05:03+00:00,2023-05-08 07:01:18+00:00,Target-Side Augmentation for Document-Level Machine Translation,"[arxiv.Result.Author('Guangsheng Bao'), arxiv.Result.Author('Zhiyang Teng'), arxiv.Result.Author('Yue Zhang')]","Document-level machine translation faces the challenge of data sparsity due
to its long input length and a small amount of training data, increasing the
risk of learning spurious patterns. To address this challenge, we propose a
target-side augmentation method, introducing a data augmentation (DA) model to
generate many potential translations for each source document. Learning on
these wider range translations, an MT model can learn a smoothed distribution,
thereby reducing the risk of data sparsity. We demonstrate that the DA model,
which estimates the posterior distribution, largely improves the MT
performance, outperforming the previous best system by 2.30 s-BLEU on News and
achieving new state-of-the-art on News and Europarl benchmarks. Our code is
available at https://github.com/baoguangsheng/target-side-augmentation.",Accepted by ACL2023 main conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.04505v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.04505v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.04505v2,"{'id': 'http://arxiv.org/abs/2305.04505v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.04505v2', 'updated': '2023-06-04T11:05:03Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=4, tm_hour=11, tm_min=5, tm_sec=3, tm_wday=6, tm_yday=155, tm_isdst=0), 'published': '2023-05-08T07:01:18Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=7, tm_min=1, tm_sec=18, tm_wday=0, tm_yday=128, tm_isdst=0), 'title': 'Target-Side Augmentation for Document-Level Machine Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Target-Side Augmentation for Document-Level Machine Translation'}, 'summary': 'Document-level machine translation faces the challenge of data sparsity due\nto its long input length and a small amount of training data, increasing the\nrisk of learning spurious patterns. To address this challenge, we propose a\ntarget-side augmentation method, introducing a data augmentation (DA) model to\ngenerate many potential translations for each source document. Learning on\nthese wider range translations, an MT model can learn a smoothed distribution,\nthereby reducing the risk of data sparsity. We demonstrate that the DA model,\nwhich estimates the posterior distribution, largely improves the MT\nperformance, outperforming the previous best system by 2.30 s-BLEU on News and\nachieving new state-of-the-art on News and Europarl benchmarks. Our code is\navailable at https://github.com/baoguangsheng/target-side-augmentation.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Document-level machine translation faces the challenge of data sparsity due\nto its long input length and a small amount of training data, increasing the\nrisk of learning spurious patterns. To address this challenge, we propose a\ntarget-side augmentation method, introducing a data augmentation (DA) model to\ngenerate many potential translations for each source document. Learning on\nthese wider range translations, an MT model can learn a smoothed distribution,\nthereby reducing the risk of data sparsity. We demonstrate that the DA model,\nwhich estimates the posterior distribution, largely improves the MT\nperformance, outperforming the previous best system by 2.30 s-BLEU on News and\nachieving new state-of-the-art on News and Europarl benchmarks. Our code is\navailable at https://github.com/baoguangsheng/target-side-augmentation.'}, 'authors': [{'name': 'Guangsheng Bao'}, {'name': 'Zhiyang Teng'}, {'name': 'Yue Zhang'}], 'author_detail': {'name': 'Yue Zhang'}, 'author': 'Yue Zhang', 'arxiv_comment': 'Accepted by ACL2023 main conference', 'links': [{'href': 'http://arxiv.org/abs/2305.04505v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.04505v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.04720v2,2023-05-25 11:40:59+00:00,2023-05-08 14:10:40+00:00,DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation,"[arxiv.Result.Author('ChaeHun Park'), arxiv.Result.Author('Seungil Chad Lee'), arxiv.Result.Author('Daniel Rim'), arxiv.Result.Author('Jaegul Choo')]","Despite the recent advances in open-domain dialogue systems, building a
reliable evaluation metric is still a challenging problem. Recent studies
proposed learnable metrics based on classification models trained to
distinguish the correct response. However, neural classifiers are known to make
overly confident predictions for examples from unseen distributions. We propose
DEnsity, which evaluates a response by utilizing density estimation on the
feature space derived from a neural classifier. Our metric measures how likely
a response would appear in the distribution of human conversations. Moreover,
to improve the performance of DEnsity, we utilize contrastive learning to
further compress the feature space. Experiments on multiple response evaluation
datasets show that DEnsity correlates better with human evaluations than the
existing metrics. Our code is available at https://github.com/ddehun/DEnsity.",Findings of ACL2023. 13 pages,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.04720v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.04720v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.04720v2,"{'id': 'http://arxiv.org/abs/2305.04720v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.04720v2', 'updated': '2023-05-25T11:40:59Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=25, tm_hour=11, tm_min=40, tm_sec=59, tm_wday=3, tm_yday=145, tm_isdst=0), 'published': '2023-05-08T14:10:40Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=14, tm_min=10, tm_sec=40, tm_wday=0, tm_yday=128, tm_isdst=0), 'title': 'DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation'}, 'summary': 'Despite the recent advances in open-domain dialogue systems, building a\nreliable evaluation metric is still a challenging problem. Recent studies\nproposed learnable metrics based on classification models trained to\ndistinguish the correct response. However, neural classifiers are known to make\noverly confident predictions for examples from unseen distributions. We propose\nDEnsity, which evaluates a response by utilizing density estimation on the\nfeature space derived from a neural classifier. Our metric measures how likely\na response would appear in the distribution of human conversations. Moreover,\nto improve the performance of DEnsity, we utilize contrastive learning to\nfurther compress the feature space. Experiments on multiple response evaluation\ndatasets show that DEnsity correlates better with human evaluations than the\nexisting metrics. Our code is available at https://github.com/ddehun/DEnsity.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Despite the recent advances in open-domain dialogue systems, building a\nreliable evaluation metric is still a challenging problem. Recent studies\nproposed learnable metrics based on classification models trained to\ndistinguish the correct response. However, neural classifiers are known to make\noverly confident predictions for examples from unseen distributions. We propose\nDEnsity, which evaluates a response by utilizing density estimation on the\nfeature space derived from a neural classifier. Our metric measures how likely\na response would appear in the distribution of human conversations. Moreover,\nto improve the performance of DEnsity, we utilize contrastive learning to\nfurther compress the feature space. Experiments on multiple response evaluation\ndatasets show that DEnsity correlates better with human evaluations than the\nexisting metrics. Our code is available at https://github.com/ddehun/DEnsity.'}, 'authors': [{'name': 'ChaeHun Park'}, {'name': 'Seungil Chad Lee'}, {'name': 'Daniel Rim'}, {'name': 'Jaegul Choo'}], 'author_detail': {'name': 'Jaegul Choo'}, 'author': 'Jaegul Choo', 'arxiv_comment': 'Findings of ACL2023. 13 pages', 'links': [{'href': 'http://arxiv.org/abs/2305.04720v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.04720v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.04808v2,2023-05-10 11:40:27+00:00,2023-05-08 16:08:42+00:00,CAT: A Contextualized Conceptualization and Instantiation Framework for Commonsense Reasoning,"[arxiv.Result.Author('Weiqi Wang'), arxiv.Result.Author('Tianqing Fang'), arxiv.Result.Author('Baixuan Xu'), arxiv.Result.Author('Chun Yi Louis Bo'), arxiv.Result.Author('Yangqiu Song'), arxiv.Result.Author('Lei Chen')]","Commonsense reasoning, aiming at endowing machines with a human-like ability
to make situational presumptions, is extremely challenging to generalize. For
someone who barely knows about ""meditation,"" while is knowledgeable about
""singing,"" he can still infer that ""meditation makes people relaxed"" from the
existing knowledge that ""singing makes people relaxed"" by first conceptualizing
""singing"" as a ""relaxing event"" and then instantiating that event to
""meditation."" This process, known as conceptual induction and deduction, is
fundamental to commonsense reasoning while lacking both labeled data and
methodologies to enhance commonsense modeling. To fill such a research gap, we
propose CAT (Contextualized ConceptuAlization and InsTantiation), a
semi-supervised learning framework that integrates event conceptualization and
instantiation to conceptualize commonsense knowledge bases at scale. Extensive
experiments show that our framework achieves state-of-the-art performances on
two conceptualization tasks, and the acquired abstract commonsense knowledge
can significantly improve commonsense inference modeling. Our code, data, and
fine-tuned models are publicly available at
https://github.com/HKUST-KnowComp/CAT.",ACL2023 Main Conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.04808v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.04808v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.04808v2,"{'id': 'http://arxiv.org/abs/2305.04808v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.04808v2', 'updated': '2023-05-10T11:40:27Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=10, tm_hour=11, tm_min=40, tm_sec=27, tm_wday=2, tm_yday=130, tm_isdst=0), 'published': '2023-05-08T16:08:42Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=16, tm_min=8, tm_sec=42, tm_wday=0, tm_yday=128, tm_isdst=0), 'title': 'CAT: A Contextualized Conceptualization and Instantiation Framework for\n  Commonsense Reasoning', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'CAT: A Contextualized Conceptualization and Instantiation Framework for\n  Commonsense Reasoning'}, 'summary': 'Commonsense reasoning, aiming at endowing machines with a human-like ability\nto make situational presumptions, is extremely challenging to generalize. For\nsomeone who barely knows about ""meditation,"" while is knowledgeable about\n""singing,"" he can still infer that ""meditation makes people relaxed"" from the\nexisting knowledge that ""singing makes people relaxed"" by first conceptualizing\n""singing"" as a ""relaxing event"" and then instantiating that event to\n""meditation."" This process, known as conceptual induction and deduction, is\nfundamental to commonsense reasoning while lacking both labeled data and\nmethodologies to enhance commonsense modeling. To fill such a research gap, we\npropose CAT (Contextualized ConceptuAlization and InsTantiation), a\nsemi-supervised learning framework that integrates event conceptualization and\ninstantiation to conceptualize commonsense knowledge bases at scale. Extensive\nexperiments show that our framework achieves state-of-the-art performances on\ntwo conceptualization tasks, and the acquired abstract commonsense knowledge\ncan significantly improve commonsense inference modeling. Our code, data, and\nfine-tuned models are publicly available at\nhttps://github.com/HKUST-KnowComp/CAT.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Commonsense reasoning, aiming at endowing machines with a human-like ability\nto make situational presumptions, is extremely challenging to generalize. For\nsomeone who barely knows about ""meditation,"" while is knowledgeable about\n""singing,"" he can still infer that ""meditation makes people relaxed"" from the\nexisting knowledge that ""singing makes people relaxed"" by first conceptualizing\n""singing"" as a ""relaxing event"" and then instantiating that event to\n""meditation."" This process, known as conceptual induction and deduction, is\nfundamental to commonsense reasoning while lacking both labeled data and\nmethodologies to enhance commonsense modeling. To fill such a research gap, we\npropose CAT (Contextualized ConceptuAlization and InsTantiation), a\nsemi-supervised learning framework that integrates event conceptualization and\ninstantiation to conceptualize commonsense knowledge bases at scale. Extensive\nexperiments show that our framework achieves state-of-the-art performances on\ntwo conceptualization tasks, and the acquired abstract commonsense knowledge\ncan significantly improve commonsense inference modeling. Our code, data, and\nfine-tuned models are publicly available at\nhttps://github.com/HKUST-KnowComp/CAT.'}, 'authors': [{'name': 'Weiqi Wang'}, {'name': 'Tianqing Fang'}, {'name': 'Baixuan Xu'}, {'name': 'Chun Yi Louis Bo'}, {'name': 'Yangqiu Song'}, {'name': 'Lei Chen'}], 'author_detail': {'name': 'Lei Chen'}, 'author': 'Lei Chen', 'arxiv_comment': 'ACL2023 Main Conference', 'links': [{'href': 'http://arxiv.org/abs/2305.04808v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.04808v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.04843v1,2023-05-08 16:41:08+00:00,2023-05-08 16:41:08+00:00,Reinforcement Learning for Topic Models,"[arxiv.Result.Author('Jeremy Costello'), arxiv.Result.Author('Marek Z. Reformat')]","We apply reinforcement learning techniques to topic modeling by replacing the
variational autoencoder in ProdLDA with a continuous action space reinforcement
learning policy. We train the system with a policy gradient algorithm
REINFORCE. Additionally, we introduced several modifications: modernize the
neural network architecture, weight the ELBO loss, use contextual embeddings,
and monitor the learning process via computing topic diversity and coherence
for each training step. Experiments are performed on 11 data sets. Our
unsupervised model outperforms all other unsupervised models and performs on
par with or better than most models using supervised labeling. Our model is
outperformed on certain data sets by a model using supervised labeling and
contrastive learning. We have also conducted an ablation study to provide
empirical evidence of performance improvements from changes we made to ProdLDA
and found that the reinforcement learning formulation boosts performance.","18 pages, 6 figures, Findings of ACL2023, code available at
  https://github.com/jeremy-costello/rl-for-topic-models",,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.04843v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.04843v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.04843v1,"{'id': 'http://arxiv.org/abs/2305.04843v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.04843v1', 'updated': '2023-05-08T16:41:08Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=16, tm_min=41, tm_sec=8, tm_wday=0, tm_yday=128, tm_isdst=0), 'published': '2023-05-08T16:41:08Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=16, tm_min=41, tm_sec=8, tm_wday=0, tm_yday=128, tm_isdst=0), 'title': 'Reinforcement Learning for Topic Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Reinforcement Learning for Topic Models'}, 'summary': 'We apply reinforcement learning techniques to topic modeling by replacing the\nvariational autoencoder in ProdLDA with a continuous action space reinforcement\nlearning policy. We train the system with a policy gradient algorithm\nREINFORCE. Additionally, we introduced several modifications: modernize the\nneural network architecture, weight the ELBO loss, use contextual embeddings,\nand monitor the learning process via computing topic diversity and coherence\nfor each training step. Experiments are performed on 11 data sets. Our\nunsupervised model outperforms all other unsupervised models and performs on\npar with or better than most models using supervised labeling. Our model is\noutperformed on certain data sets by a model using supervised labeling and\ncontrastive learning. We have also conducted an ablation study to provide\nempirical evidence of performance improvements from changes we made to ProdLDA\nand found that the reinforcement learning formulation boosts performance.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We apply reinforcement learning techniques to topic modeling by replacing the\nvariational autoencoder in ProdLDA with a continuous action space reinforcement\nlearning policy. We train the system with a policy gradient algorithm\nREINFORCE. Additionally, we introduced several modifications: modernize the\nneural network architecture, weight the ELBO loss, use contextual embeddings,\nand monitor the learning process via computing topic diversity and coherence\nfor each training step. Experiments are performed on 11 data sets. Our\nunsupervised model outperforms all other unsupervised models and performs on\npar with or better than most models using supervised labeling. Our model is\noutperformed on certain data sets by a model using supervised labeling and\ncontrastive learning. We have also conducted an ablation study to provide\nempirical evidence of performance improvements from changes we made to ProdLDA\nand found that the reinforcement learning formulation boosts performance.'}, 'authors': [{'name': 'Jeremy Costello'}, {'name': 'Marek Z. Reformat'}], 'author_detail': {'name': 'Marek Z. Reformat'}, 'author': 'Marek Z. Reformat', 'arxiv_comment': '18 pages, 6 figures, Findings of ACL2023, code available at\n  https://github.com/jeremy-costello/rl-for-topic-models', 'links': [{'href': 'http://arxiv.org/abs/2305.04843v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.04843v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.04971v1,2023-05-08 18:04:18+00:00,2023-05-08 18:04:18+00:00,LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization,"[arxiv.Result.Author('Peng Lu'), arxiv.Result.Author('Ahmad Rashid'), arxiv.Result.Author('Ivan Kobyzev'), arxiv.Result.Author('Mehdi Rezagholizadeh'), arxiv.Result.Author('Philippe Langlais')]","Regularization techniques are crucial to improving the generalization
performance and training efficiency of deep neural networks. Many deep learning
algorithms rely on weight decay, dropout, batch/layer normalization to converge
faster and generalize. Label Smoothing (LS) is another simple, versatile and
efficient regularization which can be applied to various supervised
classification tasks. Conventional LS, however, regardless of the training
instance assumes that each non-target class is equally likely. In this work, we
present a general framework for training with label regularization, which
includes conventional LS but can also model instance-specific variants. Based
on this formulation, we propose an efficient way of learning LAbel
regularization by devising a Bi-level Optimization (LABO) problem. We derive a
deterministic and interpretable solution of the inner loop as the optimal label
smoothing without the need to store the parameters or the output of a trained
model. Finally, we conduct extensive experiments and demonstrate our LABO
consistently yields improvement over conventional label regularization on
various fields, including seven machine translation and three image
classification tasks across various",Accepted at ACL2023 (Findings),,,cs.LG,"['cs.LG', 'cs.CL']","[arxiv.Result.Link('http://arxiv.org/abs/2305.04971v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.04971v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.04971v1,"{'id': 'http://arxiv.org/abs/2305.04971v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.04971v1', 'updated': '2023-05-08T18:04:18Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=18, tm_min=4, tm_sec=18, tm_wday=0, tm_yday=128, tm_isdst=0), 'published': '2023-05-08T18:04:18Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=8, tm_hour=18, tm_min=4, tm_sec=18, tm_wday=0, tm_yday=128, tm_isdst=0), 'title': 'LABO: Towards Learning Optimal Label Regularization via Bi-level\n  Optimization', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'LABO: Towards Learning Optimal Label Regularization via Bi-level\n  Optimization'}, 'summary': 'Regularization techniques are crucial to improving the generalization\nperformance and training efficiency of deep neural networks. Many deep learning\nalgorithms rely on weight decay, dropout, batch/layer normalization to converge\nfaster and generalize. Label Smoothing (LS) is another simple, versatile and\nefficient regularization which can be applied to various supervised\nclassification tasks. Conventional LS, however, regardless of the training\ninstance assumes that each non-target class is equally likely. In this work, we\npresent a general framework for training with label regularization, which\nincludes conventional LS but can also model instance-specific variants. Based\non this formulation, we propose an efficient way of learning LAbel\nregularization by devising a Bi-level Optimization (LABO) problem. We derive a\ndeterministic and interpretable solution of the inner loop as the optimal label\nsmoothing without the need to store the parameters or the output of a trained\nmodel. Finally, we conduct extensive experiments and demonstrate our LABO\nconsistently yields improvement over conventional label regularization on\nvarious fields, including seven machine translation and three image\nclassification tasks across various', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Regularization techniques are crucial to improving the generalization\nperformance and training efficiency of deep neural networks. Many deep learning\nalgorithms rely on weight decay, dropout, batch/layer normalization to converge\nfaster and generalize. Label Smoothing (LS) is another simple, versatile and\nefficient regularization which can be applied to various supervised\nclassification tasks. Conventional LS, however, regardless of the training\ninstance assumes that each non-target class is equally likely. In this work, we\npresent a general framework for training with label regularization, which\nincludes conventional LS but can also model instance-specific variants. Based\non this formulation, we propose an efficient way of learning LAbel\nregularization by devising a Bi-level Optimization (LABO) problem. We derive a\ndeterministic and interpretable solution of the inner loop as the optimal label\nsmoothing without the need to store the parameters or the output of a trained\nmodel. Finally, we conduct extensive experiments and demonstrate our LABO\nconsistently yields improvement over conventional label regularization on\nvarious fields, including seven machine translation and three image\nclassification tasks across various'}, 'authors': [{'name': 'Peng Lu'}, {'name': 'Ahmad Rashid'}, {'name': 'Ivan Kobyzev'}, {'name': 'Mehdi Rezagholizadeh'}, {'name': 'Philippe Langlais'}], 'author_detail': {'name': 'Philippe Langlais'}, 'author': 'Philippe Langlais', 'arxiv_comment': 'Accepted at ACL2023 (Findings)', 'links': [{'href': 'http://arxiv.org/abs/2305.04971v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.04971v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.05496v2,2024-02-24 04:23:25+00:00,2023-05-09 14:47:25+00:00,Exploiting Pseudo Image Captions for Multimodal Summarization,"[arxiv.Result.Author('Chaoya Jiang'), arxiv.Result.Author('Rui Xie'), arxiv.Result.Author('Wei Ye'), arxiv.Result.Author('Jinan Sun'), arxiv.Result.Author('Shikun Zhang')]","Cross-modal contrastive learning in vision language pretraining (VLP) faces
the challenge of (partial) false negatives. In this paper, we study this
problem from the perspective of Mutual Information (MI) optimization. It is
common sense that InfoNCE loss used in contrastive learning will maximize the
lower bound of MI between anchors and their positives, while we theoretically
prove that MI involving negatives also matters when noises commonly exist.
Guided by a more general lower bound form for optimization, we propose a
contrastive learning strategy regulated by progressively refined cross-modal
similarity, to more accurately optimize MI between an image/text anchor and its
negative texts/images instead of improperly minimizing it. Our method performs
competitively on four downstream cross-modal tasks and systematically balances
the beneficial and harmful effects of (partial) false negative samples under
theoretical guidance.",Accepted at ACL2023 Findings,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.05496v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.05496v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.05496v2,"{'id': 'http://arxiv.org/abs/2305.05496v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.05496v2', 'updated': '2024-02-24T04:23:25Z', 'updated_parsed': time.struct_time(tm_year=2024, tm_mon=2, tm_mday=24, tm_hour=4, tm_min=23, tm_sec=25, tm_wday=5, tm_yday=55, tm_isdst=0), 'published': '2023-05-09T14:47:25Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=9, tm_hour=14, tm_min=47, tm_sec=25, tm_wday=1, tm_yday=129, tm_isdst=0), 'title': 'Exploiting Pseudo Image Captions for Multimodal Summarization', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Exploiting Pseudo Image Captions for Multimodal Summarization'}, 'summary': 'Cross-modal contrastive learning in vision language pretraining (VLP) faces\nthe challenge of (partial) false negatives. In this paper, we study this\nproblem from the perspective of Mutual Information (MI) optimization. It is\ncommon sense that InfoNCE loss used in contrastive learning will maximize the\nlower bound of MI between anchors and their positives, while we theoretically\nprove that MI involving negatives also matters when noises commonly exist.\nGuided by a more general lower bound form for optimization, we propose a\ncontrastive learning strategy regulated by progressively refined cross-modal\nsimilarity, to more accurately optimize MI between an image/text anchor and its\nnegative texts/images instead of improperly minimizing it. Our method performs\ncompetitively on four downstream cross-modal tasks and systematically balances\nthe beneficial and harmful effects of (partial) false negative samples under\ntheoretical guidance.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Cross-modal contrastive learning in vision language pretraining (VLP) faces\nthe challenge of (partial) false negatives. In this paper, we study this\nproblem from the perspective of Mutual Information (MI) optimization. It is\ncommon sense that InfoNCE loss used in contrastive learning will maximize the\nlower bound of MI between anchors and their positives, while we theoretically\nprove that MI involving negatives also matters when noises commonly exist.\nGuided by a more general lower bound form for optimization, we propose a\ncontrastive learning strategy regulated by progressively refined cross-modal\nsimilarity, to more accurately optimize MI between an image/text anchor and its\nnegative texts/images instead of improperly minimizing it. Our method performs\ncompetitively on four downstream cross-modal tasks and systematically balances\nthe beneficial and harmful effects of (partial) false negative samples under\ntheoretical guidance.'}, 'authors': [{'name': 'Chaoya Jiang'}, {'name': 'Rui Xie'}, {'name': 'Wei Ye'}, {'name': 'Jinan Sun'}, {'name': 'Shikun Zhang'}], 'author_detail': {'name': 'Shikun Zhang'}, 'author': 'Shikun Zhang', 'arxiv_comment': 'Accepted at ACL2023 Findings', 'links': [{'href': 'http://arxiv.org/abs/2305.05496v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.05496v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.07565v1,2023-05-12 15:46:36+00:00,2023-05-12 15:46:36+00:00,A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information,"[arxiv.Result.Author('Vladimir Araujo'), arxiv.Result.Author('Alvaro Soto'), arxiv.Result.Author('Marie-Francine Moens')]","Existing question answering methods often assume that the input content
(e.g., documents or videos) is always accessible to solve the task.
Alternatively, memory networks were introduced to mimic the human process of
incremental comprehension and compression of the information in a
fixed-capacity memory. However, these models only learn how to maintain memory
by backpropagating errors in the answers through the entire network. Instead,
it has been suggested that humans have effective mechanisms to boost their
memorization capacities, such as rehearsal and anticipation. Drawing
inspiration from these, we propose a memory model that performs rehearsal and
anticipation while processing inputs to memorize important information for
solving question answering tasks from streaming data. The proposed mechanisms
are applied self-supervised during training through masked modeling tasks
focused on coreference information. We validate our model on a short-sequence
(bAbI) dataset as well as large-sequence textual (NarrativeQA) and video
(ActivityNet-QA) question answering datasets, where it achieves substantial
improvements over previous memory network approaches. Furthermore, our ablation
study confirms the proposed mechanisms' importance for memory models.",Accepted paper at ACL2023 Findings,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.07565v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.07565v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.07565v1,"{'id': 'http://arxiv.org/abs/2305.07565v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.07565v1', 'updated': '2023-05-12T15:46:36Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=12, tm_hour=15, tm_min=46, tm_sec=36, tm_wday=4, tm_yday=132, tm_isdst=0), 'published': '2023-05-12T15:46:36Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=12, tm_hour=15, tm_min=46, tm_sec=36, tm_wday=4, tm_yday=132, tm_isdst=0), 'title': 'A Memory Model for Question Answering from Streaming Data Supported by\n  Rehearsal and Anticipation of Coreference Information', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A Memory Model for Question Answering from Streaming Data Supported by\n  Rehearsal and Anticipation of Coreference Information'}, 'summary': ""Existing question answering methods often assume that the input content\n(e.g., documents or videos) is always accessible to solve the task.\nAlternatively, memory networks were introduced to mimic the human process of\nincremental comprehension and compression of the information in a\nfixed-capacity memory. However, these models only learn how to maintain memory\nby backpropagating errors in the answers through the entire network. Instead,\nit has been suggested that humans have effective mechanisms to boost their\nmemorization capacities, such as rehearsal and anticipation. Drawing\ninspiration from these, we propose a memory model that performs rehearsal and\nanticipation while processing inputs to memorize important information for\nsolving question answering tasks from streaming data. The proposed mechanisms\nare applied self-supervised during training through masked modeling tasks\nfocused on coreference information. We validate our model on a short-sequence\n(bAbI) dataset as well as large-sequence textual (NarrativeQA) and video\n(ActivityNet-QA) question answering datasets, where it achieves substantial\nimprovements over previous memory network approaches. Furthermore, our ablation\nstudy confirms the proposed mechanisms' importance for memory models."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Existing question answering methods often assume that the input content\n(e.g., documents or videos) is always accessible to solve the task.\nAlternatively, memory networks were introduced to mimic the human process of\nincremental comprehension and compression of the information in a\nfixed-capacity memory. However, these models only learn how to maintain memory\nby backpropagating errors in the answers through the entire network. Instead,\nit has been suggested that humans have effective mechanisms to boost their\nmemorization capacities, such as rehearsal and anticipation. Drawing\ninspiration from these, we propose a memory model that performs rehearsal and\nanticipation while processing inputs to memorize important information for\nsolving question answering tasks from streaming data. The proposed mechanisms\nare applied self-supervised during training through masked modeling tasks\nfocused on coreference information. We validate our model on a short-sequence\n(bAbI) dataset as well as large-sequence textual (NarrativeQA) and video\n(ActivityNet-QA) question answering datasets, where it achieves substantial\nimprovements over previous memory network approaches. Furthermore, our ablation\nstudy confirms the proposed mechanisms' importance for memory models.""}, 'authors': [{'name': 'Vladimir Araujo'}, {'name': 'Alvaro Soto'}, {'name': 'Marie-Francine Moens'}], 'author_detail': {'name': 'Marie-Francine Moens'}, 'author': 'Marie-Francine Moens', 'arxiv_comment': 'Accepted paper at ACL2023 Findings', 'links': [{'href': 'http://arxiv.org/abs/2305.07565v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.07565v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.08654v1,2023-05-15 13:58:21+00:00,2023-05-15 13:58:21+00:00,Unsupervised Semantic Variation Prediction using the Distribution of Sibling Embeddings,"[arxiv.Result.Author('Taichi Aida'), arxiv.Result.Author('Danushka Bollegala')]","Languages are dynamic entities, where the meanings associated with words
constantly change with time. Detecting the semantic variation of words is an
important task for various NLP applications that must make time-sensitive
predictions. Existing work on semantic variation prediction have predominantly
focused on comparing some form of an averaged contextualised representation of
a target word computed from a given corpus. However, some of the previously
associated meanings of a target word can become obsolete over time (e.g.
meaning of gay as happy), while novel usages of existing words are observed
(e.g. meaning of cell as a mobile phone). We argue that mean representations
alone cannot accurately capture such semantic variations and propose a method
that uses the entire cohort of the contextualised embeddings of the target
word, which we refer to as the sibling distribution. Experimental results on
SemEval-2020 Task 1 benchmark dataset for semantic variation prediction show
that our method outperforms prior work that consider only the mean embeddings,
and is comparable to the current state-of-the-art. Moreover, a qualitative
analysis shows that our method detects important semantic changes in words that
are not captured by the existing methods. Source code is available at
https://github.com/a1da4/svp-gauss .",Findings of ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.08654v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.08654v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.08654v1,"{'id': 'http://arxiv.org/abs/2305.08654v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.08654v1', 'updated': '2023-05-15T13:58:21Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=15, tm_hour=13, tm_min=58, tm_sec=21, tm_wday=0, tm_yday=135, tm_isdst=0), 'published': '2023-05-15T13:58:21Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=15, tm_hour=13, tm_min=58, tm_sec=21, tm_wday=0, tm_yday=135, tm_isdst=0), 'title': 'Unsupervised Semantic Variation Prediction using the Distribution of\n  Sibling Embeddings', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Unsupervised Semantic Variation Prediction using the Distribution of\n  Sibling Embeddings'}, 'summary': 'Languages are dynamic entities, where the meanings associated with words\nconstantly change with time. Detecting the semantic variation of words is an\nimportant task for various NLP applications that must make time-sensitive\npredictions. Existing work on semantic variation prediction have predominantly\nfocused on comparing some form of an averaged contextualised representation of\na target word computed from a given corpus. However, some of the previously\nassociated meanings of a target word can become obsolete over time (e.g.\nmeaning of gay as happy), while novel usages of existing words are observed\n(e.g. meaning of cell as a mobile phone). We argue that mean representations\nalone cannot accurately capture such semantic variations and propose a method\nthat uses the entire cohort of the contextualised embeddings of the target\nword, which we refer to as the sibling distribution. Experimental results on\nSemEval-2020 Task 1 benchmark dataset for semantic variation prediction show\nthat our method outperforms prior work that consider only the mean embeddings,\nand is comparable to the current state-of-the-art. Moreover, a qualitative\nanalysis shows that our method detects important semantic changes in words that\nare not captured by the existing methods. Source code is available at\nhttps://github.com/a1da4/svp-gauss .', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Languages are dynamic entities, where the meanings associated with words\nconstantly change with time. Detecting the semantic variation of words is an\nimportant task for various NLP applications that must make time-sensitive\npredictions. Existing work on semantic variation prediction have predominantly\nfocused on comparing some form of an averaged contextualised representation of\na target word computed from a given corpus. However, some of the previously\nassociated meanings of a target word can become obsolete over time (e.g.\nmeaning of gay as happy), while novel usages of existing words are observed\n(e.g. meaning of cell as a mobile phone). We argue that mean representations\nalone cannot accurately capture such semantic variations and propose a method\nthat uses the entire cohort of the contextualised embeddings of the target\nword, which we refer to as the sibling distribution. Experimental results on\nSemEval-2020 Task 1 benchmark dataset for semantic variation prediction show\nthat our method outperforms prior work that consider only the mean embeddings,\nand is comparable to the current state-of-the-art. Moreover, a qualitative\nanalysis shows that our method detects important semantic changes in words that\nare not captured by the existing methods. Source code is available at\nhttps://github.com/a1da4/svp-gauss .'}, 'authors': [{'name': 'Taichi Aida'}, {'name': 'Danushka Bollegala'}], 'author_detail': {'name': 'Danushka Bollegala'}, 'author': 'Danushka Bollegala', 'arxiv_comment': 'Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.08654v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.08654v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.09137v1,2023-05-16 03:38:06+00:00,2023-05-16 03:38:06+00:00,Pre-Training to Learn in Context,"[arxiv.Result.Author('Yuxian Gu'), arxiv.Result.Author('Li Dong'), arxiv.Result.Author('Furu Wei'), arxiv.Result.Author('Minlie Huang')]","In-context learning, where pre-trained language models learn to perform tasks
from task examples and instructions in their contexts, has attracted much
attention in the NLP community. However, the ability of in-context learning is
not fully exploited because language models are not explicitly trained to learn
in context. To this end, we propose PICL (Pre-training for In-Context
Learning), a framework to enhance the language models' in-context learning
ability by pre-training the model on a large collection of ""intrinsic tasks"" in
the general plain-text corpus using the simple language modeling objective.
PICL encourages the model to infer and perform tasks by conditioning on the
contexts while maintaining task generalization of pre-trained models. We
evaluate the in-context learning performance of the model trained with PICL on
seven widely-used text classification datasets and the Super-NaturalInstrctions
benchmark, which contains 100+ NLP tasks formulated to text generation. Our
experiments show that PICL is more effective and task-generalizable than a
range of baselines, outperforming larger language models with nearly 4x
parameters. The code is publicly available at https://github.com/thu-coai/PICL.",ACL2023 Main Conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.09137v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.09137v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.09137v1,"{'id': 'http://arxiv.org/abs/2305.09137v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.09137v1', 'updated': '2023-05-16T03:38:06Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=16, tm_hour=3, tm_min=38, tm_sec=6, tm_wday=1, tm_yday=136, tm_isdst=0), 'published': '2023-05-16T03:38:06Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=16, tm_hour=3, tm_min=38, tm_sec=6, tm_wday=1, tm_yday=136, tm_isdst=0), 'title': 'Pre-Training to Learn in Context', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Pre-Training to Learn in Context'}, 'summary': 'In-context learning, where pre-trained language models learn to perform tasks\nfrom task examples and instructions in their contexts, has attracted much\nattention in the NLP community. However, the ability of in-context learning is\nnot fully exploited because language models are not explicitly trained to learn\nin context. To this end, we propose PICL (Pre-training for In-Context\nLearning), a framework to enhance the language models\' in-context learning\nability by pre-training the model on a large collection of ""intrinsic tasks"" in\nthe general plain-text corpus using the simple language modeling objective.\nPICL encourages the model to infer and perform tasks by conditioning on the\ncontexts while maintaining task generalization of pre-trained models. We\nevaluate the in-context learning performance of the model trained with PICL on\nseven widely-used text classification datasets and the Super-NaturalInstrctions\nbenchmark, which contains 100+ NLP tasks formulated to text generation. Our\nexperiments show that PICL is more effective and task-generalizable than a\nrange of baselines, outperforming larger language models with nearly 4x\nparameters. The code is publicly available at https://github.com/thu-coai/PICL.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'In-context learning, where pre-trained language models learn to perform tasks\nfrom task examples and instructions in their contexts, has attracted much\nattention in the NLP community. However, the ability of in-context learning is\nnot fully exploited because language models are not explicitly trained to learn\nin context. To this end, we propose PICL (Pre-training for In-Context\nLearning), a framework to enhance the language models\' in-context learning\nability by pre-training the model on a large collection of ""intrinsic tasks"" in\nthe general plain-text corpus using the simple language modeling objective.\nPICL encourages the model to infer and perform tasks by conditioning on the\ncontexts while maintaining task generalization of pre-trained models. We\nevaluate the in-context learning performance of the model trained with PICL on\nseven widely-used text classification datasets and the Super-NaturalInstrctions\nbenchmark, which contains 100+ NLP tasks formulated to text generation. Our\nexperiments show that PICL is more effective and task-generalizable than a\nrange of baselines, outperforming larger language models with nearly 4x\nparameters. The code is publicly available at https://github.com/thu-coai/PICL.'}, 'authors': [{'name': 'Yuxian Gu'}, {'name': 'Li Dong'}, {'name': 'Furu Wei'}, {'name': 'Minlie Huang'}], 'author_detail': {'name': 'Minlie Huang'}, 'author': 'Minlie Huang', 'arxiv_comment': 'ACL2023 Main Conference', 'links': [{'href': 'http://arxiv.org/abs/2305.09137v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.09137v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.09892v1,2023-05-17 02:06:47+00:00,2023-05-17 02:06:47+00:00,Clustering-Aware Negative Sampling for Unsupervised Sentence Representation,"[arxiv.Result.Author('Jinghao Deng'), arxiv.Result.Author('Fanqi Wan'), arxiv.Result.Author('Tao Yang'), arxiv.Result.Author('Xiaojun Quan'), arxiv.Result.Author('Rui Wang')]","Contrastive learning has been widely studied in sentence representation
learning. However, earlier works mainly focus on the construction of positive
examples, while in-batch samples are often simply treated as negative examples.
This approach overlooks the importance of selecting appropriate negative
examples, potentially leading to a scarcity of hard negatives and the inclusion
of false negatives. To address these issues, we propose ClusterNS
(Clustering-aware Negative Sampling), a novel method that incorporates cluster
information into contrastive learning for unsupervised sentence representation
learning. We apply a modified K-means clustering algorithm to supply hard
negatives and recognize in-batch false negatives during training, aiming to
solve the two issues in one unified framework. Experiments on semantic textual
similarity (STS) tasks demonstrate that our proposed ClusterNS compares
favorably with baselines in unsupervised sentence representation learning. Our
code has been made publicly available.","accepted to Finding of ACL2023, 16 pages",,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.09892v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.09892v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.09892v1,"{'id': 'http://arxiv.org/abs/2305.09892v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.09892v1', 'updated': '2023-05-17T02:06:47Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=17, tm_hour=2, tm_min=6, tm_sec=47, tm_wday=2, tm_yday=137, tm_isdst=0), 'published': '2023-05-17T02:06:47Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=17, tm_hour=2, tm_min=6, tm_sec=47, tm_wday=2, tm_yday=137, tm_isdst=0), 'title': 'Clustering-Aware Negative Sampling for Unsupervised Sentence\n  Representation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Clustering-Aware Negative Sampling for Unsupervised Sentence\n  Representation'}, 'summary': 'Contrastive learning has been widely studied in sentence representation\nlearning. However, earlier works mainly focus on the construction of positive\nexamples, while in-batch samples are often simply treated as negative examples.\nThis approach overlooks the importance of selecting appropriate negative\nexamples, potentially leading to a scarcity of hard negatives and the inclusion\nof false negatives. To address these issues, we propose ClusterNS\n(Clustering-aware Negative Sampling), a novel method that incorporates cluster\ninformation into contrastive learning for unsupervised sentence representation\nlearning. We apply a modified K-means clustering algorithm to supply hard\nnegatives and recognize in-batch false negatives during training, aiming to\nsolve the two issues in one unified framework. Experiments on semantic textual\nsimilarity (STS) tasks demonstrate that our proposed ClusterNS compares\nfavorably with baselines in unsupervised sentence representation learning. Our\ncode has been made publicly available.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Contrastive learning has been widely studied in sentence representation\nlearning. However, earlier works mainly focus on the construction of positive\nexamples, while in-batch samples are often simply treated as negative examples.\nThis approach overlooks the importance of selecting appropriate negative\nexamples, potentially leading to a scarcity of hard negatives and the inclusion\nof false negatives. To address these issues, we propose ClusterNS\n(Clustering-aware Negative Sampling), a novel method that incorporates cluster\ninformation into contrastive learning for unsupervised sentence representation\nlearning. We apply a modified K-means clustering algorithm to supply hard\nnegatives and recognize in-batch false negatives during training, aiming to\nsolve the two issues in one unified framework. Experiments on semantic textual\nsimilarity (STS) tasks demonstrate that our proposed ClusterNS compares\nfavorably with baselines in unsupervised sentence representation learning. Our\ncode has been made publicly available.'}, 'authors': [{'name': 'Jinghao Deng'}, {'name': 'Fanqi Wan'}, {'name': 'Tao Yang'}, {'name': 'Xiaojun Quan'}, {'name': 'Rui Wang'}], 'author_detail': {'name': 'Rui Wang'}, 'author': 'Rui Wang', 'arxiv_comment': 'accepted to Finding of ACL2023, 16 pages', 'links': [{'href': 'http://arxiv.org/abs/2305.09892v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.09892v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.10231v1,2023-05-17 14:12:29+00:00,2023-05-17 14:12:29+00:00,"OpenSLU: A Unified, Modularized, and Extensible Toolkit for Spoken Language Understanding","[arxiv.Result.Author('Libo Qin'), arxiv.Result.Author('Qiguang Chen'), arxiv.Result.Author('Xiao Xu'), arxiv.Result.Author('Yunlong Feng'), arxiv.Result.Author('Wanxiang Che')]","Spoken Language Understanding (SLU) is one of the core components of a
task-oriented dialogue system, which aims to extract the semantic meaning of
user queries (e.g., intents and slots). In this work, we introduce OpenSLU, an
open-source toolkit to provide a unified, modularized, and extensible toolkit
for spoken language understanding. Specifically, OpenSLU unifies 10 SLU models
for both single-intent and multi-intent scenarios, which support both
non-pretrained and pretrained models simultaneously. Additionally, OpenSLU is
highly modularized and extensible by decomposing the model architecture,
inference, and learning process into reusable modules, which allows researchers
to quickly set up SLU experiments with highly flexible configurations. OpenSLU
is implemented based on PyTorch, and released at
\url{https://github.com/LightChen233/OpenSLU}.",ACL2023 Demo Paper,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.10231v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.10231v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.10231v1,"{'id': 'http://arxiv.org/abs/2305.10231v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.10231v1', 'updated': '2023-05-17T14:12:29Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=17, tm_hour=14, tm_min=12, tm_sec=29, tm_wday=2, tm_yday=137, tm_isdst=0), 'published': '2023-05-17T14:12:29Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=17, tm_hour=14, tm_min=12, tm_sec=29, tm_wday=2, tm_yday=137, tm_isdst=0), 'title': 'OpenSLU: A Unified, Modularized, and Extensible Toolkit for Spoken\n  Language Understanding', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'OpenSLU: A Unified, Modularized, and Extensible Toolkit for Spoken\n  Language Understanding'}, 'summary': 'Spoken Language Understanding (SLU) is one of the core components of a\ntask-oriented dialogue system, which aims to extract the semantic meaning of\nuser queries (e.g., intents and slots). In this work, we introduce OpenSLU, an\nopen-source toolkit to provide a unified, modularized, and extensible toolkit\nfor spoken language understanding. Specifically, OpenSLU unifies 10 SLU models\nfor both single-intent and multi-intent scenarios, which support both\nnon-pretrained and pretrained models simultaneously. Additionally, OpenSLU is\nhighly modularized and extensible by decomposing the model architecture,\ninference, and learning process into reusable modules, which allows researchers\nto quickly set up SLU experiments with highly flexible configurations. OpenSLU\nis implemented based on PyTorch, and released at\n\\url{https://github.com/LightChen233/OpenSLU}.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Spoken Language Understanding (SLU) is one of the core components of a\ntask-oriented dialogue system, which aims to extract the semantic meaning of\nuser queries (e.g., intents and slots). In this work, we introduce OpenSLU, an\nopen-source toolkit to provide a unified, modularized, and extensible toolkit\nfor spoken language understanding. Specifically, OpenSLU unifies 10 SLU models\nfor both single-intent and multi-intent scenarios, which support both\nnon-pretrained and pretrained models simultaneously. Additionally, OpenSLU is\nhighly modularized and extensible by decomposing the model architecture,\ninference, and learning process into reusable modules, which allows researchers\nto quickly set up SLU experiments with highly flexible configurations. OpenSLU\nis implemented based on PyTorch, and released at\n\\url{https://github.com/LightChen233/OpenSLU}.'}, 'authors': [{'name': 'Libo Qin'}, {'name': 'Qiguang Chen'}, {'name': 'Xiao Xu'}, {'name': 'Yunlong Feng'}, {'name': 'Wanxiang Che'}], 'author_detail': {'name': 'Wanxiang Che'}, 'author': 'Wanxiang Che', 'arxiv_comment': 'ACL2023 Demo Paper', 'links': [{'href': 'http://arxiv.org/abs/2305.10231v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.10231v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.10496v1,2023-05-17 18:05:49+00:00,2023-05-17 18:05:49+00:00,Incorporating Attribution Importance for Improving Faithfulness Metrics,"[arxiv.Result.Author('Zhixue Zhao'), arxiv.Result.Author('Nikolaos Aletras')]","Feature attribution methods (FAs) are popular approaches for providing
insights into the model reasoning process of making predictions. The more
faithful a FA is, the more accurately it reflects which parts of the input are
more important for the prediction. Widely used faithfulness metrics, such as
sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely
removing or retaining the top most important tokens ranked by a given FA and
observing the changes in predictive likelihood. However, this hard criterion
ignores the importance of each individual token, treating them all equally for
computing sufficiency and comprehensiveness. In this paper, we propose a simple
yet effective soft erasure criterion. Instead of entirely removing or retaining
tokens from the input, we randomly mask parts of the token vector
representations proportionately to their FA importance. Extensive experiments
across various natural language processing tasks and different FAs show that
our soft-sufficiency and soft-comprehensiveness metrics consistently prefer
more faithful explanations compared to hard sufficiency and comprehensiveness.
Our code: https://github.com/casszhao/SoftFaith",Accepted at ACL2023,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.10496v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.10496v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.10496v1,"{'id': 'http://arxiv.org/abs/2305.10496v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.10496v1', 'updated': '2023-05-17T18:05:49Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=17, tm_hour=18, tm_min=5, tm_sec=49, tm_wday=2, tm_yday=137, tm_isdst=0), 'published': '2023-05-17T18:05:49Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=17, tm_hour=18, tm_min=5, tm_sec=49, tm_wday=2, tm_yday=137, tm_isdst=0), 'title': 'Incorporating Attribution Importance for Improving Faithfulness Metrics', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Incorporating Attribution Importance for Improving Faithfulness Metrics'}, 'summary': 'Feature attribution methods (FAs) are popular approaches for providing\ninsights into the model reasoning process of making predictions. The more\nfaithful a FA is, the more accurately it reflects which parts of the input are\nmore important for the prediction. Widely used faithfulness metrics, such as\nsufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely\nremoving or retaining the top most important tokens ranked by a given FA and\nobserving the changes in predictive likelihood. However, this hard criterion\nignores the importance of each individual token, treating them all equally for\ncomputing sufficiency and comprehensiveness. In this paper, we propose a simple\nyet effective soft erasure criterion. Instead of entirely removing or retaining\ntokens from the input, we randomly mask parts of the token vector\nrepresentations proportionately to their FA importance. Extensive experiments\nacross various natural language processing tasks and different FAs show that\nour soft-sufficiency and soft-comprehensiveness metrics consistently prefer\nmore faithful explanations compared to hard sufficiency and comprehensiveness.\nOur code: https://github.com/casszhao/SoftFaith', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Feature attribution methods (FAs) are popular approaches for providing\ninsights into the model reasoning process of making predictions. The more\nfaithful a FA is, the more accurately it reflects which parts of the input are\nmore important for the prediction. Widely used faithfulness metrics, such as\nsufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely\nremoving or retaining the top most important tokens ranked by a given FA and\nobserving the changes in predictive likelihood. However, this hard criterion\nignores the importance of each individual token, treating them all equally for\ncomputing sufficiency and comprehensiveness. In this paper, we propose a simple\nyet effective soft erasure criterion. Instead of entirely removing or retaining\ntokens from the input, we randomly mask parts of the token vector\nrepresentations proportionately to their FA importance. Extensive experiments\nacross various natural language processing tasks and different FAs show that\nour soft-sufficiency and soft-comprehensiveness metrics consistently prefer\nmore faithful explanations compared to hard sufficiency and comprehensiveness.\nOur code: https://github.com/casszhao/SoftFaith'}, 'authors': [{'name': 'Zhixue Zhao'}, {'name': 'Nikolaos Aletras'}], 'author_detail': {'name': 'Nikolaos Aletras'}, 'author': 'Nikolaos Aletras', 'arxiv_comment': 'Accepted at ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.10496v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.10496v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.10928v1,2023-05-18 12:40:41+00:00,2023-05-18 12:40:41+00:00,Multilingual Event Extraction from Historical Newspaper Adverts,"[arxiv.Result.Author('Nadav Borenstein'), arxiv.Result.Author('Natalia da Silva Perez'), arxiv.Result.Author('Isabelle Augenstein')]","NLP methods can aid historians in analyzing textual materials in greater
volumes than manually feasible. Developing such methods poses substantial
challenges though. First, acquiring large, annotated historical datasets is
difficult, as only domain experts can reliably label them. Second, most
available off-the-shelf NLP models are trained on modern language texts,
rendering them significantly less effective when applied to historical corpora.
This is particularly problematic for less well studied tasks, and for languages
other than English. This paper addresses these challenges while focusing on the
under-explored task of event extraction from a novel domain of historical
texts. We introduce a new multilingual dataset in English, French, and Dutch
composed of newspaper ads from the early modern colonial period reporting on
enslaved people who liberated themselves from enslavement. We find that: 1)
even with scarce annotated data, it is possible to achieve surprisingly good
results by formulating the problem as an extractive QA task and leveraging
existing datasets and models for modern languages; and 2) cross-lingual
low-resource learning for historical languages is highly challenging, and
machine translation of the historical datasets to the considered target
languages is, in practice, often the best-performing solution.",Accepted to the main track of ACL2023,,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.10928v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.10928v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.10928v1,"{'id': 'http://arxiv.org/abs/2305.10928v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.10928v1', 'updated': '2023-05-18T12:40:41Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=18, tm_hour=12, tm_min=40, tm_sec=41, tm_wday=3, tm_yday=138, tm_isdst=0), 'published': '2023-05-18T12:40:41Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=18, tm_hour=12, tm_min=40, tm_sec=41, tm_wday=3, tm_yday=138, tm_isdst=0), 'title': 'Multilingual Event Extraction from Historical Newspaper Adverts', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Multilingual Event Extraction from Historical Newspaper Adverts'}, 'summary': 'NLP methods can aid historians in analyzing textual materials in greater\nvolumes than manually feasible. Developing such methods poses substantial\nchallenges though. First, acquiring large, annotated historical datasets is\ndifficult, as only domain experts can reliably label them. Second, most\navailable off-the-shelf NLP models are trained on modern language texts,\nrendering them significantly less effective when applied to historical corpora.\nThis is particularly problematic for less well studied tasks, and for languages\nother than English. This paper addresses these challenges while focusing on the\nunder-explored task of event extraction from a novel domain of historical\ntexts. We introduce a new multilingual dataset in English, French, and Dutch\ncomposed of newspaper ads from the early modern colonial period reporting on\nenslaved people who liberated themselves from enslavement. We find that: 1)\neven with scarce annotated data, it is possible to achieve surprisingly good\nresults by formulating the problem as an extractive QA task and leveraging\nexisting datasets and models for modern languages; and 2) cross-lingual\nlow-resource learning for historical languages is highly challenging, and\nmachine translation of the historical datasets to the considered target\nlanguages is, in practice, often the best-performing solution.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'NLP methods can aid historians in analyzing textual materials in greater\nvolumes than manually feasible. Developing such methods poses substantial\nchallenges though. First, acquiring large, annotated historical datasets is\ndifficult, as only domain experts can reliably label them. Second, most\navailable off-the-shelf NLP models are trained on modern language texts,\nrendering them significantly less effective when applied to historical corpora.\nThis is particularly problematic for less well studied tasks, and for languages\nother than English. This paper addresses these challenges while focusing on the\nunder-explored task of event extraction from a novel domain of historical\ntexts. We introduce a new multilingual dataset in English, French, and Dutch\ncomposed of newspaper ads from the early modern colonial period reporting on\nenslaved people who liberated themselves from enslavement. We find that: 1)\neven with scarce annotated data, it is possible to achieve surprisingly good\nresults by formulating the problem as an extractive QA task and leveraging\nexisting datasets and models for modern languages; and 2) cross-lingual\nlow-resource learning for historical languages is highly challenging, and\nmachine translation of the historical datasets to the considered target\nlanguages is, in practice, often the best-performing solution.'}, 'authors': [{'name': 'Nadav Borenstein'}, {'name': 'Natalia da Silva Perez'}, {'name': 'Isabelle Augenstein'}], 'author_detail': {'name': 'Isabelle Augenstein'}, 'author': 'Isabelle Augenstein', 'arxiv_comment': 'Accepted to the main track of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.10928v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.10928v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.11255v4,2023-06-09 01:27:58+00:00,2023-05-18 18:38:32+00:00,Reasoning Implicit Sentiment with Chain-of-Thought Prompting,"[arxiv.Result.Author('Hao Fei'), arxiv.Result.Author('Bobo Li'), arxiv.Result.Author('Qian Liu'), arxiv.Result.Author('Lidong Bing'), arxiv.Result.Author('Fei Li'), arxiv.Result.Author('Tat-Seng Chua')]","While sentiment analysis systems try to determine the sentiment polarities of
given targets based on the key opinion expressions in input texts, in implicit
sentiment analysis (ISA) the opinion cues come in an implicit and obscure
manner. Thus detecting implicit sentiment requires the common-sense and
multi-hop reasoning ability to infer the latent intent of opinion. Inspired by
the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop
Reasoning (THOR) CoT framework to mimic the human-like reasoning process for
ISA. We design a three-step prompting principle for THOR to step-by-step induce
the implicit aspect, opinion, and finally the sentiment polarity. Our
THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on
supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%
F1 on zero-shot setting. Our code is open at
https://github.com/scofield7419/THOR-ISA.",ACL2023 Short Paper,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.11255v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.11255v4', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.11255v4,"{'id': 'http://arxiv.org/abs/2305.11255v4', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.11255v4', 'updated': '2023-06-09T01:27:58Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=9, tm_hour=1, tm_min=27, tm_sec=58, tm_wday=4, tm_yday=160, tm_isdst=0), 'published': '2023-05-18T18:38:32Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=18, tm_hour=18, tm_min=38, tm_sec=32, tm_wday=3, tm_yday=138, tm_isdst=0), 'title': 'Reasoning Implicit Sentiment with Chain-of-Thought Prompting', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Reasoning Implicit Sentiment with Chain-of-Thought Prompting'}, 'summary': 'While sentiment analysis systems try to determine the sentiment polarities of\ngiven targets based on the key opinion expressions in input texts, in implicit\nsentiment analysis (ISA) the opinion cues come in an implicit and obscure\nmanner. Thus detecting implicit sentiment requires the common-sense and\nmulti-hop reasoning ability to infer the latent intent of opinion. Inspired by\nthe recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop\nReasoning (THOR) CoT framework to mimic the human-like reasoning process for\nISA. We design a three-step prompting principle for THOR to step-by-step induce\nthe implicit aspect, opinion, and finally the sentiment polarity. Our\nTHOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on\nsupervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%\nF1 on zero-shot setting. Our code is open at\nhttps://github.com/scofield7419/THOR-ISA.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'While sentiment analysis systems try to determine the sentiment polarities of\ngiven targets based on the key opinion expressions in input texts, in implicit\nsentiment analysis (ISA) the opinion cues come in an implicit and obscure\nmanner. Thus detecting implicit sentiment requires the common-sense and\nmulti-hop reasoning ability to infer the latent intent of opinion. Inspired by\nthe recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop\nReasoning (THOR) CoT framework to mimic the human-like reasoning process for\nISA. We design a three-step prompting principle for THOR to step-by-step induce\nthe implicit aspect, opinion, and finally the sentiment polarity. Our\nTHOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on\nsupervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%\nF1 on zero-shot setting. Our code is open at\nhttps://github.com/scofield7419/THOR-ISA.'}, 'authors': [{'name': 'Hao Fei'}, {'name': 'Bobo Li'}, {'name': 'Qian Liu'}, {'name': 'Lidong Bing'}, {'name': 'Fei Li'}, {'name': 'Tat-Seng Chua'}], 'author_detail': {'name': 'Tat-Seng Chua'}, 'author': 'Tat-Seng Chua', 'arxiv_comment': 'ACL2023 Short Paper', 'links': [{'href': 'http://arxiv.org/abs/2305.11255v4', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.11255v4', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.11449v1,2023-05-19 06:04:21+00:00,2023-05-19 06:04:21+00:00,Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer with Fine-tuning Slow and Fast,"[arxiv.Result.Author('Yiduo Guo'), arxiv.Result.Author('Yaobo Liang'), arxiv.Result.Author('Dongyan Zhao'), arxiv.Result.Author('Bing Liu'), arxiv.Result.Author('Duan Nan')]","Existing research has shown that a multilingual pre-trained language model
fine-tuned with one (source) language also performs well on downstream tasks
for non-source languages, even though no fine-tuning is done on these
languages. However, there is a clear gap between the performance of the source
language and that of the non-source languages. This paper analyzes the
fine-tuning process, discovers when the performance gap changes and identifies
which network weights affect the overall performance most. Additionally, the
paper seeks to answer to what extent the gap can be reduced by reducing
forgetting. Based on the analysis results, a method named Fine-tuning slow and
fast with four training policies is proposed to address these issues.
Experimental results show the proposed method outperforms baselines by a clear
margin.",Accepted by ACL2023 (Long paper),,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.11449v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.11449v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.11449v1,"{'id': 'http://arxiv.org/abs/2305.11449v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.11449v1', 'updated': '2023-05-19T06:04:21Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=19, tm_hour=6, tm_min=4, tm_sec=21, tm_wday=4, tm_yday=139, tm_isdst=0), 'published': '2023-05-19T06:04:21Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=19, tm_hour=6, tm_min=4, tm_sec=21, tm_wday=4, tm_yday=139, tm_isdst=0), 'title': 'Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer\n  with Fine-tuning Slow and Fast', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Analyzing and Reducing the Performance Gap in Cross-Lingual Transfer\n  with Fine-tuning Slow and Fast'}, 'summary': 'Existing research has shown that a multilingual pre-trained language model\nfine-tuned with one (source) language also performs well on downstream tasks\nfor non-source languages, even though no fine-tuning is done on these\nlanguages. However, there is a clear gap between the performance of the source\nlanguage and that of the non-source languages. This paper analyzes the\nfine-tuning process, discovers when the performance gap changes and identifies\nwhich network weights affect the overall performance most. Additionally, the\npaper seeks to answer to what extent the gap can be reduced by reducing\nforgetting. Based on the analysis results, a method named Fine-tuning slow and\nfast with four training policies is proposed to address these issues.\nExperimental results show the proposed method outperforms baselines by a clear\nmargin.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Existing research has shown that a multilingual pre-trained language model\nfine-tuned with one (source) language also performs well on downstream tasks\nfor non-source languages, even though no fine-tuning is done on these\nlanguages. However, there is a clear gap between the performance of the source\nlanguage and that of the non-source languages. This paper analyzes the\nfine-tuning process, discovers when the performance gap changes and identifies\nwhich network weights affect the overall performance most. Additionally, the\npaper seeks to answer to what extent the gap can be reduced by reducing\nforgetting. Based on the analysis results, a method named Fine-tuning slow and\nfast with four training policies is proposed to address these issues.\nExperimental results show the proposed method outperforms baselines by a clear\nmargin.'}, 'authors': [{'name': 'Yiduo Guo'}, {'name': 'Yaobo Liang'}, {'name': 'Dongyan Zhao'}, {'name': 'Bing Liu'}, {'name': 'Duan Nan'}], 'author_detail': {'name': 'Duan Nan'}, 'author': 'Duan Nan', 'arxiv_comment': 'Accepted by ACL2023 (Long paper)', 'links': [{'href': 'http://arxiv.org/abs/2305.11449v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.11449v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.12092v1,2023-05-20 04:50:20+00:00,2023-05-20 04:50:20+00:00,ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain,"[arxiv.Result.Author('Mike Zhang'), arxiv.Result.Author('Rob van der Goot'), arxiv.Result.Author('Barbara Plank')]","The increasing number of benchmarks for Natural Language Processing (NLP)
tasks in the computational job market domain highlights the demand for methods
that can handle job-related tasks such as skill extraction, skill
classification, job title classification, and de-identification. While some
approaches have been developed that are specific to the job market domain,
there is a lack of generalized, multilingual models and benchmarks for these
tasks. In this study, we introduce a language model called ESCOXLM-R, based on
XLM-R, which uses domain-adaptive pre-training on the European Skills,
Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27
languages. The pre-training objectives for ESCOXLM-R include dynamic masked
language modeling and a novel additional objective for inducing multilingual
taxonomical ESCO relations. We comprehensively evaluate the performance of
ESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and
find that it achieves state-of-the-art results on 6 out of 9 datasets. Our
analysis reveals that ESCOXLM-R performs better on short spans and outperforms
XLM-R on entity-level and surface-level span-F1, likely due to ESCO containing
short skill and occupation titles, and encoding information on the
entity-level.",Accepted at ACL2023 (Main),,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.12092v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.12092v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.12092v1,"{'id': 'http://arxiv.org/abs/2305.12092v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.12092v1', 'updated': '2023-05-20T04:50:20Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=20, tm_hour=4, tm_min=50, tm_sec=20, tm_wday=5, tm_yday=140, tm_isdst=0), 'published': '2023-05-20T04:50:20Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=20, tm_hour=4, tm_min=50, tm_sec=20, tm_wday=5, tm_yday=140, tm_isdst=0), 'title': 'ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market\n  Domain', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market\n  Domain'}, 'summary': 'The increasing number of benchmarks for Natural Language Processing (NLP)\ntasks in the computational job market domain highlights the demand for methods\nthat can handle job-related tasks such as skill extraction, skill\nclassification, job title classification, and de-identification. While some\napproaches have been developed that are specific to the job market domain,\nthere is a lack of generalized, multilingual models and benchmarks for these\ntasks. In this study, we introduce a language model called ESCOXLM-R, based on\nXLM-R, which uses domain-adaptive pre-training on the European Skills,\nCompetences, Qualifications and Occupations (ESCO) taxonomy, covering 27\nlanguages. The pre-training objectives for ESCOXLM-R include dynamic masked\nlanguage modeling and a novel additional objective for inducing multilingual\ntaxonomical ESCO relations. We comprehensively evaluate the performance of\nESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and\nfind that it achieves state-of-the-art results on 6 out of 9 datasets. Our\nanalysis reveals that ESCOXLM-R performs better on short spans and outperforms\nXLM-R on entity-level and surface-level span-F1, likely due to ESCO containing\nshort skill and occupation titles, and encoding information on the\nentity-level.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The increasing number of benchmarks for Natural Language Processing (NLP)\ntasks in the computational job market domain highlights the demand for methods\nthat can handle job-related tasks such as skill extraction, skill\nclassification, job title classification, and de-identification. While some\napproaches have been developed that are specific to the job market domain,\nthere is a lack of generalized, multilingual models and benchmarks for these\ntasks. In this study, we introduce a language model called ESCOXLM-R, based on\nXLM-R, which uses domain-adaptive pre-training on the European Skills,\nCompetences, Qualifications and Occupations (ESCO) taxonomy, covering 27\nlanguages. The pre-training objectives for ESCOXLM-R include dynamic masked\nlanguage modeling and a novel additional objective for inducing multilingual\ntaxonomical ESCO relations. We comprehensively evaluate the performance of\nESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and\nfind that it achieves state-of-the-art results on 6 out of 9 datasets. Our\nanalysis reveals that ESCOXLM-R performs better on short spans and outperforms\nXLM-R on entity-level and surface-level span-F1, likely due to ESCO containing\nshort skill and occupation titles, and encoding information on the\nentity-level.'}, 'authors': [{'name': 'Mike Zhang'}, {'name': 'Rob van der Goot'}, {'name': 'Barbara Plank'}], 'author_detail': {'name': 'Barbara Plank'}, 'author': 'Barbara Plank', 'arxiv_comment': 'Accepted at ACL2023 (Main)', 'links': [{'href': 'http://arxiv.org/abs/2305.12092v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.12092v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.12228v1,2023-05-20 16:41:48+00:00,2023-05-20 16:41:48+00:00,Dynamic Transformers Provide a False Sense of Efficiency,"[arxiv.Result.Author('Yiming Chen'), arxiv.Result.Author('Simin Chen'), arxiv.Result.Author('Zexin Li'), arxiv.Result.Author('Wei Yang'), arxiv.Result.Author('Cong Liu'), arxiv.Result.Author('Robby T. Tan'), arxiv.Result.Author('Haizhou Li')]","Despite much success in natural language processing (NLP), pre-trained
language models typically lead to a high computational cost during inference.
Multi-exit is a mainstream approach to address this issue by making a trade-off
between efficiency and accuracy, where the saving of computation comes from an
early exit. However, whether such saving from early-exiting is robust remains
unknown. Motivated by this, we first show that directly adapting existing
adversarial attack approaches targeting model accuracy cannot significantly
reduce inference efficiency. To this end, we propose a simple yet effective
attacking framework, SAME, a novel slowdown attack framework on multi-exit
models, which is specially tailored to reduce the efficiency of the multi-exit
models. By leveraging the multi-exit models' design characteristics, we utilize
all internal predictions to guide the adversarial sample generation instead of
merely considering the final prediction. Experiments on the GLUE benchmark show
that SAME can effectively diminish the efficiency gain of various multi-exit
models by 80% on average, convincingly validating its effectiveness and
generalization ability.",Accepted by ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.12228v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.12228v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.12228v1,"{'id': 'http://arxiv.org/abs/2305.12228v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.12228v1', 'updated': '2023-05-20T16:41:48Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=20, tm_hour=16, tm_min=41, tm_sec=48, tm_wday=5, tm_yday=140, tm_isdst=0), 'published': '2023-05-20T16:41:48Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=20, tm_hour=16, tm_min=41, tm_sec=48, tm_wday=5, tm_yday=140, tm_isdst=0), 'title': 'Dynamic Transformers Provide a False Sense of Efficiency', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Dynamic Transformers Provide a False Sense of Efficiency'}, 'summary': ""Despite much success in natural language processing (NLP), pre-trained\nlanguage models typically lead to a high computational cost during inference.\nMulti-exit is a mainstream approach to address this issue by making a trade-off\nbetween efficiency and accuracy, where the saving of computation comes from an\nearly exit. However, whether such saving from early-exiting is robust remains\nunknown. Motivated by this, we first show that directly adapting existing\nadversarial attack approaches targeting model accuracy cannot significantly\nreduce inference efficiency. To this end, we propose a simple yet effective\nattacking framework, SAME, a novel slowdown attack framework on multi-exit\nmodels, which is specially tailored to reduce the efficiency of the multi-exit\nmodels. By leveraging the multi-exit models' design characteristics, we utilize\nall internal predictions to guide the adversarial sample generation instead of\nmerely considering the final prediction. Experiments on the GLUE benchmark show\nthat SAME can effectively diminish the efficiency gain of various multi-exit\nmodels by 80% on average, convincingly validating its effectiveness and\ngeneralization ability."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Despite much success in natural language processing (NLP), pre-trained\nlanguage models typically lead to a high computational cost during inference.\nMulti-exit is a mainstream approach to address this issue by making a trade-off\nbetween efficiency and accuracy, where the saving of computation comes from an\nearly exit. However, whether such saving from early-exiting is robust remains\nunknown. Motivated by this, we first show that directly adapting existing\nadversarial attack approaches targeting model accuracy cannot significantly\nreduce inference efficiency. To this end, we propose a simple yet effective\nattacking framework, SAME, a novel slowdown attack framework on multi-exit\nmodels, which is specially tailored to reduce the efficiency of the multi-exit\nmodels. By leveraging the multi-exit models' design characteristics, we utilize\nall internal predictions to guide the adversarial sample generation instead of\nmerely considering the final prediction. Experiments on the GLUE benchmark show\nthat SAME can effectively diminish the efficiency gain of various multi-exit\nmodels by 80% on average, convincingly validating its effectiveness and\ngeneralization ability.""}, 'authors': [{'name': 'Yiming Chen'}, {'name': 'Simin Chen'}, {'name': 'Zexin Li'}, {'name': 'Wei Yang'}, {'name': 'Cong Liu'}, {'name': 'Robby T. Tan'}, {'name': 'Haizhou Li'}], 'author_detail': {'name': 'Haizhou Li'}, 'author': 'Haizhou Li', 'arxiv_comment': 'Accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.12228v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.12228v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.12761v1,2023-05-22 06:31:29+00:00,2023-05-22 06:31:29+00:00,Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer,"[arxiv.Result.Author('Shuang Li'), arxiv.Result.Author('Xuming Hu'), arxiv.Result.Author('Aiwei Liu'), arxiv.Result.Author('Yawen Yang'), arxiv.Result.Author('Fukun Ma'), arxiv.Result.Author('Philip S. Yu'), arxiv.Result.Author('Lijie Wen')]","Cross-lingual natural language inference is a fundamental problem in
cross-lingual language understanding. Many recent works have used prompt
learning to address the lack of annotated parallel corpora in XNLI. However,
these methods adopt discrete prompting by simply translating the templates to
the target language and need external expert knowledge to design the templates.
Besides, discrete prompts of human-designed template words are not trainable
vectors and can not be migrated to target languages in the inference stage
flexibly. In this paper, we propose a novel Soft prompt learning framework with
the Multilingual Verbalizer (SoftMV) for XNLI. SoftMV first constructs
cloze-style question with soft prompts for the input sample. Then we leverage
bilingual dictionaries to generate an augmented multilingual question for the
original question. SoftMV adopts a multilingual verbalizer to align the
representations of original and augmented multilingual questions into the same
semantic space with consistency regularization. Experimental results on XNLI
demonstrate that SoftMV can achieve state-of-the-art performance and
significantly outperform the previous methods under the few-shot and full-shot
cross-lingual transfer settings.",Accept at ACL2023,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.12761v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.12761v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.12761v1,"{'id': 'http://arxiv.org/abs/2305.12761v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.12761v1', 'updated': '2023-05-22T06:31:29Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=22, tm_hour=6, tm_min=31, tm_sec=29, tm_wday=0, tm_yday=142, tm_isdst=0), 'published': '2023-05-22T06:31:29Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=22, tm_hour=6, tm_min=31, tm_sec=29, tm_wday=0, tm_yday=142, tm_isdst=0), 'title': 'Enhancing Cross-lingual Natural Language Inference by Soft Prompting\n  with Multilingual Verbalizer', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Enhancing Cross-lingual Natural Language Inference by Soft Prompting\n  with Multilingual Verbalizer'}, 'summary': 'Cross-lingual natural language inference is a fundamental problem in\ncross-lingual language understanding. Many recent works have used prompt\nlearning to address the lack of annotated parallel corpora in XNLI. However,\nthese methods adopt discrete prompting by simply translating the templates to\nthe target language and need external expert knowledge to design the templates.\nBesides, discrete prompts of human-designed template words are not trainable\nvectors and can not be migrated to target languages in the inference stage\nflexibly. In this paper, we propose a novel Soft prompt learning framework with\nthe Multilingual Verbalizer (SoftMV) for XNLI. SoftMV first constructs\ncloze-style question with soft prompts for the input sample. Then we leverage\nbilingual dictionaries to generate an augmented multilingual question for the\noriginal question. SoftMV adopts a multilingual verbalizer to align the\nrepresentations of original and augmented multilingual questions into the same\nsemantic space with consistency regularization. Experimental results on XNLI\ndemonstrate that SoftMV can achieve state-of-the-art performance and\nsignificantly outperform the previous methods under the few-shot and full-shot\ncross-lingual transfer settings.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Cross-lingual natural language inference is a fundamental problem in\ncross-lingual language understanding. Many recent works have used prompt\nlearning to address the lack of annotated parallel corpora in XNLI. However,\nthese methods adopt discrete prompting by simply translating the templates to\nthe target language and need external expert knowledge to design the templates.\nBesides, discrete prompts of human-designed template words are not trainable\nvectors and can not be migrated to target languages in the inference stage\nflexibly. In this paper, we propose a novel Soft prompt learning framework with\nthe Multilingual Verbalizer (SoftMV) for XNLI. SoftMV first constructs\ncloze-style question with soft prompts for the input sample. Then we leverage\nbilingual dictionaries to generate an augmented multilingual question for the\noriginal question. SoftMV adopts a multilingual verbalizer to align the\nrepresentations of original and augmented multilingual questions into the same\nsemantic space with consistency regularization. Experimental results on XNLI\ndemonstrate that SoftMV can achieve state-of-the-art performance and\nsignificantly outperform the previous methods under the few-shot and full-shot\ncross-lingual transfer settings.'}, 'authors': [{'name': 'Shuang Li'}, {'name': 'Xuming Hu'}, {'name': 'Aiwei Liu'}, {'name': 'Yawen Yang'}, {'name': 'Fukun Ma'}, {'name': 'Philip S. Yu'}, {'name': 'Lijie Wen'}], 'author_detail': {'name': 'Lijie Wen'}, 'author': 'Lijie Wen', 'arxiv_comment': 'Accept at ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.12761v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.12761v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.12816v1,2023-05-22 08:18:58+00:00,2023-05-22 08:18:58+00:00,Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model,"[arxiv.Result.Author('Xiao Wang'), arxiv.Result.Author('Weikang Zhou'), arxiv.Result.Author('Qi Zhang'), arxiv.Result.Author('Jie Zhou'), arxiv.Result.Author('Songyang Gao'), arxiv.Result.Author('Junzhe Wang'), arxiv.Result.Author('Menghan Zhang'), arxiv.Result.Author('Xiang Gao'), arxiv.Result.Author('Yunwen Chen'), arxiv.Result.Author('Tao Gui')]","Pretrained language models have achieved remarkable success in various
natural language processing tasks. However, pretraining has recently shifted
toward larger models and larger data, and this has resulted in significant
computational and energy costs. In this paper, we propose Influence Subset
Selection (ISS) for language model, which explicitly utilizes end-task
knowledge to select a tiny subset of the pretraining corpus. Specifically, the
ISS selects the samples that will provide the most positive influence on the
performance of the end-task. Furthermore, we design a gradient matching based
influence estimation method, which can drastically reduce the computation time
of influence. With only 0.45% of the data and a three-orders-of-magnitude lower
computational cost, ISS outperformed pretrained models (e.g., RoBERTa) on eight
datasets covering four domains.",Accepted by ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.12816v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.12816v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.12816v1,"{'id': 'http://arxiv.org/abs/2305.12816v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.12816v1', 'updated': '2023-05-22T08:18:58Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=22, tm_hour=8, tm_min=18, tm_sec=58, tm_wday=0, tm_yday=142, tm_isdst=0), 'published': '2023-05-22T08:18:58Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=22, tm_hour=8, tm_min=18, tm_sec=58, tm_wday=0, tm_yday=142, tm_isdst=0), 'title': 'Farewell to Aimless Large-scale Pretraining: Influential Subset\n  Selection for Language Model', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Farewell to Aimless Large-scale Pretraining: Influential Subset\n  Selection for Language Model'}, 'summary': 'Pretrained language models have achieved remarkable success in various\nnatural language processing tasks. However, pretraining has recently shifted\ntoward larger models and larger data, and this has resulted in significant\ncomputational and energy costs. In this paper, we propose Influence Subset\nSelection (ISS) for language model, which explicitly utilizes end-task\nknowledge to select a tiny subset of the pretraining corpus. Specifically, the\nISS selects the samples that will provide the most positive influence on the\nperformance of the end-task. Furthermore, we design a gradient matching based\ninfluence estimation method, which can drastically reduce the computation time\nof influence. With only 0.45% of the data and a three-orders-of-magnitude lower\ncomputational cost, ISS outperformed pretrained models (e.g., RoBERTa) on eight\ndatasets covering four domains.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Pretrained language models have achieved remarkable success in various\nnatural language processing tasks. However, pretraining has recently shifted\ntoward larger models and larger data, and this has resulted in significant\ncomputational and energy costs. In this paper, we propose Influence Subset\nSelection (ISS) for language model, which explicitly utilizes end-task\nknowledge to select a tiny subset of the pretraining corpus. Specifically, the\nISS selects the samples that will provide the most positive influence on the\nperformance of the end-task. Furthermore, we design a gradient matching based\ninfluence estimation method, which can drastically reduce the computation time\nof influence. With only 0.45% of the data and a three-orders-of-magnitude lower\ncomputational cost, ISS outperformed pretrained models (e.g., RoBERTa) on eight\ndatasets covering four domains.'}, 'authors': [{'name': 'Xiao Wang'}, {'name': 'Weikang Zhou'}, {'name': 'Qi Zhang'}, {'name': 'Jie Zhou'}, {'name': 'Songyang Gao'}, {'name': 'Junzhe Wang'}, {'name': 'Menghan Zhang'}, {'name': 'Xiang Gao'}, {'name': 'Yunwen Chen'}, {'name': 'Tao Gui'}], 'author_detail': {'name': 'Tao Gui'}, 'author': 'Tao Gui', 'arxiv_comment': 'Accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.12816v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.12816v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.13628v2,2023-06-04 16:32:41+00:00,2023-05-23 02:52:16+00:00,Improving Self-training for Cross-lingual Named Entity Recognition with Contrastive and Prototype Learning,"[arxiv.Result.Author('Ran Zhou'), arxiv.Result.Author('Xin Li'), arxiv.Result.Author('Lidong Bing'), arxiv.Result.Author('Erik Cambria'), arxiv.Result.Author('Chunyan Miao')]","In cross-lingual named entity recognition (NER), self-training is commonly
used to bridge the linguistic gap by training on pseudo-labeled target-language
data. However, due to sub-optimal performance on target languages, the pseudo
labels are often noisy and limit the overall performance. In this work, we aim
to improve self-training for cross-lingual NER by combining representation
learning and pseudo label refinement in one coherent framework. Our proposed
method, namely ContProto mainly comprises two components: (1) contrastive
self-training and (2) prototype-based pseudo-labeling. Our contrastive
self-training facilitates span classification by separating clusters of
different classes, and enhances cross-lingual transferability by producing
closely-aligned representations between the source and target language.
Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of
pseudo labels during training. We evaluate ContProto on multiple transfer
pairs, and experimental results show our method brings in substantial
improvements over current state-of-the-art methods.",Accepted by ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.13628v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.13628v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.13628v2,"{'id': 'http://arxiv.org/abs/2305.13628v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.13628v2', 'updated': '2023-06-04T16:32:41Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=4, tm_hour=16, tm_min=32, tm_sec=41, tm_wday=6, tm_yday=155, tm_isdst=0), 'published': '2023-05-23T02:52:16Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=23, tm_hour=2, tm_min=52, tm_sec=16, tm_wday=1, tm_yday=143, tm_isdst=0), 'title': 'Improving Self-training for Cross-lingual Named Entity Recognition with\n  Contrastive and Prototype Learning', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Improving Self-training for Cross-lingual Named Entity Recognition with\n  Contrastive and Prototype Learning'}, 'summary': 'In cross-lingual named entity recognition (NER), self-training is commonly\nused to bridge the linguistic gap by training on pseudo-labeled target-language\ndata. However, due to sub-optimal performance on target languages, the pseudo\nlabels are often noisy and limit the overall performance. In this work, we aim\nto improve self-training for cross-lingual NER by combining representation\nlearning and pseudo label refinement in one coherent framework. Our proposed\nmethod, namely ContProto mainly comprises two components: (1) contrastive\nself-training and (2) prototype-based pseudo-labeling. Our contrastive\nself-training facilitates span classification by separating clusters of\ndifferent classes, and enhances cross-lingual transferability by producing\nclosely-aligned representations between the source and target language.\nMeanwhile, prototype-based pseudo-labeling effectively improves the accuracy of\npseudo labels during training. We evaluate ContProto on multiple transfer\npairs, and experimental results show our method brings in substantial\nimprovements over current state-of-the-art methods.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'In cross-lingual named entity recognition (NER), self-training is commonly\nused to bridge the linguistic gap by training on pseudo-labeled target-language\ndata. However, due to sub-optimal performance on target languages, the pseudo\nlabels are often noisy and limit the overall performance. In this work, we aim\nto improve self-training for cross-lingual NER by combining representation\nlearning and pseudo label refinement in one coherent framework. Our proposed\nmethod, namely ContProto mainly comprises two components: (1) contrastive\nself-training and (2) prototype-based pseudo-labeling. Our contrastive\nself-training facilitates span classification by separating clusters of\ndifferent classes, and enhances cross-lingual transferability by producing\nclosely-aligned representations between the source and target language.\nMeanwhile, prototype-based pseudo-labeling effectively improves the accuracy of\npseudo labels during training. We evaluate ContProto on multiple transfer\npairs, and experimental results show our method brings in substantial\nimprovements over current state-of-the-art methods.'}, 'authors': [{'name': 'Ran Zhou'}, {'name': 'Xin Li'}, {'name': 'Lidong Bing'}, {'name': 'Erik Cambria'}, {'name': 'Chunyan Miao'}], 'author_detail': {'name': 'Chunyan Miao'}, 'author': 'Chunyan Miao', 'arxiv_comment': 'Accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.13628v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.13628v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.15045v1,2023-05-24 11:35:31+00:00,2023-05-24 11:35:31+00:00,SETI: Systematicity Evaluation of Textual Inference,"[arxiv.Result.Author('Xiyan Fu'), arxiv.Result.Author('Anette Frank')]","We propose SETI (Systematicity Evaluation of Textual Inference), a novel and
comprehensive benchmark designed for evaluating pre-trained language models
(PLMs) for their systematicity capabilities in the domain of textual inference.
Specifically, SETI offers three different NLI tasks and corresponding datasets
to evaluate various types of systematicity in reasoning processes. In order to
solve these tasks, models are required to perform compositional inference based
on known primitive constituents. We conduct experiments of SETI on six widely
used PLMs. Results show that various PLMs are able to solve unseen
compositional inferences when having encountered the knowledge of how to
combine primitives, with good performance. However, they are considerably
limited when this knowledge is unknown to the model (40-100% points decrease).
Furthermore, we find that PLMs can improve drastically once exposed to crucial
compositional knowledge in minimalistic shots. These findings position SETI as
the first benchmark for measuring the future progress of PLMs in achieving
systematicity generalization in the textual inference.",Accepted to Findings of ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.15045v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.15045v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.15045v1,"{'id': 'http://arxiv.org/abs/2305.15045v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.15045v1', 'updated': '2023-05-24T11:35:31Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=24, tm_hour=11, tm_min=35, tm_sec=31, tm_wday=2, tm_yday=144, tm_isdst=0), 'published': '2023-05-24T11:35:31Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=24, tm_hour=11, tm_min=35, tm_sec=31, tm_wday=2, tm_yday=144, tm_isdst=0), 'title': 'SETI: Systematicity Evaluation of Textual Inference', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'SETI: Systematicity Evaluation of Textual Inference'}, 'summary': 'We propose SETI (Systematicity Evaluation of Textual Inference), a novel and\ncomprehensive benchmark designed for evaluating pre-trained language models\n(PLMs) for their systematicity capabilities in the domain of textual inference.\nSpecifically, SETI offers three different NLI tasks and corresponding datasets\nto evaluate various types of systematicity in reasoning processes. In order to\nsolve these tasks, models are required to perform compositional inference based\non known primitive constituents. We conduct experiments of SETI on six widely\nused PLMs. Results show that various PLMs are able to solve unseen\ncompositional inferences when having encountered the knowledge of how to\ncombine primitives, with good performance. However, they are considerably\nlimited when this knowledge is unknown to the model (40-100% points decrease).\nFurthermore, we find that PLMs can improve drastically once exposed to crucial\ncompositional knowledge in minimalistic shots. These findings position SETI as\nthe first benchmark for measuring the future progress of PLMs in achieving\nsystematicity generalization in the textual inference.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We propose SETI (Systematicity Evaluation of Textual Inference), a novel and\ncomprehensive benchmark designed for evaluating pre-trained language models\n(PLMs) for their systematicity capabilities in the domain of textual inference.\nSpecifically, SETI offers three different NLI tasks and corresponding datasets\nto evaluate various types of systematicity in reasoning processes. In order to\nsolve these tasks, models are required to perform compositional inference based\non known primitive constituents. We conduct experiments of SETI on six widely\nused PLMs. Results show that various PLMs are able to solve unseen\ncompositional inferences when having encountered the knowledge of how to\ncombine primitives, with good performance. However, they are considerably\nlimited when this knowledge is unknown to the model (40-100% points decrease).\nFurthermore, we find that PLMs can improve drastically once exposed to crucial\ncompositional knowledge in minimalistic shots. These findings position SETI as\nthe first benchmark for measuring the future progress of PLMs in achieving\nsystematicity generalization in the textual inference.'}, 'authors': [{'name': 'Xiyan Fu'}, {'name': 'Anette Frank'}], 'author_detail': {'name': 'Anette Frank'}, 'author': 'Anette Frank', 'arxiv_comment': 'Accepted to Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.15045v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.15045v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.15056v1,2023-05-24 11:45:59+00:00,2023-05-24 11:45:59+00:00,Reasoning over Hierarchical Question Decomposition Tree for Explainable Question Answering,"[arxiv.Result.Author('Jiajie Zhang'), arxiv.Result.Author('Shulin Cao'), arxiv.Result.Author('Tingjia Zhang'), arxiv.Result.Author('Xin Lv'), arxiv.Result.Author('Jiaxin Shi'), arxiv.Result.Author('Qi Tian'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Lei Hou')]","Explainable question answering (XQA) aims to answer a given question and
provide an explanation why the answer is selected. Existing XQA methods focus
on reasoning on a single knowledge source, e.g., structured knowledge bases,
unstructured corpora, etc. However, integrating information from heterogeneous
knowledge sources is essential to answer complex questions. In this paper, we
propose to leverage question decomposing for heterogeneous knowledge
integration, by breaking down a complex question into simpler ones, and
selecting the appropriate knowledge source for each sub-question. To facilitate
reasoning, we propose a novel two-stage XQA framework, Reasoning over
Hierarchical Question Decomposition Tree (RoHT). First, we build the
Hierarchical Question Decomposition Tree (HQDT) to understand the semantics of
a complex question; then, we conduct probabilistic reasoning over HQDT from
root to leaves recursively, to aggregate heterogeneous knowledge at different
tree levels and search for a best solution considering the decomposing and
answering probabilities. The experiments on complex QA datasets KQA Pro and
Musique show that our framework outperforms SOTA methods significantly,
demonstrating the effectiveness of leveraging question decomposing for
knowledge integration and our RoHT framework.",has been accepted by ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.15056v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.15056v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.15056v1,"{'id': 'http://arxiv.org/abs/2305.15056v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.15056v1', 'updated': '2023-05-24T11:45:59Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=24, tm_hour=11, tm_min=45, tm_sec=59, tm_wday=2, tm_yday=144, tm_isdst=0), 'published': '2023-05-24T11:45:59Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=24, tm_hour=11, tm_min=45, tm_sec=59, tm_wday=2, tm_yday=144, tm_isdst=0), 'title': 'Reasoning over Hierarchical Question Decomposition Tree for Explainable\n  Question Answering', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Reasoning over Hierarchical Question Decomposition Tree for Explainable\n  Question Answering'}, 'summary': 'Explainable question answering (XQA) aims to answer a given question and\nprovide an explanation why the answer is selected. Existing XQA methods focus\non reasoning on a single knowledge source, e.g., structured knowledge bases,\nunstructured corpora, etc. However, integrating information from heterogeneous\nknowledge sources is essential to answer complex questions. In this paper, we\npropose to leverage question decomposing for heterogeneous knowledge\nintegration, by breaking down a complex question into simpler ones, and\nselecting the appropriate knowledge source for each sub-question. To facilitate\nreasoning, we propose a novel two-stage XQA framework, Reasoning over\nHierarchical Question Decomposition Tree (RoHT). First, we build the\nHierarchical Question Decomposition Tree (HQDT) to understand the semantics of\na complex question; then, we conduct probabilistic reasoning over HQDT from\nroot to leaves recursively, to aggregate heterogeneous knowledge at different\ntree levels and search for a best solution considering the decomposing and\nanswering probabilities. The experiments on complex QA datasets KQA Pro and\nMusique show that our framework outperforms SOTA methods significantly,\ndemonstrating the effectiveness of leveraging question decomposing for\nknowledge integration and our RoHT framework.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Explainable question answering (XQA) aims to answer a given question and\nprovide an explanation why the answer is selected. Existing XQA methods focus\non reasoning on a single knowledge source, e.g., structured knowledge bases,\nunstructured corpora, etc. However, integrating information from heterogeneous\nknowledge sources is essential to answer complex questions. In this paper, we\npropose to leverage question decomposing for heterogeneous knowledge\nintegration, by breaking down a complex question into simpler ones, and\nselecting the appropriate knowledge source for each sub-question. To facilitate\nreasoning, we propose a novel two-stage XQA framework, Reasoning over\nHierarchical Question Decomposition Tree (RoHT). First, we build the\nHierarchical Question Decomposition Tree (HQDT) to understand the semantics of\na complex question; then, we conduct probabilistic reasoning over HQDT from\nroot to leaves recursively, to aggregate heterogeneous knowledge at different\ntree levels and search for a best solution considering the decomposing and\nanswering probabilities. The experiments on complex QA datasets KQA Pro and\nMusique show that our framework outperforms SOTA methods significantly,\ndemonstrating the effectiveness of leveraging question decomposing for\nknowledge integration and our RoHT framework.'}, 'authors': [{'name': 'Jiajie Zhang'}, {'name': 'Shulin Cao'}, {'name': 'Tingjia Zhang'}, {'name': 'Xin Lv'}, {'name': 'Jiaxin Shi'}, {'name': 'Qi Tian'}, {'name': 'Juanzi Li'}, {'name': 'Lei Hou'}], 'author_detail': {'name': 'Lei Hou'}, 'author': 'Lei Hou', 'arxiv_comment': 'has been accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.15056v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.15056v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.15273v1,2023-05-24 15:59:44+00:00,2023-05-24 15:59:44+00:00,Revisiting Token Dropping Strategy in Efficient BERT Pretraining,"[arxiv.Result.Author('Qihuang Zhong'), arxiv.Result.Author('Liang Ding'), arxiv.Result.Author('Juhua Liu'), arxiv.Result.Author('Xuebo Liu'), arxiv.Result.Author('Min Zhang'), arxiv.Result.Author('Bo Du'), arxiv.Result.Author('Dacheng Tao')]","Token dropping is a recently-proposed strategy to speed up the pretraining of
masked language models, such as BERT, by skipping the computation of a subset
of the input tokens at several middle layers. It can effectively reduce the
training time without degrading much performance on downstream tasks. However,
we empirically find that token dropping is prone to a semantic loss problem and
falls short in handling semantic-intense tasks. Motivated by this, we propose a
simple yet effective semantic-consistent learning method (ScTD) to improve the
token dropping. ScTD aims to encourage the model to learn how to preserve the
semantic information in the representation space. Extensive experiments on 12
tasks show that, with the help of our ScTD, token dropping can achieve
consistent and significant performance gains across all task types and model
sizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings
up to +1.56% average improvement over the vanilla token dropping.",Accepted to ACL2023 Main Conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.15273v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.15273v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.15273v1,"{'id': 'http://arxiv.org/abs/2305.15273v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.15273v1', 'updated': '2023-05-24T15:59:44Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=24, tm_hour=15, tm_min=59, tm_sec=44, tm_wday=2, tm_yday=144, tm_isdst=0), 'published': '2023-05-24T15:59:44Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=24, tm_hour=15, tm_min=59, tm_sec=44, tm_wday=2, tm_yday=144, tm_isdst=0), 'title': 'Revisiting Token Dropping Strategy in Efficient BERT Pretraining', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Revisiting Token Dropping Strategy in Efficient BERT Pretraining'}, 'summary': 'Token dropping is a recently-proposed strategy to speed up the pretraining of\nmasked language models, such as BERT, by skipping the computation of a subset\nof the input tokens at several middle layers. It can effectively reduce the\ntraining time without degrading much performance on downstream tasks. However,\nwe empirically find that token dropping is prone to a semantic loss problem and\nfalls short in handling semantic-intense tasks. Motivated by this, we propose a\nsimple yet effective semantic-consistent learning method (ScTD) to improve the\ntoken dropping. ScTD aims to encourage the model to learn how to preserve the\nsemantic information in the representation space. Extensive experiments on 12\ntasks show that, with the help of our ScTD, token dropping can achieve\nconsistent and significant performance gains across all task types and model\nsizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings\nup to +1.56% average improvement over the vanilla token dropping.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Token dropping is a recently-proposed strategy to speed up the pretraining of\nmasked language models, such as BERT, by skipping the computation of a subset\nof the input tokens at several middle layers. It can effectively reduce the\ntraining time without degrading much performance on downstream tasks. However,\nwe empirically find that token dropping is prone to a semantic loss problem and\nfalls short in handling semantic-intense tasks. Motivated by this, we propose a\nsimple yet effective semantic-consistent learning method (ScTD) to improve the\ntoken dropping. ScTD aims to encourage the model to learn how to preserve the\nsemantic information in the representation space. Extensive experiments on 12\ntasks show that, with the help of our ScTD, token dropping can achieve\nconsistent and significant performance gains across all task types and model\nsizes. More encouragingly, ScTD saves up to 57% of pretraining time and brings\nup to +1.56% average improvement over the vanilla token dropping.'}, 'authors': [{'name': 'Qihuang Zhong'}, {'name': 'Liang Ding'}, {'name': 'Juhua Liu'}, {'name': 'Xuebo Liu'}, {'name': 'Min Zhang'}, {'name': 'Bo Du'}, {'name': 'Dacheng Tao'}], 'author_detail': {'name': 'Dacheng Tao'}, 'author': 'Dacheng Tao', 'arxiv_comment': 'Accepted to ACL2023 Main Conference', 'links': [{'href': 'http://arxiv.org/abs/2305.15273v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.15273v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.15275v1,2023-05-24 16:00:54+00:00,2023-05-24 16:00:54+00:00,Self-Evolution Learning for Discriminative Language Model Pretraining,"[arxiv.Result.Author('Qihuang Zhong'), arxiv.Result.Author('Liang Ding'), arxiv.Result.Author('Juhua Liu'), arxiv.Result.Author('Bo Du'), arxiv.Result.Author('Dacheng Tao')]","Masked language modeling, widely used in discriminative language model (e.g.,
BERT) pretraining, commonly adopts a random masking strategy. However, random
masking does not consider the importance of the different words in the sentence
meaning, where some of them are more worthy to be predicted. Therefore, various
masking strategies (e.g., entity-level masking) are proposed, but most of them
require expensive prior knowledge and generally train from scratch without
reusing existing model weights. In this paper, we present Self-Evolution
learning (SE), a simple and effective token masking and learning method to
fully and wisely exploit the knowledge from data. SE focuses on learning the
informative yet under-explored tokens and adaptively regularizes the training
by introducing a novel Token-specific Label Smoothing approach. Experiments on
10 tasks show that our SE brings consistent and significant improvements
(+1.43~2.12 average scores) upon different PLMs. In-depth analyses demonstrate
that SE improves linguistic knowledge learning and generalization.",Accepted to Findings of ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.15275v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.15275v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.15275v1,"{'id': 'http://arxiv.org/abs/2305.15275v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.15275v1', 'updated': '2023-05-24T16:00:54Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=24, tm_hour=16, tm_min=0, tm_sec=54, tm_wday=2, tm_yday=144, tm_isdst=0), 'published': '2023-05-24T16:00:54Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=24, tm_hour=16, tm_min=0, tm_sec=54, tm_wday=2, tm_yday=144, tm_isdst=0), 'title': 'Self-Evolution Learning for Discriminative Language Model Pretraining', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Self-Evolution Learning for Discriminative Language Model Pretraining'}, 'summary': 'Masked language modeling, widely used in discriminative language model (e.g.,\nBERT) pretraining, commonly adopts a random masking strategy. However, random\nmasking does not consider the importance of the different words in the sentence\nmeaning, where some of them are more worthy to be predicted. Therefore, various\nmasking strategies (e.g., entity-level masking) are proposed, but most of them\nrequire expensive prior knowledge and generally train from scratch without\nreusing existing model weights. In this paper, we present Self-Evolution\nlearning (SE), a simple and effective token masking and learning method to\nfully and wisely exploit the knowledge from data. SE focuses on learning the\ninformative yet under-explored tokens and adaptively regularizes the training\nby introducing a novel Token-specific Label Smoothing approach. Experiments on\n10 tasks show that our SE brings consistent and significant improvements\n(+1.43~2.12 average scores) upon different PLMs. In-depth analyses demonstrate\nthat SE improves linguistic knowledge learning and generalization.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Masked language modeling, widely used in discriminative language model (e.g.,\nBERT) pretraining, commonly adopts a random masking strategy. However, random\nmasking does not consider the importance of the different words in the sentence\nmeaning, where some of them are more worthy to be predicted. Therefore, various\nmasking strategies (e.g., entity-level masking) are proposed, but most of them\nrequire expensive prior knowledge and generally train from scratch without\nreusing existing model weights. In this paper, we present Self-Evolution\nlearning (SE), a simple and effective token masking and learning method to\nfully and wisely exploit the knowledge from data. SE focuses on learning the\ninformative yet under-explored tokens and adaptively regularizes the training\nby introducing a novel Token-specific Label Smoothing approach. Experiments on\n10 tasks show that our SE brings consistent and significant improvements\n(+1.43~2.12 average scores) upon different PLMs. In-depth analyses demonstrate\nthat SE improves linguistic knowledge learning and generalization.'}, 'authors': [{'name': 'Qihuang Zhong'}, {'name': 'Liang Ding'}, {'name': 'Juhua Liu'}, {'name': 'Bo Du'}, {'name': 'Dacheng Tao'}], 'author_detail': {'name': 'Dacheng Tao'}, 'author': 'Dacheng Tao', 'arxiv_comment': 'Accepted to Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.15275v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.15275v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.15718v1,2023-05-25 05:01:33+00:00,2023-05-25 05:01:33+00:00,Towards Higher Pareto Frontier in Multilingual Machine Translation,"[arxiv.Result.Author('Yichong Huang'), arxiv.Result.Author('Xiaocheng Feng'), arxiv.Result.Author('Xinwei Geng'), arxiv.Result.Author('Baohang Li'), arxiv.Result.Author('Bing Qin')]","Multilingual neural machine translation has witnessed remarkable progress in
recent years. However, the long-tailed distribution of multilingual corpora
poses a challenge of Pareto optimization, i.e., optimizing for some languages
may come at the cost of degrading the performance of others. Existing balancing
training strategies are equivalent to a series of Pareto optimal solutions,
which trade off on a Pareto frontier. In this work, we propose a new training
framework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto
frontier outwards rather than making trade-offs. Specifically, Pareto-MD
collaboratively trains two Pareto optimal solutions that favor different
languages and allows them to learn from the strengths of each other via
knowledge distillation. Furthermore, we introduce a novel strategy to enable
stronger communication between Pareto optimal solutions and broaden the
applicability of our approach. Experimental results on the widely-used WMT and
TED datasets show that our method significantly pushes the Pareto frontier and
outperforms baselines by up to +2.46 BLEU.",Accepted by ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.15718v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.15718v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.15718v1,"{'id': 'http://arxiv.org/abs/2305.15718v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.15718v1', 'updated': '2023-05-25T05:01:33Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=25, tm_hour=5, tm_min=1, tm_sec=33, tm_wday=3, tm_yday=145, tm_isdst=0), 'published': '2023-05-25T05:01:33Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=25, tm_hour=5, tm_min=1, tm_sec=33, tm_wday=3, tm_yday=145, tm_isdst=0), 'title': 'Towards Higher Pareto Frontier in Multilingual Machine Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Towards Higher Pareto Frontier in Multilingual Machine Translation'}, 'summary': 'Multilingual neural machine translation has witnessed remarkable progress in\nrecent years. However, the long-tailed distribution of multilingual corpora\nposes a challenge of Pareto optimization, i.e., optimizing for some languages\nmay come at the cost of degrading the performance of others. Existing balancing\ntraining strategies are equivalent to a series of Pareto optimal solutions,\nwhich trade off on a Pareto frontier. In this work, we propose a new training\nframework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto\nfrontier outwards rather than making trade-offs. Specifically, Pareto-MD\ncollaboratively trains two Pareto optimal solutions that favor different\nlanguages and allows them to learn from the strengths of each other via\nknowledge distillation. Furthermore, we introduce a novel strategy to enable\nstronger communication between Pareto optimal solutions and broaden the\napplicability of our approach. Experimental results on the widely-used WMT and\nTED datasets show that our method significantly pushes the Pareto frontier and\noutperforms baselines by up to +2.46 BLEU.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Multilingual neural machine translation has witnessed remarkable progress in\nrecent years. However, the long-tailed distribution of multilingual corpora\nposes a challenge of Pareto optimization, i.e., optimizing for some languages\nmay come at the cost of degrading the performance of others. Existing balancing\ntraining strategies are equivalent to a series of Pareto optimal solutions,\nwhich trade off on a Pareto frontier. In this work, we propose a new training\nframework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto\nfrontier outwards rather than making trade-offs. Specifically, Pareto-MD\ncollaboratively trains two Pareto optimal solutions that favor different\nlanguages and allows them to learn from the strengths of each other via\nknowledge distillation. Furthermore, we introduce a novel strategy to enable\nstronger communication between Pareto optimal solutions and broaden the\napplicability of our approach. Experimental results on the widely-used WMT and\nTED datasets show that our method significantly pushes the Pareto frontier and\noutperforms baselines by up to +2.46 BLEU.'}, 'authors': [{'name': 'Yichong Huang'}, {'name': 'Xiaocheng Feng'}, {'name': 'Xinwei Geng'}, {'name': 'Baohang Li'}, {'name': 'Bing Qin'}], 'author_detail': {'name': 'Bing Qin'}, 'author': 'Bing Qin', 'arxiv_comment': 'Accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.15718v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.15718v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.15932v2,2023-06-07 20:33:09+00:00,2023-05-25 10:59:47+00:00,BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering,"[arxiv.Result.Author('Jie He'), arxiv.Result.Author('Simon Chi Lok U'), arxiv.Result.Author('Víctor Gutiérrez-Basulto'), arxiv.Result.Author('Jeff Z. Pan')]","Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as
the construction of commonsense reasoning datasets is expensive, and they are
inevitably limited in their scope. A popular approach to UCR is to fine-tune
language models with external knowledge (e.g., knowledge graphs), but this
usually requires a large number of training examples. In this paper, we propose
to transform the downstream multiple choice question answering task into a
simpler binary classification task by ranking all candidate answers according
to their reasonableness. To this end, for training the model, we convert the
knowledge graph triples into reasonable and unreasonable texts. Extensive
experimental results show the effectiveness of our approach on various multiple
choice question answering benchmarks. Furthermore, compared with existing UCR
approaches using KGs, ours is less data hungry. Our code is available at
https://github.com/probe2/BUCA.",Accepted by ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.15932v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.15932v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.15932v2,"{'id': 'http://arxiv.org/abs/2305.15932v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.15932v2', 'updated': '2023-06-07T20:33:09Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=7, tm_hour=20, tm_min=33, tm_sec=9, tm_wday=2, tm_yday=158, tm_isdst=0), 'published': '2023-05-25T10:59:47Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=25, tm_hour=10, tm_min=59, tm_sec=47, tm_wday=3, tm_yday=145, tm_isdst=0), 'title': 'BUCA: A Binary Classification Approach to Unsupervised Commonsense\n  Question Answering', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'BUCA: A Binary Classification Approach to Unsupervised Commonsense\n  Question Answering'}, 'summary': 'Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as\nthe construction of commonsense reasoning datasets is expensive, and they are\ninevitably limited in their scope. A popular approach to UCR is to fine-tune\nlanguage models with external knowledge (e.g., knowledge graphs), but this\nusually requires a large number of training examples. In this paper, we propose\nto transform the downstream multiple choice question answering task into a\nsimpler binary classification task by ranking all candidate answers according\nto their reasonableness. To this end, for training the model, we convert the\nknowledge graph triples into reasonable and unreasonable texts. Extensive\nexperimental results show the effectiveness of our approach on various multiple\nchoice question answering benchmarks. Furthermore, compared with existing UCR\napproaches using KGs, ours is less data hungry. Our code is available at\nhttps://github.com/probe2/BUCA.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as\nthe construction of commonsense reasoning datasets is expensive, and they are\ninevitably limited in their scope. A popular approach to UCR is to fine-tune\nlanguage models with external knowledge (e.g., knowledge graphs), but this\nusually requires a large number of training examples. In this paper, we propose\nto transform the downstream multiple choice question answering task into a\nsimpler binary classification task by ranking all candidate answers according\nto their reasonableness. To this end, for training the model, we convert the\nknowledge graph triples into reasonable and unreasonable texts. Extensive\nexperimental results show the effectiveness of our approach on various multiple\nchoice question answering benchmarks. Furthermore, compared with existing UCR\napproaches using KGs, ours is less data hungry. Our code is available at\nhttps://github.com/probe2/BUCA.'}, 'authors': [{'name': 'Jie He'}, {'name': 'Simon Chi Lok U'}, {'name': 'Víctor Gutiérrez-Basulto'}, {'name': 'Jeff Z. Pan'}], 'author_detail': {'name': 'Jeff Z. Pan'}, 'author': 'Jeff Z. Pan', 'arxiv_comment': 'Accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.15932v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.15932v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.16626v1,2023-05-26 04:40:56+00:00,2023-05-26 04:40:56+00:00,Evaluation of Question Generation Needs More References,"[arxiv.Result.Author('Shinhyeok Oh'), arxiv.Result.Author('Hyojun Go'), arxiv.Result.Author('Hyeongdon Moon'), arxiv.Result.Author('Yunsung Lee'), arxiv.Result.Author('Myeongho Jeong'), arxiv.Result.Author('Hyun Seung Lee'), arxiv.Result.Author('Seungtaek Choi')]","Question generation (QG) is the task of generating a valid and fluent
question based on a given context and the target answer. According to various
purposes, even given the same context, instructors can ask questions about
different concepts, and even the same concept can be written in different ways.
However, the evaluation for QG usually depends on single reference-based
similarity metrics, such as n-gram-based metric or learned metric, which is not
sufficient to fully evaluate the potential of QG methods. To this end, we
propose to paraphrase the reference question for a more robust QG evaluation.
Using large language models such as GPT-3, we created semantically and
syntactically diverse questions, then adopt the simple aggregation of the
popular evaluation metrics as the final scores. Through our experiments, we
found that using multiple (pseudo) references is more effective for QG
evaluation while showing a higher correlation with human evaluations than
evaluation with a single reference.",Accepted to Findings of ACL2023,,,cs.CL,"['cs.CL', 'cs.AI', 'I.2.7']","[arxiv.Result.Link('http://arxiv.org/abs/2305.16626v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.16626v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.16626v1,"{'id': 'http://arxiv.org/abs/2305.16626v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.16626v1', 'updated': '2023-05-26T04:40:56Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=4, tm_min=40, tm_sec=56, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2023-05-26T04:40:56Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=4, tm_min=40, tm_sec=56, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'Evaluation of Question Generation Needs More References', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Evaluation of Question Generation Needs More References'}, 'summary': 'Question generation (QG) is the task of generating a valid and fluent\nquestion based on a given context and the target answer. According to various\npurposes, even given the same context, instructors can ask questions about\ndifferent concepts, and even the same concept can be written in different ways.\nHowever, the evaluation for QG usually depends on single reference-based\nsimilarity metrics, such as n-gram-based metric or learned metric, which is not\nsufficient to fully evaluate the potential of QG methods. To this end, we\npropose to paraphrase the reference question for a more robust QG evaluation.\nUsing large language models such as GPT-3, we created semantically and\nsyntactically diverse questions, then adopt the simple aggregation of the\npopular evaluation metrics as the final scores. Through our experiments, we\nfound that using multiple (pseudo) references is more effective for QG\nevaluation while showing a higher correlation with human evaluations than\nevaluation with a single reference.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Question generation (QG) is the task of generating a valid and fluent\nquestion based on a given context and the target answer. According to various\npurposes, even given the same context, instructors can ask questions about\ndifferent concepts, and even the same concept can be written in different ways.\nHowever, the evaluation for QG usually depends on single reference-based\nsimilarity metrics, such as n-gram-based metric or learned metric, which is not\nsufficient to fully evaluate the potential of QG methods. To this end, we\npropose to paraphrase the reference question for a more robust QG evaluation.\nUsing large language models such as GPT-3, we created semantically and\nsyntactically diverse questions, then adopt the simple aggregation of the\npopular evaluation metrics as the final scores. Through our experiments, we\nfound that using multiple (pseudo) references is more effective for QG\nevaluation while showing a higher correlation with human evaluations than\nevaluation with a single reference.'}, 'authors': [{'name': 'Shinhyeok Oh'}, {'name': 'Hyojun Go'}, {'name': 'Hyeongdon Moon'}, {'name': 'Yunsung Lee'}, {'name': 'Myeongho Jeong'}, {'name': 'Hyun Seung Lee'}, {'name': 'Seungtaek Choi'}], 'author_detail': {'name': 'Seungtaek Choi'}, 'author': 'Seungtaek Choi', 'arxiv_comment': 'Accepted to Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.16626v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.16626v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.7', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.16739v1,2023-05-26 08:41:59+00:00,2023-05-26 08:41:59+00:00,AlignScore: Evaluating Factual Consistency with a Unified Alignment Function,"[arxiv.Result.Author('Yuheng Zha'), arxiv.Result.Author('Yichi Yang'), arxiv.Result.Author('Ruichen Li'), arxiv.Result.Author('Zhiting Hu')]","Many text generation applications require the generated text to be factually
consistent with input information. Automatic evaluation of factual consistency
is challenging. Previous work has developed various metrics that often depend
on specific functions, such as natural language inference (NLI) or question
answering (QA), trained on limited data. Those metrics thus can hardly assess
diverse factual inconsistencies (e.g., contradictions, hallucinations) that
occur in varying inputs/outputs (e.g., sentences, documents) from different
tasks. In this paper, we propose AlignScore, a new holistic metric that applies
to a variety of factual inconsistency scenarios as above. AlignScore is based
on a general function of information alignment between two arbitrary text
pieces. Crucially, we develop a unified training framework of the alignment
function by integrating a large diversity of data sources, resulting in 4.7M
training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact
verification, information retrieval, semantic similarity, and summarization).
We conduct extensive experiments on large-scale benchmarks including 22
evaluation datasets, where 19 of the datasets were never seen in the alignment
training. AlignScore achieves substantial improvement over a wide range of
previous metrics. Moreover, AlignScore (355M parameters) matches or even
outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude
larger.","19 pages, 5 figures, ACL2023",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.16739v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.16739v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.16739v1,"{'id': 'http://arxiv.org/abs/2305.16739v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.16739v1', 'updated': '2023-05-26T08:41:59Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=8, tm_min=41, tm_sec=59, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2023-05-26T08:41:59Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=8, tm_min=41, tm_sec=59, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'AlignScore: Evaluating Factual Consistency with a Unified Alignment\n  Function', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'AlignScore: Evaluating Factual Consistency with a Unified Alignment\n  Function'}, 'summary': 'Many text generation applications require the generated text to be factually\nconsistent with input information. Automatic evaluation of factual consistency\nis challenging. Previous work has developed various metrics that often depend\non specific functions, such as natural language inference (NLI) or question\nanswering (QA), trained on limited data. Those metrics thus can hardly assess\ndiverse factual inconsistencies (e.g., contradictions, hallucinations) that\noccur in varying inputs/outputs (e.g., sentences, documents) from different\ntasks. In this paper, we propose AlignScore, a new holistic metric that applies\nto a variety of factual inconsistency scenarios as above. AlignScore is based\non a general function of information alignment between two arbitrary text\npieces. Crucially, we develop a unified training framework of the alignment\nfunction by integrating a large diversity of data sources, resulting in 4.7M\ntraining examples from 7 well-established tasks (NLI, QA, paraphrasing, fact\nverification, information retrieval, semantic similarity, and summarization).\nWe conduct extensive experiments on large-scale benchmarks including 22\nevaluation datasets, where 19 of the datasets were never seen in the alignment\ntraining. AlignScore achieves substantial improvement over a wide range of\nprevious metrics. Moreover, AlignScore (355M parameters) matches or even\noutperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude\nlarger.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Many text generation applications require the generated text to be factually\nconsistent with input information. Automatic evaluation of factual consistency\nis challenging. Previous work has developed various metrics that often depend\non specific functions, such as natural language inference (NLI) or question\nanswering (QA), trained on limited data. Those metrics thus can hardly assess\ndiverse factual inconsistencies (e.g., contradictions, hallucinations) that\noccur in varying inputs/outputs (e.g., sentences, documents) from different\ntasks. In this paper, we propose AlignScore, a new holistic metric that applies\nto a variety of factual inconsistency scenarios as above. AlignScore is based\non a general function of information alignment between two arbitrary text\npieces. Crucially, we develop a unified training framework of the alignment\nfunction by integrating a large diversity of data sources, resulting in 4.7M\ntraining examples from 7 well-established tasks (NLI, QA, paraphrasing, fact\nverification, information retrieval, semantic similarity, and summarization).\nWe conduct extensive experiments on large-scale benchmarks including 22\nevaluation datasets, where 19 of the datasets were never seen in the alignment\ntraining. AlignScore achieves substantial improvement over a wide range of\nprevious metrics. Moreover, AlignScore (355M parameters) matches or even\noutperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude\nlarger.'}, 'authors': [{'name': 'Yuheng Zha'}, {'name': 'Yichi Yang'}, {'name': 'Ruichen Li'}, {'name': 'Zhiting Hu'}], 'author_detail': {'name': 'Zhiting Hu'}, 'author': 'Zhiting Hu', 'arxiv_comment': '19 pages, 5 figures, ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.16739v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.16739v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.16958v1,2023-05-26 14:14:51+00:00,2023-05-26 14:14:51+00:00,MixCE: Training Autoregressive Language Models by Mixing Forward and Reverse Cross-Entropies,"[arxiv.Result.Author('Shiyue Zhang'), arxiv.Result.Author('Shijie Wu'), arxiv.Result.Author('Ozan Irsoy'), arxiv.Result.Author('Steven Lu'), arxiv.Result.Author('Mohit Bansal'), arxiv.Result.Author('Mark Dredze'), arxiv.Result.Author('David Rosenberg')]","Autoregressive language models are trained by minimizing the cross-entropy of
the model distribution Q relative to the data distribution P -- that is,
minimizing the forward cross-entropy, which is equivalent to maximum likelihood
estimation (MLE). We have observed that models trained in this way may
""over-generalize"", in the sense that they produce non-human-like text.
Moreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P
relative to Q, is a better reflection of how a human would evaluate text
generated by a model. Hence, we propose learning with MixCE, an objective that
mixes the forward and reverse cross-entropies. We evaluate models trained with
this objective on synthetic data settings (where P is known) and real data, and
show that the resulting models yield better generated text without complex
decoding strategies. Our code and models are publicly available at
https://github.com/bloomberg/mixce-acl2023",ACL 2023 (22 pages),,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.16958v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.16958v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.16958v1,"{'id': 'http://arxiv.org/abs/2305.16958v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.16958v1', 'updated': '2023-05-26T14:14:51Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=14, tm_min=14, tm_sec=51, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2023-05-26T14:14:51Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=14, tm_min=14, tm_sec=51, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'MixCE: Training Autoregressive Language Models by Mixing Forward and\n  Reverse Cross-Entropies', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'MixCE: Training Autoregressive Language Models by Mixing Forward and\n  Reverse Cross-Entropies'}, 'summary': 'Autoregressive language models are trained by minimizing the cross-entropy of\nthe model distribution Q relative to the data distribution P -- that is,\nminimizing the forward cross-entropy, which is equivalent to maximum likelihood\nestimation (MLE). We have observed that models trained in this way may\n""over-generalize"", in the sense that they produce non-human-like text.\nMoreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P\nrelative to Q, is a better reflection of how a human would evaluate text\ngenerated by a model. Hence, we propose learning with MixCE, an objective that\nmixes the forward and reverse cross-entropies. We evaluate models trained with\nthis objective on synthetic data settings (where P is known) and real data, and\nshow that the resulting models yield better generated text without complex\ndecoding strategies. Our code and models are publicly available at\nhttps://github.com/bloomberg/mixce-acl2023', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Autoregressive language models are trained by minimizing the cross-entropy of\nthe model distribution Q relative to the data distribution P -- that is,\nminimizing the forward cross-entropy, which is equivalent to maximum likelihood\nestimation (MLE). We have observed that models trained in this way may\n""over-generalize"", in the sense that they produce non-human-like text.\nMoreover, we believe that reverse cross-entropy, i.e., the cross-entropy of P\nrelative to Q, is a better reflection of how a human would evaluate text\ngenerated by a model. Hence, we propose learning with MixCE, an objective that\nmixes the forward and reverse cross-entropies. We evaluate models trained with\nthis objective on synthetic data settings (where P is known) and real data, and\nshow that the resulting models yield better generated text without complex\ndecoding strategies. Our code and models are publicly available at\nhttps://github.com/bloomberg/mixce-acl2023'}, 'authors': [{'name': 'Shiyue Zhang'}, {'name': 'Shijie Wu'}, {'name': 'Ozan Irsoy'}, {'name': 'Steven Lu'}, {'name': 'Mohit Bansal'}, {'name': 'Mark Dredze'}, {'name': 'David Rosenberg'}], 'author_detail': {'name': 'David Rosenberg'}, 'author': 'David Rosenberg', 'arxiv_comment': 'ACL 2023 (22 pages)', 'links': [{'href': 'http://arxiv.org/abs/2305.16958v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.16958v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.16967v3,2023-06-10 13:23:41+00:00,2023-05-26 14:21:54+00:00,Evaluating Open-Domain Dialogues in Latent Space with Next Sentence Prediction and Mutual Information,"[arxiv.Result.Author('Kun Zhao'), arxiv.Result.Author('Bohao Yang'), arxiv.Result.Author('Chenghua Lin'), arxiv.Result.Author('Wenge Rong'), arxiv.Result.Author('Aline Villavicencio'), arxiv.Result.Author('Xiaohui Cui')]","The long-standing one-to-many issue of the open-domain dialogues poses
significant challenges for automatic evaluation methods, i.e., there may be
multiple suitable responses which differ in semantics for a given
conversational context. To tackle this challenge, we propose a novel
learning-based automatic evaluation metric (CMN), which can robustly evaluate
open-domain dialogues by augmenting Conditional Variational Autoencoders
(CVAEs) with a Next Sentence Prediction (NSP) objective and employing Mutual
Information (MI) to model the semantic similarity of text in the latent space.
Experimental results on two open-domain dialogue datasets demonstrate the
superiority of our method compared with a wide range of baselines, especially
in handling responses which are distant to the golden reference responses in
semantics.",Accepted at ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.16967v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.16967v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.16967v3,"{'id': 'http://arxiv.org/abs/2305.16967v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.16967v3', 'updated': '2023-06-10T13:23:41Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=10, tm_hour=13, tm_min=23, tm_sec=41, tm_wday=5, tm_yday=161, tm_isdst=0), 'published': '2023-05-26T14:21:54Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=14, tm_min=21, tm_sec=54, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'Evaluating Open-Domain Dialogues in Latent Space with Next Sentence\n  Prediction and Mutual Information', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Evaluating Open-Domain Dialogues in Latent Space with Next Sentence\n  Prediction and Mutual Information'}, 'summary': 'The long-standing one-to-many issue of the open-domain dialogues poses\nsignificant challenges for automatic evaluation methods, i.e., there may be\nmultiple suitable responses which differ in semantics for a given\nconversational context. To tackle this challenge, we propose a novel\nlearning-based automatic evaluation metric (CMN), which can robustly evaluate\nopen-domain dialogues by augmenting Conditional Variational Autoencoders\n(CVAEs) with a Next Sentence Prediction (NSP) objective and employing Mutual\nInformation (MI) to model the semantic similarity of text in the latent space.\nExperimental results on two open-domain dialogue datasets demonstrate the\nsuperiority of our method compared with a wide range of baselines, especially\nin handling responses which are distant to the golden reference responses in\nsemantics.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The long-standing one-to-many issue of the open-domain dialogues poses\nsignificant challenges for automatic evaluation methods, i.e., there may be\nmultiple suitable responses which differ in semantics for a given\nconversational context. To tackle this challenge, we propose a novel\nlearning-based automatic evaluation metric (CMN), which can robustly evaluate\nopen-domain dialogues by augmenting Conditional Variational Autoencoders\n(CVAEs) with a Next Sentence Prediction (NSP) objective and employing Mutual\nInformation (MI) to model the semantic similarity of text in the latent space.\nExperimental results on two open-domain dialogue datasets demonstrate the\nsuperiority of our method compared with a wide range of baselines, especially\nin handling responses which are distant to the golden reference responses in\nsemantics.'}, 'authors': [{'name': 'Kun Zhao'}, {'name': 'Bohao Yang'}, {'name': 'Chenghua Lin'}, {'name': 'Wenge Rong'}, {'name': 'Aline Villavicencio'}, {'name': 'Xiaohui Cui'}], 'author_detail': {'name': 'Xiaohui Cui'}, 'author': 'Xiaohui Cui', 'arxiv_comment': 'Accepted at ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.16967v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.16967v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.16982v1,2023-05-26 14:37:38+00:00,2023-05-26 14:37:38+00:00,TranSFormer: Slow-Fast Transformer for Machine Translation,"[arxiv.Result.Author('Bei Li'), arxiv.Result.Author('Yi Jing'), arxiv.Result.Author('Xu Tan'), arxiv.Result.Author('Zhen Xing'), arxiv.Result.Author('Tong Xiao'), arxiv.Result.Author('Jingbo Zhu')]","Learning multiscale Transformer models has been evidenced as a viable
approach to augmenting machine translation systems. Prior research has
primarily focused on treating subwords as basic units in developing such
systems. However, the incorporation of fine-grained character-level features
into multiscale Transformer has not yet been explored. In this work, we present
a \textbf{S}low-\textbf{F}ast two-stream learning model, referred to as
Tran\textbf{SF}ormer, which utilizes a ``slow'' branch to deal with subword
sequences and a ``fast'' branch to deal with longer character sequences. This
model is efficient since the fast branch is very lightweight by reducing the
model width, and yet provides useful fine-grained features for the slow branch.
Our TranSFormer shows consistent BLEU improvements (larger than 1 BLEU point)
on several machine translation benchmarks.",Accepted by Findings of ACL2023,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.16982v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.16982v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.16982v1,"{'id': 'http://arxiv.org/abs/2305.16982v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.16982v1', 'updated': '2023-05-26T14:37:38Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=14, tm_min=37, tm_sec=38, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2023-05-26T14:37:38Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=14, tm_min=37, tm_sec=38, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'TranSFormer: Slow-Fast Transformer for Machine Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'TranSFormer: Slow-Fast Transformer for Machine Translation'}, 'summary': ""Learning multiscale Transformer models has been evidenced as a viable\napproach to augmenting machine translation systems. Prior research has\nprimarily focused on treating subwords as basic units in developing such\nsystems. However, the incorporation of fine-grained character-level features\ninto multiscale Transformer has not yet been explored. In this work, we present\na \\textbf{S}low-\\textbf{F}ast two-stream learning model, referred to as\nTran\\textbf{SF}ormer, which utilizes a ``slow'' branch to deal with subword\nsequences and a ``fast'' branch to deal with longer character sequences. This\nmodel is efficient since the fast branch is very lightweight by reducing the\nmodel width, and yet provides useful fine-grained features for the slow branch.\nOur TranSFormer shows consistent BLEU improvements (larger than 1 BLEU point)\non several machine translation benchmarks."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Learning multiscale Transformer models has been evidenced as a viable\napproach to augmenting machine translation systems. Prior research has\nprimarily focused on treating subwords as basic units in developing such\nsystems. However, the incorporation of fine-grained character-level features\ninto multiscale Transformer has not yet been explored. In this work, we present\na \\textbf{S}low-\\textbf{F}ast two-stream learning model, referred to as\nTran\\textbf{SF}ormer, which utilizes a ``slow'' branch to deal with subword\nsequences and a ``fast'' branch to deal with longer character sequences. This\nmodel is efficient since the fast branch is very lightweight by reducing the\nmodel width, and yet provides useful fine-grained features for the slow branch.\nOur TranSFormer shows consistent BLEU improvements (larger than 1 BLEU point)\non several machine translation benchmarks.""}, 'authors': [{'name': 'Bei Li'}, {'name': 'Yi Jing'}, {'name': 'Xu Tan'}, {'name': 'Zhen Xing'}, {'name': 'Tong Xiao'}, {'name': 'Jingbo Zhu'}], 'author_detail': {'name': 'Jingbo Zhu'}, 'author': 'Jingbo Zhu', 'arxiv_comment': 'Accepted by Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.16982v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.16982v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.17006v1,2023-05-26 15:04:20+00:00,2023-05-26 15:04:20+00:00,Zero-shot Visual Question Answering with Language Model Feedback,"[arxiv.Result.Author('Yifan Du'), arxiv.Result.Author('Junyi Li'), arxiv.Result.Author('Tianyi Tang'), arxiv.Result.Author('Wayne Xin Zhao'), arxiv.Result.Author('Ji-Rong Wen')]","In this paper, we propose a novel language model guided captioning approach,
LAMOC, for knowledge-based visual question answering (VQA). Our approach
employs the generated captions by a captioning model as the context of an
answer prediction model, which is a Pre-trained Language model (PLM). As the
major contribution, we leverage the guidance and feedback of the prediction
model to improve the capability of the captioning model. In this way, the
captioning model can become aware of the task goal and information need from
the PLM. To develop our approach, we design two specific training stages, where
the first stage adapts the captioning model to the prediction model (selecting
more suitable caption propositions for training) and the second stage tunes the
captioning model according to the task goal (learning from feedback of the
PLM). Extensive experiments demonstrate the effectiveness of the proposed
approach on the knowledge-based VQA task. Specifically, on the challenging
A-OKVQA dataset, LAMOC outperforms several competitive zero-shot methods and
even achieves comparable results to a fine-tuned VLP model. Our code is
publicly available at https://github.com/RUCAIBox/LAMOC.",Accepted by ACL2023 findings,,,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Link('http://arxiv.org/abs/2305.17006v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.17006v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.17006v1,"{'id': 'http://arxiv.org/abs/2305.17006v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.17006v1', 'updated': '2023-05-26T15:04:20Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=15, tm_min=4, tm_sec=20, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2023-05-26T15:04:20Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=15, tm_min=4, tm_sec=20, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'Zero-shot Visual Question Answering with Language Model Feedback', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Zero-shot Visual Question Answering with Language Model Feedback'}, 'summary': 'In this paper, we propose a novel language model guided captioning approach,\nLAMOC, for knowledge-based visual question answering (VQA). Our approach\nemploys the generated captions by a captioning model as the context of an\nanswer prediction model, which is a Pre-trained Language model (PLM). As the\nmajor contribution, we leverage the guidance and feedback of the prediction\nmodel to improve the capability of the captioning model. In this way, the\ncaptioning model can become aware of the task goal and information need from\nthe PLM. To develop our approach, we design two specific training stages, where\nthe first stage adapts the captioning model to the prediction model (selecting\nmore suitable caption propositions for training) and the second stage tunes the\ncaptioning model according to the task goal (learning from feedback of the\nPLM). Extensive experiments demonstrate the effectiveness of the proposed\napproach on the knowledge-based VQA task. Specifically, on the challenging\nA-OKVQA dataset, LAMOC outperforms several competitive zero-shot methods and\neven achieves comparable results to a fine-tuned VLP model. Our code is\npublicly available at https://github.com/RUCAIBox/LAMOC.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'In this paper, we propose a novel language model guided captioning approach,\nLAMOC, for knowledge-based visual question answering (VQA). Our approach\nemploys the generated captions by a captioning model as the context of an\nanswer prediction model, which is a Pre-trained Language model (PLM). As the\nmajor contribution, we leverage the guidance and feedback of the prediction\nmodel to improve the capability of the captioning model. In this way, the\ncaptioning model can become aware of the task goal and information need from\nthe PLM. To develop our approach, we design two specific training stages, where\nthe first stage adapts the captioning model to the prediction model (selecting\nmore suitable caption propositions for training) and the second stage tunes the\ncaptioning model according to the task goal (learning from feedback of the\nPLM). Extensive experiments demonstrate the effectiveness of the proposed\napproach on the knowledge-based VQA task. Specifically, on the challenging\nA-OKVQA dataset, LAMOC outperforms several competitive zero-shot methods and\neven achieves comparable results to a fine-tuned VLP model. Our code is\npublicly available at https://github.com/RUCAIBox/LAMOC.'}, 'authors': [{'name': 'Yifan Du'}, {'name': 'Junyi Li'}, {'name': 'Tianyi Tang'}, {'name': 'Wayne Xin Zhao'}, {'name': 'Ji-Rong Wen'}], 'author_detail': {'name': 'Ji-Rong Wen'}, 'author': 'Ji-Rong Wen', 'arxiv_comment': 'Accepted by ACL2023 findings', 'links': [{'href': 'http://arxiv.org/abs/2305.17006v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.17006v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.17041v1,2023-05-26 15:51:25+00:00,2023-05-26 15:51:25+00:00,RFiD: Towards Rational Fusion-in-Decoder for Open-Domain Question Answering,"[arxiv.Result.Author('Cunxiang Wang'), arxiv.Result.Author('Haofei Yu'), arxiv.Result.Author('Yue Zhang')]","Open-Domain Question Answering (ODQA) systems necessitate a reader model
capable of generating answers by simultaneously referring to multiple passages.
Although representative models like Fusion-in-Decoder (FiD) have been proposed
to address this challenge, these systems can inadvertently rely on spurious
features instead of genuine causal relationships between the question and the
passages to generate answers. To counter this problem, we introduce the
Rational Fusion-in-Decoder (RFiD) model. Our model leverages the encoders of
FiD to differentiate between causal relationships and spurious features,
subsequently guiding the decoder to generate answers informed by this
discernment. Experimental results on two ODQA datasets, Natural Questions (NQ)
and TriviaQA (TQ), demonstrate that our model surpasses previous methods,
achieving improvements of up to 1.5 and 0.7 in Exact Match scores on NQ, and
exhibits an enhanced ability to identify causal relationships.",Accepted by ACL2023 findings,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.17041v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.17041v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.17041v1,"{'id': 'http://arxiv.org/abs/2305.17041v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.17041v1', 'updated': '2023-05-26T15:51:25Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=15, tm_min=51, tm_sec=25, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2023-05-26T15:51:25Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=15, tm_min=51, tm_sec=25, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'RFiD: Towards Rational Fusion-in-Decoder for Open-Domain Question\n  Answering', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'RFiD: Towards Rational Fusion-in-Decoder for Open-Domain Question\n  Answering'}, 'summary': 'Open-Domain Question Answering (ODQA) systems necessitate a reader model\ncapable of generating answers by simultaneously referring to multiple passages.\nAlthough representative models like Fusion-in-Decoder (FiD) have been proposed\nto address this challenge, these systems can inadvertently rely on spurious\nfeatures instead of genuine causal relationships between the question and the\npassages to generate answers. To counter this problem, we introduce the\nRational Fusion-in-Decoder (RFiD) model. Our model leverages the encoders of\nFiD to differentiate between causal relationships and spurious features,\nsubsequently guiding the decoder to generate answers informed by this\ndiscernment. Experimental results on two ODQA datasets, Natural Questions (NQ)\nand TriviaQA (TQ), demonstrate that our model surpasses previous methods,\nachieving improvements of up to 1.5 and 0.7 in Exact Match scores on NQ, and\nexhibits an enhanced ability to identify causal relationships.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Open-Domain Question Answering (ODQA) systems necessitate a reader model\ncapable of generating answers by simultaneously referring to multiple passages.\nAlthough representative models like Fusion-in-Decoder (FiD) have been proposed\nto address this challenge, these systems can inadvertently rely on spurious\nfeatures instead of genuine causal relationships between the question and the\npassages to generate answers. To counter this problem, we introduce the\nRational Fusion-in-Decoder (RFiD) model. Our model leverages the encoders of\nFiD to differentiate between causal relationships and spurious features,\nsubsequently guiding the decoder to generate answers informed by this\ndiscernment. Experimental results on two ODQA datasets, Natural Questions (NQ)\nand TriviaQA (TQ), demonstrate that our model surpasses previous methods,\nachieving improvements of up to 1.5 and 0.7 in Exact Match scores on NQ, and\nexhibits an enhanced ability to identify causal relationships.'}, 'authors': [{'name': 'Cunxiang Wang'}, {'name': 'Haofei Yu'}, {'name': 'Yue Zhang'}], 'author_detail': {'name': 'Yue Zhang'}, 'author': 'Yue Zhang', 'arxiv_comment': 'Accepted by ACL2023 findings', 'links': [{'href': 'http://arxiv.org/abs/2305.17041v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.17041v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.17050v1,2023-05-26 16:00:16+00:00,2023-05-26 16:00:16+00:00,Exploiting Abstract Meaning Representation for Open-Domain Question Answering,"[arxiv.Result.Author('Cunxiang Wang'), arxiv.Result.Author('Zhikun Xu'), arxiv.Result.Author('Qipeng Guo'), arxiv.Result.Author('Xiangkun Hu'), arxiv.Result.Author('Xuefeng Bai'), arxiv.Result.Author('Zheng Zhang'), arxiv.Result.Author('Yue Zhang')]","The Open-Domain Question Answering (ODQA) task involves retrieving and
subsequently generating answers from fine-grained relevant passages within a
database. Current systems leverage Pretrained Language Models (PLMs) to model
the relationship between questions and passages. However, the diversity in
surface form expressions can hinder the model's ability to capture accurate
correlations, especially within complex contexts. Therefore, we utilize
Abstract Meaning Representation (AMR) graphs to assist the model in
understanding complex semantic information. We introduce a method known as
Graph-as-Token (GST) to incorporate AMRs into PLMs. Results from Natural
Questions (NQ) and TriviaQA (TQ) demonstrate that our GST method can
significantly improve performance, resulting in up to 2.44/3.17 Exact Match
score improvements on NQ/TQ respectively. Furthermore, our method enhances
robustness and outperforms alternative Graph Neural Network (GNN) methods for
integrating AMRs. To the best of our knowledge, we are the first to employ
semantic graphs in ODQA.","Accepted by ACL2023 findings, reviewer scores: 4 4 4",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.17050v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.17050v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.17050v1,"{'id': 'http://arxiv.org/abs/2305.17050v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.17050v1', 'updated': '2023-05-26T16:00:16Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=16, tm_min=0, tm_sec=16, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2023-05-26T16:00:16Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=16, tm_min=0, tm_sec=16, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'Exploiting Abstract Meaning Representation for Open-Domain Question\n  Answering', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Exploiting Abstract Meaning Representation for Open-Domain Question\n  Answering'}, 'summary': ""The Open-Domain Question Answering (ODQA) task involves retrieving and\nsubsequently generating answers from fine-grained relevant passages within a\ndatabase. Current systems leverage Pretrained Language Models (PLMs) to model\nthe relationship between questions and passages. However, the diversity in\nsurface form expressions can hinder the model's ability to capture accurate\ncorrelations, especially within complex contexts. Therefore, we utilize\nAbstract Meaning Representation (AMR) graphs to assist the model in\nunderstanding complex semantic information. We introduce a method known as\nGraph-as-Token (GST) to incorporate AMRs into PLMs. Results from Natural\nQuestions (NQ) and TriviaQA (TQ) demonstrate that our GST method can\nsignificantly improve performance, resulting in up to 2.44/3.17 Exact Match\nscore improvements on NQ/TQ respectively. Furthermore, our method enhances\nrobustness and outperforms alternative Graph Neural Network (GNN) methods for\nintegrating AMRs. To the best of our knowledge, we are the first to employ\nsemantic graphs in ODQA."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""The Open-Domain Question Answering (ODQA) task involves retrieving and\nsubsequently generating answers from fine-grained relevant passages within a\ndatabase. Current systems leverage Pretrained Language Models (PLMs) to model\nthe relationship between questions and passages. However, the diversity in\nsurface form expressions can hinder the model's ability to capture accurate\ncorrelations, especially within complex contexts. Therefore, we utilize\nAbstract Meaning Representation (AMR) graphs to assist the model in\nunderstanding complex semantic information. We introduce a method known as\nGraph-as-Token (GST) to incorporate AMRs into PLMs. Results from Natural\nQuestions (NQ) and TriviaQA (TQ) demonstrate that our GST method can\nsignificantly improve performance, resulting in up to 2.44/3.17 Exact Match\nscore improvements on NQ/TQ respectively. Furthermore, our method enhances\nrobustness and outperforms alternative Graph Neural Network (GNN) methods for\nintegrating AMRs. To the best of our knowledge, we are the first to employ\nsemantic graphs in ODQA.""}, 'authors': [{'name': 'Cunxiang Wang'}, {'name': 'Zhikun Xu'}, {'name': 'Qipeng Guo'}, {'name': 'Xiangkun Hu'}, {'name': 'Xuefeng Bai'}, {'name': 'Zheng Zhang'}, {'name': 'Yue Zhang'}], 'author_detail': {'name': 'Yue Zhang'}, 'author': 'Yue Zhang', 'arxiv_comment': 'Accepted by ACL2023 findings, reviewer scores: 4 4 4', 'links': [{'href': 'http://arxiv.org/abs/2305.17050v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.17050v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.17378v1,2023-05-27 06:09:03+00:00,2023-05-27 06:09:03+00:00,Improving Generalization in Language Model-Based Text-to-SQL Semantic Parsing: Two Simple Semantic Boundary-Based Techniques,"[arxiv.Result.Author('Daking Rai'), arxiv.Result.Author('Bailin Wang'), arxiv.Result.Author('Yilun Zhou'), arxiv.Result.Author('Ziyu Yao')]","Compositional and domain generalization present significant challenges in
semantic parsing, even for state-of-the-art semantic parsers based on
pre-trained language models (LMs). In this study, we empirically investigate
improving an LM's generalization in semantic parsing with two simple
techniques: at the token level, we introduce a token preprocessing method to
preserve the semantic boundaries of tokens produced by LM tokenizers; at the
sequence level, we propose to use special tokens to mark the boundaries of
components aligned between input and output. Our experimental results on two
text-to-SQL semantic parsing datasets show that our token preprocessing,
although simple, can substantially improve the LM performance on both types of
generalization, and our component boundary marking method is particularly
helpful for compositional generalization.","9 pages, to be published in ACL2023",,,cs.CL,"['cs.CL', 'cs.AI', 'I.2.7']","[arxiv.Result.Link('http://arxiv.org/abs/2305.17378v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.17378v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.17378v1,"{'id': 'http://arxiv.org/abs/2305.17378v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.17378v1', 'updated': '2023-05-27T06:09:03Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=27, tm_hour=6, tm_min=9, tm_sec=3, tm_wday=5, tm_yday=147, tm_isdst=0), 'published': '2023-05-27T06:09:03Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=27, tm_hour=6, tm_min=9, tm_sec=3, tm_wday=5, tm_yday=147, tm_isdst=0), 'title': 'Improving Generalization in Language Model-Based Text-to-SQL Semantic\n  Parsing: Two Simple Semantic Boundary-Based Techniques', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Improving Generalization in Language Model-Based Text-to-SQL Semantic\n  Parsing: Two Simple Semantic Boundary-Based Techniques'}, 'summary': ""Compositional and domain generalization present significant challenges in\nsemantic parsing, even for state-of-the-art semantic parsers based on\npre-trained language models (LMs). In this study, we empirically investigate\nimproving an LM's generalization in semantic parsing with two simple\ntechniques: at the token level, we introduce a token preprocessing method to\npreserve the semantic boundaries of tokens produced by LM tokenizers; at the\nsequence level, we propose to use special tokens to mark the boundaries of\ncomponents aligned between input and output. Our experimental results on two\ntext-to-SQL semantic parsing datasets show that our token preprocessing,\nalthough simple, can substantially improve the LM performance on both types of\ngeneralization, and our component boundary marking method is particularly\nhelpful for compositional generalization."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Compositional and domain generalization present significant challenges in\nsemantic parsing, even for state-of-the-art semantic parsers based on\npre-trained language models (LMs). In this study, we empirically investigate\nimproving an LM's generalization in semantic parsing with two simple\ntechniques: at the token level, we introduce a token preprocessing method to\npreserve the semantic boundaries of tokens produced by LM tokenizers; at the\nsequence level, we propose to use special tokens to mark the boundaries of\ncomponents aligned between input and output. Our experimental results on two\ntext-to-SQL semantic parsing datasets show that our token preprocessing,\nalthough simple, can substantially improve the LM performance on both types of\ngeneralization, and our component boundary marking method is particularly\nhelpful for compositional generalization.""}, 'authors': [{'name': 'Daking Rai'}, {'name': 'Bailin Wang'}, {'name': 'Yilun Zhou'}, {'name': 'Ziyu Yao'}], 'author_detail': {'name': 'Ziyu Yao'}, 'author': 'Ziyu Yao', 'arxiv_comment': '9 pages, to be published in ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.17378v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.17378v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.7', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.17448v1,2023-05-27 11:21:32+00:00,2023-05-27 11:21:32+00:00,Measuring Your ASTE Models in The Wild: A Diversified Multi-domain Dataset For Aspect Sentiment Triplet Extraction,"[arxiv.Result.Author('Ting Xu'), arxiv.Result.Author('Huiyun Yang'), arxiv.Result.Author('Zhen Wu'), arxiv.Result.Author('Jiaze Chen'), arxiv.Result.Author('Fei Zhao'), arxiv.Result.Author('Xinyu Dai')]","Aspect Sentiment Triplet Extraction (ASTE) is widely used in various
applications. However, existing ASTE datasets are limited in their ability to
represent real-world scenarios, hindering the advancement of research in this
area. In this paper, we introduce a new dataset, named DMASTE, which is
manually annotated to better fit real-world scenarios by providing more diverse
and realistic reviews for the task. The dataset includes various lengths,
diverse expressions, more aspect types, and more domains than existing
datasets. We conduct extensive experiments on DMASTE in multiple settings to
evaluate previous ASTE approaches. Empirical results demonstrate that DMASTE is
a more challenging ASTE dataset. Further analyses of in-domain and cross-domain
settings provide promising directions for future research. Our code and dataset
are available at https://github.com/NJUNLP/DMASTE.","15pages, 5 figures, ACL2023",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.17448v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.17448v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.17448v1,"{'id': 'http://arxiv.org/abs/2305.17448v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.17448v1', 'updated': '2023-05-27T11:21:32Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=27, tm_hour=11, tm_min=21, tm_sec=32, tm_wday=5, tm_yday=147, tm_isdst=0), 'published': '2023-05-27T11:21:32Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=27, tm_hour=11, tm_min=21, tm_sec=32, tm_wday=5, tm_yday=147, tm_isdst=0), 'title': 'Measuring Your ASTE Models in The Wild: A Diversified Multi-domain\n  Dataset For Aspect Sentiment Triplet Extraction', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Measuring Your ASTE Models in The Wild: A Diversified Multi-domain\n  Dataset For Aspect Sentiment Triplet Extraction'}, 'summary': 'Aspect Sentiment Triplet Extraction (ASTE) is widely used in various\napplications. However, existing ASTE datasets are limited in their ability to\nrepresent real-world scenarios, hindering the advancement of research in this\narea. In this paper, we introduce a new dataset, named DMASTE, which is\nmanually annotated to better fit real-world scenarios by providing more diverse\nand realistic reviews for the task. The dataset includes various lengths,\ndiverse expressions, more aspect types, and more domains than existing\ndatasets. We conduct extensive experiments on DMASTE in multiple settings to\nevaluate previous ASTE approaches. Empirical results demonstrate that DMASTE is\na more challenging ASTE dataset. Further analyses of in-domain and cross-domain\nsettings provide promising directions for future research. Our code and dataset\nare available at https://github.com/NJUNLP/DMASTE.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Aspect Sentiment Triplet Extraction (ASTE) is widely used in various\napplications. However, existing ASTE datasets are limited in their ability to\nrepresent real-world scenarios, hindering the advancement of research in this\narea. In this paper, we introduce a new dataset, named DMASTE, which is\nmanually annotated to better fit real-world scenarios by providing more diverse\nand realistic reviews for the task. The dataset includes various lengths,\ndiverse expressions, more aspect types, and more domains than existing\ndatasets. We conduct extensive experiments on DMASTE in multiple settings to\nevaluate previous ASTE approaches. Empirical results demonstrate that DMASTE is\na more challenging ASTE dataset. Further analyses of in-domain and cross-domain\nsettings provide promising directions for future research. Our code and dataset\nare available at https://github.com/NJUNLP/DMASTE.'}, 'authors': [{'name': 'Ting Xu'}, {'name': 'Huiyun Yang'}, {'name': 'Zhen Wu'}, {'name': 'Jiaze Chen'}, {'name': 'Fei Zhao'}, {'name': 'Xinyu Dai'}], 'author_detail': {'name': 'Xinyu Dai'}, 'author': 'Xinyu Dai', 'arxiv_comment': '15pages, 5 figures, ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.17448v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.17448v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.17699v1,2023-05-28 12:01:34+00:00,2023-05-28 12:01:34+00:00,Decoupling Pseudo Label Disambiguation and Representation Learning for Generalized Intent Discovery,"[arxiv.Result.Author('Yutao Mou'), arxiv.Result.Author('Xiaoshuai Song'), arxiv.Result.Author('Keqing He'), arxiv.Result.Author('Chen Zeng'), arxiv.Result.Author('Pei Wang'), arxiv.Result.Author('Jingang Wang'), arxiv.Result.Author('Yunsen Xian'), arxiv.Result.Author('Weiran Xu')]","Generalized intent discovery aims to extend a closed-set in-domain intent
classifier to an open-world intent set including in-domain and out-of-domain
intents. The key challenges lie in pseudo label disambiguation and
representation learning. Previous methods suffer from a coupling of pseudo
label disambiguation and representation learning, that is, the reliability of
pseudo labels relies on representation learning, and representation learning is
restricted by pseudo labels in turn. In this paper, we propose a decoupled
prototype learning framework (DPL) to decouple pseudo label disambiguation and
representation learning. Specifically, we firstly introduce prototypical
contrastive representation learning (PCL) to get discriminative
representations. And then we adopt a prototype-based label disambiguation
method (PLD) to obtain pseudo labels. We theoretically prove that PCL and PLD
work in a collaborative fashion and facilitate pseudo label disambiguation.
Experiments and analysis on three benchmark datasets show the effectiveness of
our method.",Accepted at ACL2023 main conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.17699v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.17699v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.17699v1,"{'id': 'http://arxiv.org/abs/2305.17699v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.17699v1', 'updated': '2023-05-28T12:01:34Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=28, tm_hour=12, tm_min=1, tm_sec=34, tm_wday=6, tm_yday=148, tm_isdst=0), 'published': '2023-05-28T12:01:34Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=28, tm_hour=12, tm_min=1, tm_sec=34, tm_wday=6, tm_yday=148, tm_isdst=0), 'title': 'Decoupling Pseudo Label Disambiguation and Representation Learning for\n  Generalized Intent Discovery', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Decoupling Pseudo Label Disambiguation and Representation Learning for\n  Generalized Intent Discovery'}, 'summary': 'Generalized intent discovery aims to extend a closed-set in-domain intent\nclassifier to an open-world intent set including in-domain and out-of-domain\nintents. The key challenges lie in pseudo label disambiguation and\nrepresentation learning. Previous methods suffer from a coupling of pseudo\nlabel disambiguation and representation learning, that is, the reliability of\npseudo labels relies on representation learning, and representation learning is\nrestricted by pseudo labels in turn. In this paper, we propose a decoupled\nprototype learning framework (DPL) to decouple pseudo label disambiguation and\nrepresentation learning. Specifically, we firstly introduce prototypical\ncontrastive representation learning (PCL) to get discriminative\nrepresentations. And then we adopt a prototype-based label disambiguation\nmethod (PLD) to obtain pseudo labels. We theoretically prove that PCL and PLD\nwork in a collaborative fashion and facilitate pseudo label disambiguation.\nExperiments and analysis on three benchmark datasets show the effectiveness of\nour method.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Generalized intent discovery aims to extend a closed-set in-domain intent\nclassifier to an open-world intent set including in-domain and out-of-domain\nintents. The key challenges lie in pseudo label disambiguation and\nrepresentation learning. Previous methods suffer from a coupling of pseudo\nlabel disambiguation and representation learning, that is, the reliability of\npseudo labels relies on representation learning, and representation learning is\nrestricted by pseudo labels in turn. In this paper, we propose a decoupled\nprototype learning framework (DPL) to decouple pseudo label disambiguation and\nrepresentation learning. Specifically, we firstly introduce prototypical\ncontrastive representation learning (PCL) to get discriminative\nrepresentations. And then we adopt a prototype-based label disambiguation\nmethod (PLD) to obtain pseudo labels. We theoretically prove that PCL and PLD\nwork in a collaborative fashion and facilitate pseudo label disambiguation.\nExperiments and analysis on three benchmark datasets show the effectiveness of\nour method.'}, 'authors': [{'name': 'Yutao Mou'}, {'name': 'Xiaoshuai Song'}, {'name': 'Keqing He'}, {'name': 'Chen Zeng'}, {'name': 'Pei Wang'}, {'name': 'Jingang Wang'}, {'name': 'Yunsen Xian'}, {'name': 'Weiran Xu'}], 'author_detail': {'name': 'Weiran Xu'}, 'author': 'Weiran Xu', 'arxiv_comment': 'Accepted at ACL2023 main conference', 'links': [{'href': 'http://arxiv.org/abs/2305.17699v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.17699v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.18977v2,2023-05-31 01:50:40+00:00,2023-05-30 12:19:30+00:00,Cross Encoding as Augmentation: Towards Effective Educational Text Classification,"[arxiv.Result.Author('Hyun Seung Lee'), arxiv.Result.Author('Seungtaek Choi'), arxiv.Result.Author('Yunsung Lee'), arxiv.Result.Author('Hyeongdon Moon'), arxiv.Result.Author('Shinhyeok Oh'), arxiv.Result.Author('Myeongho Jeong'), arxiv.Result.Author('Hyojun Go'), arxiv.Result.Author('Christian Wallraven')]","Text classification in education, usually called auto-tagging, is the
automated process of assigning relevant tags to educational content, such as
questions and textbooks. However, auto-tagging suffers from a data scarcity
problem, which stems from two major challenges: 1) it possesses a large tag
space and 2) it is multi-label. Though a retrieval approach is reportedly good
at low-resource scenarios, there have been fewer efforts to directly address
the data scarcity problem. To mitigate these issues, here we propose a novel
retrieval approach CEAA that provides effective learning in educational text
classification. Our main contributions are as follows: 1) we leverage transfer
learning from question-answering datasets, and 2) we propose a simple but
effective data augmentation method introducing cross-encoder style texts to a
bi-encoder architecture for more efficient inference. An extensive set of
experiments shows that our proposed method is effective in multi-label
scenarios and low-resource tags compared to state-of-the-art models.",Accepted to Findings of ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.18977v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.18977v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.18977v2,"{'id': 'http://arxiv.org/abs/2305.18977v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.18977v2', 'updated': '2023-05-31T01:50:40Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=31, tm_hour=1, tm_min=50, tm_sec=40, tm_wday=2, tm_yday=151, tm_isdst=0), 'published': '2023-05-30T12:19:30Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=30, tm_hour=12, tm_min=19, tm_sec=30, tm_wday=1, tm_yday=150, tm_isdst=0), 'title': 'Cross Encoding as Augmentation: Towards Effective Educational Text\n  Classification', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Cross Encoding as Augmentation: Towards Effective Educational Text\n  Classification'}, 'summary': 'Text classification in education, usually called auto-tagging, is the\nautomated process of assigning relevant tags to educational content, such as\nquestions and textbooks. However, auto-tagging suffers from a data scarcity\nproblem, which stems from two major challenges: 1) it possesses a large tag\nspace and 2) it is multi-label. Though a retrieval approach is reportedly good\nat low-resource scenarios, there have been fewer efforts to directly address\nthe data scarcity problem. To mitigate these issues, here we propose a novel\nretrieval approach CEAA that provides effective learning in educational text\nclassification. Our main contributions are as follows: 1) we leverage transfer\nlearning from question-answering datasets, and 2) we propose a simple but\neffective data augmentation method introducing cross-encoder style texts to a\nbi-encoder architecture for more efficient inference. An extensive set of\nexperiments shows that our proposed method is effective in multi-label\nscenarios and low-resource tags compared to state-of-the-art models.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Text classification in education, usually called auto-tagging, is the\nautomated process of assigning relevant tags to educational content, such as\nquestions and textbooks. However, auto-tagging suffers from a data scarcity\nproblem, which stems from two major challenges: 1) it possesses a large tag\nspace and 2) it is multi-label. Though a retrieval approach is reportedly good\nat low-resource scenarios, there have been fewer efforts to directly address\nthe data scarcity problem. To mitigate these issues, here we propose a novel\nretrieval approach CEAA that provides effective learning in educational text\nclassification. Our main contributions are as follows: 1) we leverage transfer\nlearning from question-answering datasets, and 2) we propose a simple but\neffective data augmentation method introducing cross-encoder style texts to a\nbi-encoder architecture for more efficient inference. An extensive set of\nexperiments shows that our proposed method is effective in multi-label\nscenarios and low-resource tags compared to state-of-the-art models.'}, 'authors': [{'name': 'Hyun Seung Lee'}, {'name': 'Seungtaek Choi'}, {'name': 'Yunsung Lee'}, {'name': 'Hyeongdon Moon'}, {'name': 'Shinhyeok Oh'}, {'name': 'Myeongho Jeong'}, {'name': 'Hyojun Go'}, {'name': 'Christian Wallraven'}], 'author_detail': {'name': 'Christian Wallraven'}, 'author': 'Christian Wallraven', 'arxiv_comment': 'Accepted to Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.18977v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.18977v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.19474v1,2023-05-31 01:04:20+00:00,2023-05-31 01:04:20+00:00,Ethical Considerations for Machine Translation of Indigenous Languages: Giving a Voice to the Speakers,"[arxiv.Result.Author('Manuel Mager'), arxiv.Result.Author('Elisabeth Mager'), arxiv.Result.Author('Katharina Kann'), arxiv.Result.Author('Ngoc Thang Vu')]","In recent years machine translation has become very successful for
high-resource language pairs. This has also sparked new interest in research on
the automatic translation of low-resource languages, including Indigenous
languages. However, the latter are deeply related to the ethnic and cultural
groups that speak (or used to speak) them. The data collection, modeling and
deploying machine translation systems thus result in new ethical questions that
must be addressed. Motivated by this, we first survey the existing literature
on ethical considerations for the documentation, translation, and general
natural language processing for Indigenous languages. Afterward, we conduct and
analyze an interview study to shed light on the positions of community leaders,
teachers, and language activists regarding ethical concerns for the automatic
translation of their languages. Our results show that the inclusion, at
different degrees, of native speakers and community members is vital to
performing better and more ethical research on Indigenous languages.",Accepted to ACL2023 Main Conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.19474v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.19474v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.19474v1,"{'id': 'http://arxiv.org/abs/2305.19474v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.19474v1', 'updated': '2023-05-31T01:04:20Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=31, tm_hour=1, tm_min=4, tm_sec=20, tm_wday=2, tm_yday=151, tm_isdst=0), 'published': '2023-05-31T01:04:20Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=31, tm_hour=1, tm_min=4, tm_sec=20, tm_wday=2, tm_yday=151, tm_isdst=0), 'title': 'Ethical Considerations for Machine Translation of Indigenous Languages:\n  Giving a Voice to the Speakers', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Ethical Considerations for Machine Translation of Indigenous Languages:\n  Giving a Voice to the Speakers'}, 'summary': 'In recent years machine translation has become very successful for\nhigh-resource language pairs. This has also sparked new interest in research on\nthe automatic translation of low-resource languages, including Indigenous\nlanguages. However, the latter are deeply related to the ethnic and cultural\ngroups that speak (or used to speak) them. The data collection, modeling and\ndeploying machine translation systems thus result in new ethical questions that\nmust be addressed. Motivated by this, we first survey the existing literature\non ethical considerations for the documentation, translation, and general\nnatural language processing for Indigenous languages. Afterward, we conduct and\nanalyze an interview study to shed light on the positions of community leaders,\nteachers, and language activists regarding ethical concerns for the automatic\ntranslation of their languages. Our results show that the inclusion, at\ndifferent degrees, of native speakers and community members is vital to\nperforming better and more ethical research on Indigenous languages.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'In recent years machine translation has become very successful for\nhigh-resource language pairs. This has also sparked new interest in research on\nthe automatic translation of low-resource languages, including Indigenous\nlanguages. However, the latter are deeply related to the ethnic and cultural\ngroups that speak (or used to speak) them. The data collection, modeling and\ndeploying machine translation systems thus result in new ethical questions that\nmust be addressed. Motivated by this, we first survey the existing literature\non ethical considerations for the documentation, translation, and general\nnatural language processing for Indigenous languages. Afterward, we conduct and\nanalyze an interview study to shed light on the positions of community leaders,\nteachers, and language activists regarding ethical concerns for the automatic\ntranslation of their languages. Our results show that the inclusion, at\ndifferent degrees, of native speakers and community members is vital to\nperforming better and more ethical research on Indigenous languages.'}, 'authors': [{'name': 'Manuel Mager'}, {'name': 'Elisabeth Mager'}, {'name': 'Katharina Kann'}, {'name': 'Ngoc Thang Vu'}], 'author_detail': {'name': 'Ngoc Thang Vu'}, 'author': 'Ngoc Thang Vu', 'arxiv_comment': 'Accepted to ACL2023 Main Conference', 'links': [{'href': 'http://arxiv.org/abs/2305.19474v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.19474v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.19748v2,2023-06-02 14:18:44+00:00,2023-05-31 11:29:04+00:00,UKP-SQuARE: An Interactive Tool for Teaching Question Answering,"[arxiv.Result.Author('Haishuo Fang'), arxiv.Result.Author('Haritz Puerto'), arxiv.Result.Author('Iryna Gurevych')]","The exponential growth of question answering (QA) has made it an
indispensable topic in any Natural Language Processing (NLP) course.
Additionally, the breadth of QA derived from this exponential growth makes it
an ideal scenario for teaching related NLP topics such as information
retrieval, explainability, and adversarial attacks among others. In this paper,
we introduce UKP-SQuARE as a platform for QA education. This platform provides
an interactive environment where students can run, compare, and analyze various
QA models from different perspectives, such as general behavior,
explainability, and robustness. Therefore, students can get a first-hand
experience in different QA techniques during the class. Thanks to this, we
propose a learner-centered approach for QA education in which students
proactively learn theoretical concepts and acquire problem-solving skills
through interactive exploration, experimentation, and practical assignments,
rather than solely relying on traditional lectures. To evaluate the
effectiveness of UKP-SQuARE in teaching scenarios, we adopted it in a
postgraduate NLP course and surveyed the students after the course. Their
positive feedback shows the platform's effectiveness in their course and
invites a wider adoption.","Accepted by BEA workshop, ACL2023",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.19748v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.19748v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.19748v2,"{'id': 'http://arxiv.org/abs/2305.19748v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.19748v2', 'updated': '2023-06-02T14:18:44Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=2, tm_hour=14, tm_min=18, tm_sec=44, tm_wday=4, tm_yday=153, tm_isdst=0), 'published': '2023-05-31T11:29:04Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=31, tm_hour=11, tm_min=29, tm_sec=4, tm_wday=2, tm_yday=151, tm_isdst=0), 'title': 'UKP-SQuARE: An Interactive Tool for Teaching Question Answering', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'UKP-SQuARE: An Interactive Tool for Teaching Question Answering'}, 'summary': ""The exponential growth of question answering (QA) has made it an\nindispensable topic in any Natural Language Processing (NLP) course.\nAdditionally, the breadth of QA derived from this exponential growth makes it\nan ideal scenario for teaching related NLP topics such as information\nretrieval, explainability, and adversarial attacks among others. In this paper,\nwe introduce UKP-SQuARE as a platform for QA education. This platform provides\nan interactive environment where students can run, compare, and analyze various\nQA models from different perspectives, such as general behavior,\nexplainability, and robustness. Therefore, students can get a first-hand\nexperience in different QA techniques during the class. Thanks to this, we\npropose a learner-centered approach for QA education in which students\nproactively learn theoretical concepts and acquire problem-solving skills\nthrough interactive exploration, experimentation, and practical assignments,\nrather than solely relying on traditional lectures. To evaluate the\neffectiveness of UKP-SQuARE in teaching scenarios, we adopted it in a\npostgraduate NLP course and surveyed the students after the course. Their\npositive feedback shows the platform's effectiveness in their course and\ninvites a wider adoption."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""The exponential growth of question answering (QA) has made it an\nindispensable topic in any Natural Language Processing (NLP) course.\nAdditionally, the breadth of QA derived from this exponential growth makes it\nan ideal scenario for teaching related NLP topics such as information\nretrieval, explainability, and adversarial attacks among others. In this paper,\nwe introduce UKP-SQuARE as a platform for QA education. This platform provides\nan interactive environment where students can run, compare, and analyze various\nQA models from different perspectives, such as general behavior,\nexplainability, and robustness. Therefore, students can get a first-hand\nexperience in different QA techniques during the class. Thanks to this, we\npropose a learner-centered approach for QA education in which students\nproactively learn theoretical concepts and acquire problem-solving skills\nthrough interactive exploration, experimentation, and practical assignments,\nrather than solely relying on traditional lectures. To evaluate the\neffectiveness of UKP-SQuARE in teaching scenarios, we adopted it in a\npostgraduate NLP course and surveyed the students after the course. Their\npositive feedback shows the platform's effectiveness in their course and\ninvites a wider adoption.""}, 'authors': [{'name': 'Haishuo Fang'}, {'name': 'Haritz Puerto'}, {'name': 'Iryna Gurevych'}], 'author_detail': {'name': 'Iryna Gurevych'}, 'author': 'Iryna Gurevych', 'arxiv_comment': 'Accepted by BEA workshop, ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.19748v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.19748v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.00014v1,2023-05-30 08:41:33+00:00,2023-05-30 08:41:33+00:00,PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language Models,"[arxiv.Result.Author('Zhuocheng Gong'), arxiv.Result.Author('Jiahao Liu'), arxiv.Result.Author('Qifan Wang'), arxiv.Result.Author('Yang Yang'), arxiv.Result.Author('Jingang Wang'), arxiv.Result.Author('Wei Wu'), arxiv.Result.Author('Yunsen Xian'), arxiv.Result.Author('Dongyan Zhao'), arxiv.Result.Author('Rui Yan')]","While transformer-based pre-trained language models (PLMs) have dominated a
number of NLP applications, these models are heavy to deploy and expensive to
use. Therefore, effectively compressing large-scale PLMs becomes an
increasingly important problem. Quantization, which represents high-precision
tensors with low-bit fix-point format, is a viable solution. However, most
existing quantization methods are task-specific, requiring customized training
and quantization with a large number of trainable parameters on each individual
task. Inspired by the observation that the over-parameterization nature of PLMs
makes it possible to freeze most of the parameters during the fine-tuning
stage, in this work, we propose a novel ``quantize before fine-tuning''
framework, PreQuant, that differs from both quantization-aware training and
post-training quantization. PreQuant is compatible with various quantization
strategies, with outlier-aware parameter-efficient fine-tuning incorporated to
correct the induced quantization error. We demonstrate the effectiveness of
PreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an
empirical investigation into the workflow of PreQuant, which sheds light on its
efficacy.",Findings of ACL2023,,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2306.00014v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.00014v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.00014v1,"{'id': 'http://arxiv.org/abs/2306.00014v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.00014v1', 'updated': '2023-05-30T08:41:33Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=30, tm_hour=8, tm_min=41, tm_sec=33, tm_wday=1, tm_yday=150, tm_isdst=0), 'published': '2023-05-30T08:41:33Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=30, tm_hour=8, tm_min=41, tm_sec=33, tm_wday=1, tm_yday=150, tm_isdst=0), 'title': 'PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language\n  Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'PreQuant: A Task-agnostic Quantization Approach for Pre-trained Language\n  Models'}, 'summary': ""While transformer-based pre-trained language models (PLMs) have dominated a\nnumber of NLP applications, these models are heavy to deploy and expensive to\nuse. Therefore, effectively compressing large-scale PLMs becomes an\nincreasingly important problem. Quantization, which represents high-precision\ntensors with low-bit fix-point format, is a viable solution. However, most\nexisting quantization methods are task-specific, requiring customized training\nand quantization with a large number of trainable parameters on each individual\ntask. Inspired by the observation that the over-parameterization nature of PLMs\nmakes it possible to freeze most of the parameters during the fine-tuning\nstage, in this work, we propose a novel ``quantize before fine-tuning''\nframework, PreQuant, that differs from both quantization-aware training and\npost-training quantization. PreQuant is compatible with various quantization\nstrategies, with outlier-aware parameter-efficient fine-tuning incorporated to\ncorrect the induced quantization error. We demonstrate the effectiveness of\nPreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an\nempirical investigation into the workflow of PreQuant, which sheds light on its\nefficacy."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""While transformer-based pre-trained language models (PLMs) have dominated a\nnumber of NLP applications, these models are heavy to deploy and expensive to\nuse. Therefore, effectively compressing large-scale PLMs becomes an\nincreasingly important problem. Quantization, which represents high-precision\ntensors with low-bit fix-point format, is a viable solution. However, most\nexisting quantization methods are task-specific, requiring customized training\nand quantization with a large number of trainable parameters on each individual\ntask. Inspired by the observation that the over-parameterization nature of PLMs\nmakes it possible to freeze most of the parameters during the fine-tuning\nstage, in this work, we propose a novel ``quantize before fine-tuning''\nframework, PreQuant, that differs from both quantization-aware training and\npost-training quantization. PreQuant is compatible with various quantization\nstrategies, with outlier-aware parameter-efficient fine-tuning incorporated to\ncorrect the induced quantization error. We demonstrate the effectiveness of\nPreQuant on the GLUE benchmark using BERT, RoBERTa, and T5. We also provide an\nempirical investigation into the workflow of PreQuant, which sheds light on its\nefficacy.""}, 'authors': [{'name': 'Zhuocheng Gong'}, {'name': 'Jiahao Liu'}, {'name': 'Qifan Wang'}, {'name': 'Yang Yang'}, {'name': 'Jingang Wang'}, {'name': 'Wei Wu'}, {'name': 'Yunsen Xian'}, {'name': 'Dongyan Zhao'}, {'name': 'Rui Yan'}], 'author_detail': {'name': 'Rui Yan'}, 'author': 'Rui Yan', 'arxiv_comment': 'Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.00014v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.00014v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.00124v1,2023-05-31 19:00:33+00:00,2023-05-31 19:00:33+00:00,Pre-Trained Language-Meaning Models for Multilingual Parsing and Generation,"[arxiv.Result.Author('Chunliu Wang'), arxiv.Result.Author('Huiyuan Lai'), arxiv.Result.Author('Malvina Nissim'), arxiv.Result.Author('Johan Bos')]","Pre-trained language models (PLMs) have achieved great success in NLP and
have recently been used for tasks in computational semantics. However, these
tasks do not fully benefit from PLMs since meaning representations are not
explicitly included in the pre-training stage. We introduce multilingual
pre-trained language-meaning models based on Discourse Representation
Structures (DRSs), including meaning representations besides natural language
texts in the same model, and design a new strategy to reduce the gap between
the pre-training and fine-tuning objectives. Since DRSs are language neutral,
cross-lingual transfer learning is adopted to further improve the performance
of non-English tasks. Automatic evaluation results show that our approach
achieves the best performance on both the multilingual DRS parsing and
DRS-to-text generation tasks. Correlation analysis between automatic metrics
and human judgements on the generation task further validates the effectiveness
of our model. Human inspection reveals that out-of-vocabulary tokens are the
main cause of erroneous results.",Accepted by ACL2023 findings,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.00124v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.00124v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.00124v1,"{'id': 'http://arxiv.org/abs/2306.00124v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.00124v1', 'updated': '2023-05-31T19:00:33Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=31, tm_hour=19, tm_min=0, tm_sec=33, tm_wday=2, tm_yday=151, tm_isdst=0), 'published': '2023-05-31T19:00:33Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=31, tm_hour=19, tm_min=0, tm_sec=33, tm_wday=2, tm_yday=151, tm_isdst=0), 'title': 'Pre-Trained Language-Meaning Models for Multilingual Parsing and\n  Generation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Pre-Trained Language-Meaning Models for Multilingual Parsing and\n  Generation'}, 'summary': 'Pre-trained language models (PLMs) have achieved great success in NLP and\nhave recently been used for tasks in computational semantics. However, these\ntasks do not fully benefit from PLMs since meaning representations are not\nexplicitly included in the pre-training stage. We introduce multilingual\npre-trained language-meaning models based on Discourse Representation\nStructures (DRSs), including meaning representations besides natural language\ntexts in the same model, and design a new strategy to reduce the gap between\nthe pre-training and fine-tuning objectives. Since DRSs are language neutral,\ncross-lingual transfer learning is adopted to further improve the performance\nof non-English tasks. Automatic evaluation results show that our approach\nachieves the best performance on both the multilingual DRS parsing and\nDRS-to-text generation tasks. Correlation analysis between automatic metrics\nand human judgements on the generation task further validates the effectiveness\nof our model. Human inspection reveals that out-of-vocabulary tokens are the\nmain cause of erroneous results.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Pre-trained language models (PLMs) have achieved great success in NLP and\nhave recently been used for tasks in computational semantics. However, these\ntasks do not fully benefit from PLMs since meaning representations are not\nexplicitly included in the pre-training stage. We introduce multilingual\npre-trained language-meaning models based on Discourse Representation\nStructures (DRSs), including meaning representations besides natural language\ntexts in the same model, and design a new strategy to reduce the gap between\nthe pre-training and fine-tuning objectives. Since DRSs are language neutral,\ncross-lingual transfer learning is adopted to further improve the performance\nof non-English tasks. Automatic evaluation results show that our approach\nachieves the best performance on both the multilingual DRS parsing and\nDRS-to-text generation tasks. Correlation analysis between automatic metrics\nand human judgements on the generation task further validates the effectiveness\nof our model. Human inspection reveals that out-of-vocabulary tokens are the\nmain cause of erroneous results.'}, 'authors': [{'name': 'Chunliu Wang'}, {'name': 'Huiyuan Lai'}, {'name': 'Malvina Nissim'}, {'name': 'Johan Bos'}], 'author_detail': {'name': 'Johan Bos'}, 'author': 'Johan Bos', 'arxiv_comment': 'Accepted by ACL2023 findings', 'links': [{'href': 'http://arxiv.org/abs/2306.00124v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.00124v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.01318v1,2023-06-02 07:33:47+00:00,2023-06-02 07:33:47+00:00,Text Style Transfer Back-Translation,"[arxiv.Result.Author('Daimeng Wei'), arxiv.Result.Author('Zhanglin Wu'), arxiv.Result.Author('Hengchao Shang'), arxiv.Result.Author('Zongyao Li'), arxiv.Result.Author('Minghan Wang'), arxiv.Result.Author('Jiaxin Guo'), arxiv.Result.Author('Xiaoyu Chen'), arxiv.Result.Author('Zhengzhe Yu'), arxiv.Result.Author('Hao Yang')]","Back Translation (BT) is widely used in the field of machine translation, as
it has been proved effective for enhancing translation quality. However, BT
mainly improves the translation of inputs that share a similar style (to be
more specific, translation-like inputs), since the source side of BT data is
machine-translated. For natural inputs, BT brings only slight improvements and
sometimes even adverse effects. To address this issue, we propose Text Style
Transfer Back Translation (TST BT), which uses a style transfer model to modify
the source side of BT data. By making the style of source-side text more
natural, we aim to improve the translation of natural inputs. Our experiments
on various language pairs, including both high-resource and low-resource ones,
demonstrate that TST BT significantly improves translation performance against
popular BT benchmarks. In addition, TST BT is proved to be effective in domain
adaptation so this strategy can be regarded as a general data augmentation
method. Our training code and text style transfer model are open-sourced.","acl2023, 14 pages, 4 figures, 19 tables",,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2306.01318v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.01318v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.01318v1,"{'id': 'http://arxiv.org/abs/2306.01318v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.01318v1', 'updated': '2023-06-02T07:33:47Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=2, tm_hour=7, tm_min=33, tm_sec=47, tm_wday=4, tm_yday=153, tm_isdst=0), 'published': '2023-06-02T07:33:47Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=2, tm_hour=7, tm_min=33, tm_sec=47, tm_wday=4, tm_yday=153, tm_isdst=0), 'title': 'Text Style Transfer Back-Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Text Style Transfer Back-Translation'}, 'summary': 'Back Translation (BT) is widely used in the field of machine translation, as\nit has been proved effective for enhancing translation quality. However, BT\nmainly improves the translation of inputs that share a similar style (to be\nmore specific, translation-like inputs), since the source side of BT data is\nmachine-translated. For natural inputs, BT brings only slight improvements and\nsometimes even adverse effects. To address this issue, we propose Text Style\nTransfer Back Translation (TST BT), which uses a style transfer model to modify\nthe source side of BT data. By making the style of source-side text more\nnatural, we aim to improve the translation of natural inputs. Our experiments\non various language pairs, including both high-resource and low-resource ones,\ndemonstrate that TST BT significantly improves translation performance against\npopular BT benchmarks. In addition, TST BT is proved to be effective in domain\nadaptation so this strategy can be regarded as a general data augmentation\nmethod. Our training code and text style transfer model are open-sourced.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Back Translation (BT) is widely used in the field of machine translation, as\nit has been proved effective for enhancing translation quality. However, BT\nmainly improves the translation of inputs that share a similar style (to be\nmore specific, translation-like inputs), since the source side of BT data is\nmachine-translated. For natural inputs, BT brings only slight improvements and\nsometimes even adverse effects. To address this issue, we propose Text Style\nTransfer Back Translation (TST BT), which uses a style transfer model to modify\nthe source side of BT data. By making the style of source-side text more\nnatural, we aim to improve the translation of natural inputs. Our experiments\non various language pairs, including both high-resource and low-resource ones,\ndemonstrate that TST BT significantly improves translation performance against\npopular BT benchmarks. In addition, TST BT is proved to be effective in domain\nadaptation so this strategy can be regarded as a general data augmentation\nmethod. Our training code and text style transfer model are open-sourced.'}, 'authors': [{'name': 'Daimeng Wei'}, {'name': 'Zhanglin Wu'}, {'name': 'Hengchao Shang'}, {'name': 'Zongyao Li'}, {'name': 'Minghan Wang'}, {'name': 'Jiaxin Guo'}, {'name': 'Xiaoyu Chen'}, {'name': 'Zhengzhe Yu'}, {'name': 'Hao Yang'}], 'author_detail': {'name': 'Hao Yang'}, 'author': 'Hao Yang', 'arxiv_comment': 'acl2023, 14 pages, 4 figures, 19 tables', 'links': [{'href': 'http://arxiv.org/abs/2306.01318v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.01318v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.02247v1,2023-06-04 03:26:43+00:00,2023-06-04 03:26:43+00:00,Sen2Pro: A Probabilistic Perspective to Sentence Embedding from Pre-trained Language Model,"[arxiv.Result.Author('Lingfeng Shen'), arxiv.Result.Author('Haiyun Jiang'), arxiv.Result.Author('Lemao Liu'), arxiv.Result.Author('Shuming Shi')]","Sentence embedding is one of the most fundamental tasks in Natural Language
Processing and plays an important role in various tasks. The recent
breakthrough in sentence embedding is achieved by pre-trained language models
(PLMs). Despite its success, an embedded vector (Sen2Vec) representing a point
estimate does not naturally express uncertainty in a taskagnostic way. This
paper thereby proposes an efficient framework on probabilistic sentence
embedding (Sen2Pro) from PLMs, and it represents a sentence as a probability
density distribution in an embedding space to reflect both model uncertainty
and data uncertainty (i.e., many-to-one nature) in the sentence representation.
The proposed framework performs in a plug-and-play way without retraining PLMs
anymore, and it is easy to implement and generally applied on top of any PLM.
The superiority of Sen2Pro over Sen2Vec has been theoretically verified and
practically illustrated on different NLP tasks.",Accepted to ACL2023 workshop Rep4NLP,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2306.02247v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.02247v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.02247v1,"{'id': 'http://arxiv.org/abs/2306.02247v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.02247v1', 'updated': '2023-06-04T03:26:43Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=4, tm_hour=3, tm_min=26, tm_sec=43, tm_wday=6, tm_yday=155, tm_isdst=0), 'published': '2023-06-04T03:26:43Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=4, tm_hour=3, tm_min=26, tm_sec=43, tm_wday=6, tm_yday=155, tm_isdst=0), 'title': 'Sen2Pro: A Probabilistic Perspective to Sentence Embedding from\n  Pre-trained Language Model', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Sen2Pro: A Probabilistic Perspective to Sentence Embedding from\n  Pre-trained Language Model'}, 'summary': 'Sentence embedding is one of the most fundamental tasks in Natural Language\nProcessing and plays an important role in various tasks. The recent\nbreakthrough in sentence embedding is achieved by pre-trained language models\n(PLMs). Despite its success, an embedded vector (Sen2Vec) representing a point\nestimate does not naturally express uncertainty in a taskagnostic way. This\npaper thereby proposes an efficient framework on probabilistic sentence\nembedding (Sen2Pro) from PLMs, and it represents a sentence as a probability\ndensity distribution in an embedding space to reflect both model uncertainty\nand data uncertainty (i.e., many-to-one nature) in the sentence representation.\nThe proposed framework performs in a plug-and-play way without retraining PLMs\nanymore, and it is easy to implement and generally applied on top of any PLM.\nThe superiority of Sen2Pro over Sen2Vec has been theoretically verified and\npractically illustrated on different NLP tasks.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Sentence embedding is one of the most fundamental tasks in Natural Language\nProcessing and plays an important role in various tasks. The recent\nbreakthrough in sentence embedding is achieved by pre-trained language models\n(PLMs). Despite its success, an embedded vector (Sen2Vec) representing a point\nestimate does not naturally express uncertainty in a taskagnostic way. This\npaper thereby proposes an efficient framework on probabilistic sentence\nembedding (Sen2Pro) from PLMs, and it represents a sentence as a probability\ndensity distribution in an embedding space to reflect both model uncertainty\nand data uncertainty (i.e., many-to-one nature) in the sentence representation.\nThe proposed framework performs in a plug-and-play way without retraining PLMs\nanymore, and it is easy to implement and generally applied on top of any PLM.\nThe superiority of Sen2Pro over Sen2Vec has been theoretically verified and\npractically illustrated on different NLP tasks.'}, 'authors': [{'name': 'Lingfeng Shen'}, {'name': 'Haiyun Jiang'}, {'name': 'Lemao Liu'}, {'name': 'Shuming Shi'}], 'author_detail': {'name': 'Shuming Shi'}, 'author': 'Shuming Shi', 'arxiv_comment': 'Accepted to ACL2023 workshop Rep4NLP', 'links': [{'href': 'http://arxiv.org/abs/2306.02247v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.02247v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.02302v1,2023-06-04 08:54:32+00:00,2023-06-04 08:54:32+00:00,Does Character-level Information Always Improve DRS-based Semantic Parsing?,"[arxiv.Result.Author('Tomoya Kurosawa'), arxiv.Result.Author('Hitomi Yanaka')]","Even in the era of massive language models, it has been suggested that
character-level representations improve the performance of neural models. The
state-of-the-art neural semantic parser for Discourse Representation Structures
uses character-level representations, improving performance in the four
languages (i.e., English, German, Dutch, and Italian) in the Parallel Meaning
Bank dataset. However, how and why character-level information improves the
parser's performance remains unclear. This study provides an in-depth analysis
of performance changes by order of character sequences. In the experiments, we
compare F1-scores by shuffling the order and randomizing character sequences
after testing the performance of character-level information. Our results
indicate that incorporating character-level information does not improve the
performance in English and German. In addition, we find that the parser is not
sensitive to correct character order in Dutch. Nevertheless, performance
improvements are observed when using character-level information.","10 pages. To appear in the 12th Joint Conference on Lexical and
  Computational Semantics (*SEM 2023) with ACL2023",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.02302v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.02302v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.02302v1,"{'id': 'http://arxiv.org/abs/2306.02302v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.02302v1', 'updated': '2023-06-04T08:54:32Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=4, tm_hour=8, tm_min=54, tm_sec=32, tm_wday=6, tm_yday=155, tm_isdst=0), 'published': '2023-06-04T08:54:32Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=4, tm_hour=8, tm_min=54, tm_sec=32, tm_wday=6, tm_yday=155, tm_isdst=0), 'title': 'Does Character-level Information Always Improve DRS-based Semantic\n  Parsing?', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Does Character-level Information Always Improve DRS-based Semantic\n  Parsing?'}, 'summary': ""Even in the era of massive language models, it has been suggested that\ncharacter-level representations improve the performance of neural models. The\nstate-of-the-art neural semantic parser for Discourse Representation Structures\nuses character-level representations, improving performance in the four\nlanguages (i.e., English, German, Dutch, and Italian) in the Parallel Meaning\nBank dataset. However, how and why character-level information improves the\nparser's performance remains unclear. This study provides an in-depth analysis\nof performance changes by order of character sequences. In the experiments, we\ncompare F1-scores by shuffling the order and randomizing character sequences\nafter testing the performance of character-level information. Our results\nindicate that incorporating character-level information does not improve the\nperformance in English and German. In addition, we find that the parser is not\nsensitive to correct character order in Dutch. Nevertheless, performance\nimprovements are observed when using character-level information."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Even in the era of massive language models, it has been suggested that\ncharacter-level representations improve the performance of neural models. The\nstate-of-the-art neural semantic parser for Discourse Representation Structures\nuses character-level representations, improving performance in the four\nlanguages (i.e., English, German, Dutch, and Italian) in the Parallel Meaning\nBank dataset. However, how and why character-level information improves the\nparser's performance remains unclear. This study provides an in-depth analysis\nof performance changes by order of character sequences. In the experiments, we\ncompare F1-scores by shuffling the order and randomizing character sequences\nafter testing the performance of character-level information. Our results\nindicate that incorporating character-level information does not improve the\nperformance in English and German. In addition, we find that the parser is not\nsensitive to correct character order in Dutch. Nevertheless, performance\nimprovements are observed when using character-level information.""}, 'authors': [{'name': 'Tomoya Kurosawa'}, {'name': 'Hitomi Yanaka'}], 'author_detail': {'name': 'Hitomi Yanaka'}, 'author': 'Hitomi Yanaka', 'arxiv_comment': '10 pages. To appear in the 12th Joint Conference on Lexical and\n  Computational Semantics (*SEM 2023) with ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.02302v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.02302v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.03055v1,2023-06-05 17:27:48+00:00,2023-06-05 17:27:48+00:00,Analyzing Syntactic Generalization Capacity of Pre-trained Language Models on Japanese Honorific Conversion,"[arxiv.Result.Author('Ryo Sekizawa'), arxiv.Result.Author('Hitomi Yanaka')]","Using Japanese honorifics is challenging because it requires not only
knowledge of the grammatical rules but also contextual information, such as
social relationships. It remains unclear whether pre-trained large language
models (LLMs) can flexibly handle Japanese honorifics like humans. To analyze
this, we introduce an honorific conversion task that considers social
relationships among people mentioned in a conversation. We construct a Japanese
honorifics dataset from problem templates of various sentence structures to
investigate the syntactic generalization capacity of GPT-3, one of the leading
LLMs, on this task under two settings: fine-tuning and prompt learning. Our
results showed that the fine-tuned GPT-3 performed better in a context-aware
honorific conversion task than the prompt-based one. The fine-tuned model
demonstrated overall syntactic generalizability towards compound honorific
sentences, except when tested with the data involving direct speech.","To appear in the Proceedings of the 12th Joint Conference on Lexical
  and Computational Semantics (*SEM2023) with ACL2023",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.03055v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.03055v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.03055v1,"{'id': 'http://arxiv.org/abs/2306.03055v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.03055v1', 'updated': '2023-06-05T17:27:48Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=5, tm_hour=17, tm_min=27, tm_sec=48, tm_wday=0, tm_yday=156, tm_isdst=0), 'published': '2023-06-05T17:27:48Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=5, tm_hour=17, tm_min=27, tm_sec=48, tm_wday=0, tm_yday=156, tm_isdst=0), 'title': 'Analyzing Syntactic Generalization Capacity of Pre-trained Language\n  Models on Japanese Honorific Conversion', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Analyzing Syntactic Generalization Capacity of Pre-trained Language\n  Models on Japanese Honorific Conversion'}, 'summary': 'Using Japanese honorifics is challenging because it requires not only\nknowledge of the grammatical rules but also contextual information, such as\nsocial relationships. It remains unclear whether pre-trained large language\nmodels (LLMs) can flexibly handle Japanese honorifics like humans. To analyze\nthis, we introduce an honorific conversion task that considers social\nrelationships among people mentioned in a conversation. We construct a Japanese\nhonorifics dataset from problem templates of various sentence structures to\ninvestigate the syntactic generalization capacity of GPT-3, one of the leading\nLLMs, on this task under two settings: fine-tuning and prompt learning. Our\nresults showed that the fine-tuned GPT-3 performed better in a context-aware\nhonorific conversion task than the prompt-based one. The fine-tuned model\ndemonstrated overall syntactic generalizability towards compound honorific\nsentences, except when tested with the data involving direct speech.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Using Japanese honorifics is challenging because it requires not only\nknowledge of the grammatical rules but also contextual information, such as\nsocial relationships. It remains unclear whether pre-trained large language\nmodels (LLMs) can flexibly handle Japanese honorifics like humans. To analyze\nthis, we introduce an honorific conversion task that considers social\nrelationships among people mentioned in a conversation. We construct a Japanese\nhonorifics dataset from problem templates of various sentence structures to\ninvestigate the syntactic generalization capacity of GPT-3, one of the leading\nLLMs, on this task under two settings: fine-tuning and prompt learning. Our\nresults showed that the fine-tuned GPT-3 performed better in a context-aware\nhonorific conversion task than the prompt-based one. The fine-tuned model\ndemonstrated overall syntactic generalizability towards compound honorific\nsentences, except when tested with the data involving direct speech.'}, 'authors': [{'name': 'Ryo Sekizawa'}, {'name': 'Hitomi Yanaka'}], 'author_detail': {'name': 'Hitomi Yanaka'}, 'author': 'Hitomi Yanaka', 'arxiv_comment': 'To appear in the Proceedings of the 12th Joint Conference on Lexical\n  and Computational Semantics (*SEM2023) with ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.03055v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.03055v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.03316v1,2023-06-05 23:58:40+00:00,2023-06-05 23:58:40+00:00,CoSiNES: Contrastive Siamese Network for Entity Standardization,"[arxiv.Result.Author('Jiaqing Yuan'), arxiv.Result.Author('Michele Merler'), arxiv.Result.Author('Mihir Choudhury'), arxiv.Result.Author('Raju Pavuluri'), arxiv.Result.Author('Munindar P. Singh'), arxiv.Result.Author('Maja Vukovic')]","Entity standardization maps noisy mentions from free-form text to standard
entities in a knowledge base. The unique challenge of this task relative to
other entity-related tasks is the lack of surrounding context and numerous
variations in the surface form of the mentions, especially when it comes to
generalization across domains where labeled data is scarce. Previous research
mostly focuses on developing models either heavily relying on context, or
dedicated solely to a specific domain. In contrast, we propose CoSiNES, a
generic and adaptable framework with Contrastive Siamese Network for Entity
Standardization that effectively adapts a pretrained language model to capture
the syntax and semantics of the entities in a new domain.
  We construct a new dataset in the technology domain, which contains 640
technical stack entities and 6,412 mentions collected from industrial content
management systems. We demonstrate that CoSiNES yields higher accuracy and
faster runtime than baselines derived from leading methods in this domain.
CoSiNES also achieves competitive performance in four standard datasets from
the chemistry, medicine, and biomedical domains, demonstrating its cross-domain
applicability.",Accepted by Matching Workshop at ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.03316v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.03316v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.03316v1,"{'id': 'http://arxiv.org/abs/2306.03316v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.03316v1', 'updated': '2023-06-05T23:58:40Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=5, tm_hour=23, tm_min=58, tm_sec=40, tm_wday=0, tm_yday=156, tm_isdst=0), 'published': '2023-06-05T23:58:40Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=5, tm_hour=23, tm_min=58, tm_sec=40, tm_wday=0, tm_yday=156, tm_isdst=0), 'title': 'CoSiNES: Contrastive Siamese Network for Entity Standardization', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'CoSiNES: Contrastive Siamese Network for Entity Standardization'}, 'summary': 'Entity standardization maps noisy mentions from free-form text to standard\nentities in a knowledge base. The unique challenge of this task relative to\nother entity-related tasks is the lack of surrounding context and numerous\nvariations in the surface form of the mentions, especially when it comes to\ngeneralization across domains where labeled data is scarce. Previous research\nmostly focuses on developing models either heavily relying on context, or\ndedicated solely to a specific domain. In contrast, we propose CoSiNES, a\ngeneric and adaptable framework with Contrastive Siamese Network for Entity\nStandardization that effectively adapts a pretrained language model to capture\nthe syntax and semantics of the entities in a new domain.\n  We construct a new dataset in the technology domain, which contains 640\ntechnical stack entities and 6,412 mentions collected from industrial content\nmanagement systems. We demonstrate that CoSiNES yields higher accuracy and\nfaster runtime than baselines derived from leading methods in this domain.\nCoSiNES also achieves competitive performance in four standard datasets from\nthe chemistry, medicine, and biomedical domains, demonstrating its cross-domain\napplicability.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Entity standardization maps noisy mentions from free-form text to standard\nentities in a knowledge base. The unique challenge of this task relative to\nother entity-related tasks is the lack of surrounding context and numerous\nvariations in the surface form of the mentions, especially when it comes to\ngeneralization across domains where labeled data is scarce. Previous research\nmostly focuses on developing models either heavily relying on context, or\ndedicated solely to a specific domain. In contrast, we propose CoSiNES, a\ngeneric and adaptable framework with Contrastive Siamese Network for Entity\nStandardization that effectively adapts a pretrained language model to capture\nthe syntax and semantics of the entities in a new domain.\n  We construct a new dataset in the technology domain, which contains 640\ntechnical stack entities and 6,412 mentions collected from industrial content\nmanagement systems. We demonstrate that CoSiNES yields higher accuracy and\nfaster runtime than baselines derived from leading methods in this domain.\nCoSiNES also achieves competitive performance in four standard datasets from\nthe chemistry, medicine, and biomedical domains, demonstrating its cross-domain\napplicability.'}, 'authors': [{'name': 'Jiaqing Yuan'}, {'name': 'Michele Merler'}, {'name': 'Mihir Choudhury'}, {'name': 'Raju Pavuluri'}, {'name': 'Munindar P. Singh'}, {'name': 'Maja Vukovic'}], 'author_detail': {'name': 'Maja Vukovic'}, 'author': 'Maja Vukovic', 'arxiv_comment': 'Accepted by Matching Workshop at ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.03316v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.03316v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.04188v1,2023-06-07 06:47:34+00:00,2023-06-07 06:47:34+00:00,A New Dataset and Empirical Study for Sentence Simplification in Chinese,"[arxiv.Result.Author('Shiping Yang'), arxiv.Result.Author('Renliang Sun'), arxiv.Result.Author('Xiaojun Wan')]","Sentence Simplification is a valuable technique that can benefit language
learners and children a lot. However, current research focuses more on English
sentence simplification. The development of Chinese sentence simplification is
relatively slow due to the lack of data. To alleviate this limitation, this
paper introduces CSS, a new dataset for assessing sentence simplification in
Chinese. We collect manual simplifications from human annotators and perform
data analysis to show the difference between English and Chinese sentence
simplifications. Furthermore, we test several unsupervised and zero/few-shot
learning methods on CSS and analyze the automatic evaluation and human
evaluation results. In the end, we explore whether Large Language Models can
serve as high-quality Chinese sentence simplification systems by evaluating
them on CSS.",Accepted by ACL2023 main conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.04188v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.04188v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.04188v1,"{'id': 'http://arxiv.org/abs/2306.04188v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.04188v1', 'updated': '2023-06-07T06:47:34Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=7, tm_hour=6, tm_min=47, tm_sec=34, tm_wday=2, tm_yday=158, tm_isdst=0), 'published': '2023-06-07T06:47:34Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=7, tm_hour=6, tm_min=47, tm_sec=34, tm_wday=2, tm_yday=158, tm_isdst=0), 'title': 'A New Dataset and Empirical Study for Sentence Simplification in Chinese', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A New Dataset and Empirical Study for Sentence Simplification in Chinese'}, 'summary': 'Sentence Simplification is a valuable technique that can benefit language\nlearners and children a lot. However, current research focuses more on English\nsentence simplification. The development of Chinese sentence simplification is\nrelatively slow due to the lack of data. To alleviate this limitation, this\npaper introduces CSS, a new dataset for assessing sentence simplification in\nChinese. We collect manual simplifications from human annotators and perform\ndata analysis to show the difference between English and Chinese sentence\nsimplifications. Furthermore, we test several unsupervised and zero/few-shot\nlearning methods on CSS and analyze the automatic evaluation and human\nevaluation results. In the end, we explore whether Large Language Models can\nserve as high-quality Chinese sentence simplification systems by evaluating\nthem on CSS.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Sentence Simplification is a valuable technique that can benefit language\nlearners and children a lot. However, current research focuses more on English\nsentence simplification. The development of Chinese sentence simplification is\nrelatively slow due to the lack of data. To alleviate this limitation, this\npaper introduces CSS, a new dataset for assessing sentence simplification in\nChinese. We collect manual simplifications from human annotators and perform\ndata analysis to show the difference between English and Chinese sentence\nsimplifications. Furthermore, we test several unsupervised and zero/few-shot\nlearning methods on CSS and analyze the automatic evaluation and human\nevaluation results. In the end, we explore whether Large Language Models can\nserve as high-quality Chinese sentence simplification systems by evaluating\nthem on CSS.'}, 'authors': [{'name': 'Shiping Yang'}, {'name': 'Renliang Sun'}, {'name': 'Xiaojun Wan'}], 'author_detail': {'name': 'Xiaojun Wan'}, 'author': 'Xiaojun Wan', 'arxiv_comment': 'Accepted by ACL2023 main conference', 'links': [{'href': 'http://arxiv.org/abs/2306.04188v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.04188v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.04235v1,2023-06-07 08:25:51+00:00,2023-06-07 08:25:51+00:00,MobileNMT: Enabling Translation in 15MB and 30ms,"[arxiv.Result.Author('Ye Lin'), arxiv.Result.Author('Xiaohui Wang'), arxiv.Result.Author('Zhexi Zhang'), arxiv.Result.Author('Mingxuan Wang'), arxiv.Result.Author('Tong Xiao'), arxiv.Result.Author('Jingbo Zhu')]","Deploying NMT models on mobile devices is essential for privacy, low latency,
and offline scenarios. For high model capacity, NMT models are rather large.
Running these models on devices is challenging with limited storage, memory,
computation, and power consumption. Existing work either only focuses on a
single metric such as FLOPs or general engine which is not good at
auto-regressive decoding. In this paper, we present MobileNMT, a system that
can translate in 15MB and 30ms on devices. We propose a series of principles
for model compression when combined with quantization. Further, we implement an
engine that is friendly to INT8 and decoding. With the co-design of model and
engine, compared with the existing system, we speed up 47.0x and save 99.5% of
memory with only 11.6% loss of BLEU. The code is publicly available at
https://github.com/zjersey/Lightseq-ARM.",accepted by ACL2023 Industry Track,,,cs.AI,"['cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2306.04235v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.04235v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.04235v1,"{'id': 'http://arxiv.org/abs/2306.04235v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.04235v1', 'updated': '2023-06-07T08:25:51Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=7, tm_hour=8, tm_min=25, tm_sec=51, tm_wday=2, tm_yday=158, tm_isdst=0), 'published': '2023-06-07T08:25:51Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=7, tm_hour=8, tm_min=25, tm_sec=51, tm_wday=2, tm_yday=158, tm_isdst=0), 'title': 'MobileNMT: Enabling Translation in 15MB and 30ms', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'MobileNMT: Enabling Translation in 15MB and 30ms'}, 'summary': 'Deploying NMT models on mobile devices is essential for privacy, low latency,\nand offline scenarios. For high model capacity, NMT models are rather large.\nRunning these models on devices is challenging with limited storage, memory,\ncomputation, and power consumption. Existing work either only focuses on a\nsingle metric such as FLOPs or general engine which is not good at\nauto-regressive decoding. In this paper, we present MobileNMT, a system that\ncan translate in 15MB and 30ms on devices. We propose a series of principles\nfor model compression when combined with quantization. Further, we implement an\nengine that is friendly to INT8 and decoding. With the co-design of model and\nengine, compared with the existing system, we speed up 47.0x and save 99.5% of\nmemory with only 11.6% loss of BLEU. The code is publicly available at\nhttps://github.com/zjersey/Lightseq-ARM.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Deploying NMT models on mobile devices is essential for privacy, low latency,\nand offline scenarios. For high model capacity, NMT models are rather large.\nRunning these models on devices is challenging with limited storage, memory,\ncomputation, and power consumption. Existing work either only focuses on a\nsingle metric such as FLOPs or general engine which is not good at\nauto-regressive decoding. In this paper, we present MobileNMT, a system that\ncan translate in 15MB and 30ms on devices. We propose a series of principles\nfor model compression when combined with quantization. Further, we implement an\nengine that is friendly to INT8 and decoding. With the co-design of model and\nengine, compared with the existing system, we speed up 47.0x and save 99.5% of\nmemory with only 11.6% loss of BLEU. The code is publicly available at\nhttps://github.com/zjersey/Lightseq-ARM.'}, 'authors': [{'name': 'Ye Lin'}, {'name': 'Xiaohui Wang'}, {'name': 'Zhexi Zhang'}, {'name': 'Mingxuan Wang'}, {'name': 'Tong Xiao'}, {'name': 'Jingbo Zhu'}], 'author_detail': {'name': 'Jingbo Zhu'}, 'author': 'Jingbo Zhu', 'arxiv_comment': 'accepted by ACL2023 Industry Track', 'links': [{'href': 'http://arxiv.org/abs/2306.04235v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.04235v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.04950v1,2023-06-08 05:45:25+00:00,2023-06-08 05:45:25+00:00,Open Set Relation Extraction via Unknown-Aware Training,"[arxiv.Result.Author('Jun Zhao'), arxiv.Result.Author('Xin Zhao'), arxiv.Result.Author('Wenyu Zhan'), arxiv.Result.Author('Qi Zhang'), arxiv.Result.Author('Tao Gui'), arxiv.Result.Author('Zhongyu Wei'), arxiv.Result.Author('Yunwen Chen'), arxiv.Result.Author('Xiang Gao'), arxiv.Result.Author('Xuanjing Huang')]","The existing supervised relation extraction methods have achieved impressive
performance in a closed-set setting, where the relations during both training
and testing remain the same. In a more realistic open-set setting, unknown
relations may appear in the test set. Due to the lack of supervision signals
from unknown relations, a well-performing closed-set relation extractor can
still confidently misclassify them into known relations. In this paper, we
propose an unknown-aware training method, regularizing the model by dynamically
synthesizing negative instances. To facilitate a compact decision boundary,
``difficult'' negative instances are necessary. Inspired by text adversarial
attacks, we adaptively apply small but critical perturbations to original
training instances and thus synthesizing negative instances that are more
likely to be mistaken by the model as known relations. Experimental results
show that this method achieves SOTA unknown relation detection without
compromising the classification of known relations.",Accepted by ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.04950v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.04950v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.04950v1,"{'id': 'http://arxiv.org/abs/2306.04950v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.04950v1', 'updated': '2023-06-08T05:45:25Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=5, tm_min=45, tm_sec=25, tm_wday=3, tm_yday=159, tm_isdst=0), 'published': '2023-06-08T05:45:25Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=5, tm_min=45, tm_sec=25, tm_wday=3, tm_yday=159, tm_isdst=0), 'title': 'Open Set Relation Extraction via Unknown-Aware Training', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Open Set Relation Extraction via Unknown-Aware Training'}, 'summary': ""The existing supervised relation extraction methods have achieved impressive\nperformance in a closed-set setting, where the relations during both training\nand testing remain the same. In a more realistic open-set setting, unknown\nrelations may appear in the test set. Due to the lack of supervision signals\nfrom unknown relations, a well-performing closed-set relation extractor can\nstill confidently misclassify them into known relations. In this paper, we\npropose an unknown-aware training method, regularizing the model by dynamically\nsynthesizing negative instances. To facilitate a compact decision boundary,\n``difficult'' negative instances are necessary. Inspired by text adversarial\nattacks, we adaptively apply small but critical perturbations to original\ntraining instances and thus synthesizing negative instances that are more\nlikely to be mistaken by the model as known relations. Experimental results\nshow that this method achieves SOTA unknown relation detection without\ncompromising the classification of known relations."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""The existing supervised relation extraction methods have achieved impressive\nperformance in a closed-set setting, where the relations during both training\nand testing remain the same. In a more realistic open-set setting, unknown\nrelations may appear in the test set. Due to the lack of supervision signals\nfrom unknown relations, a well-performing closed-set relation extractor can\nstill confidently misclassify them into known relations. In this paper, we\npropose an unknown-aware training method, regularizing the model by dynamically\nsynthesizing negative instances. To facilitate a compact decision boundary,\n``difficult'' negative instances are necessary. Inspired by text adversarial\nattacks, we adaptively apply small but critical perturbations to original\ntraining instances and thus synthesizing negative instances that are more\nlikely to be mistaken by the model as known relations. Experimental results\nshow that this method achieves SOTA unknown relation detection without\ncompromising the classification of known relations.""}, 'authors': [{'name': 'Jun Zhao'}, {'name': 'Xin Zhao'}, {'name': 'Wenyu Zhan'}, {'name': 'Qi Zhang'}, {'name': 'Tao Gui'}, {'name': 'Zhongyu Wei'}, {'name': 'Yunwen Chen'}, {'name': 'Xiang Gao'}, {'name': 'Xuanjing Huang'}], 'author_detail': {'name': 'Xuanjing Huang'}, 'author': 'Xuanjing Huang', 'arxiv_comment': 'Accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.04950v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.04950v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.04954v1,2023-06-08 06:02:34+00:00,2023-06-08 06:02:34+00:00,RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot Relation Extraction,"[arxiv.Result.Author('Jun Zhao'), arxiv.Result.Author('Wenyu Zhan'), arxiv.Result.Author('Xin Zhao'), arxiv.Result.Author('Qi Zhang'), arxiv.Result.Author('Tao Gui'), arxiv.Result.Author('Zhongyu Wei'), arxiv.Result.Author('Junzhe Wang'), arxiv.Result.Author('Minlong Peng'), arxiv.Result.Author('Mingming Sun')]","Semantic matching is a mainstream paradigm of zero-shot relation extraction,
which matches a given input with a corresponding label description. The
entities in the input should exactly match their hypernyms in the description,
while the irrelevant contexts should be ignored when matching. However, general
matching methods lack explicit modeling of the above matching pattern. In this
work, we propose a fine-grained semantic matching method tailored for zero-shot
relation extraction. Following the above matching pattern, we decompose the
sentence-level similarity score into entity and context matching scores. Due to
the lack of explicit annotations of the redundant components, we design a
feature distillation module to adaptively identify the relation-irrelevant
features and reduce their negative impact on context matching. Experimental
results show that our method achieves higher matching $F_1$ score and has an
inference speed 10 times faster, when compared with the state-of-the-art
methods.",Accepted by ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.04954v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.04954v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.04954v1,"{'id': 'http://arxiv.org/abs/2306.04954v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.04954v1', 'updated': '2023-06-08T06:02:34Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=6, tm_min=2, tm_sec=34, tm_wday=3, tm_yday=159, tm_isdst=0), 'published': '2023-06-08T06:02:34Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=6, tm_min=2, tm_sec=34, tm_wday=3, tm_yday=159, tm_isdst=0), 'title': 'RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot\n  Relation Extraction', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot\n  Relation Extraction'}, 'summary': 'Semantic matching is a mainstream paradigm of zero-shot relation extraction,\nwhich matches a given input with a corresponding label description. The\nentities in the input should exactly match their hypernyms in the description,\nwhile the irrelevant contexts should be ignored when matching. However, general\nmatching methods lack explicit modeling of the above matching pattern. In this\nwork, we propose a fine-grained semantic matching method tailored for zero-shot\nrelation extraction. Following the above matching pattern, we decompose the\nsentence-level similarity score into entity and context matching scores. Due to\nthe lack of explicit annotations of the redundant components, we design a\nfeature distillation module to adaptively identify the relation-irrelevant\nfeatures and reduce their negative impact on context matching. Experimental\nresults show that our method achieves higher matching $F_1$ score and has an\ninference speed 10 times faster, when compared with the state-of-the-art\nmethods.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Semantic matching is a mainstream paradigm of zero-shot relation extraction,\nwhich matches a given input with a corresponding label description. The\nentities in the input should exactly match their hypernyms in the description,\nwhile the irrelevant contexts should be ignored when matching. However, general\nmatching methods lack explicit modeling of the above matching pattern. In this\nwork, we propose a fine-grained semantic matching method tailored for zero-shot\nrelation extraction. Following the above matching pattern, we decompose the\nsentence-level similarity score into entity and context matching scores. Due to\nthe lack of explicit annotations of the redundant components, we design a\nfeature distillation module to adaptively identify the relation-irrelevant\nfeatures and reduce their negative impact on context matching. Experimental\nresults show that our method achieves higher matching $F_1$ score and has an\ninference speed 10 times faster, when compared with the state-of-the-art\nmethods.'}, 'authors': [{'name': 'Jun Zhao'}, {'name': 'Wenyu Zhan'}, {'name': 'Xin Zhao'}, {'name': 'Qi Zhang'}, {'name': 'Tao Gui'}, {'name': 'Zhongyu Wei'}, {'name': 'Junzhe Wang'}, {'name': 'Minlong Peng'}, {'name': 'Mingming Sun'}], 'author_detail': {'name': 'Mingming Sun'}, 'author': 'Mingming Sun', 'arxiv_comment': 'Accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.04954v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.04954v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.04968v1,2023-06-08 06:55:02+00:00,2023-06-08 06:55:02+00:00,Actively Supervised Clustering for Open Relation Extraction,"[arxiv.Result.Author('Jun Zhao'), arxiv.Result.Author('Yongxin Zhang'), arxiv.Result.Author('Qi Zhang'), arxiv.Result.Author('Tao Gui'), arxiv.Result.Author('Zhongyu Wei'), arxiv.Result.Author('Minlong Peng'), arxiv.Result.Author('Mingming Sun')]","Current clustering-based Open Relation Extraction (OpenRE) methods usually
adopt a two-stage pipeline. The first stage simultaneously learns relation
representations and assignments. The second stage manually labels several
instances and thus names the relation for each cluster. However, unsupervised
objectives struggle to optimize the model to derive accurate clustering
assignments, and the number of clusters has to be supplied in advance. In this
paper, we present a novel setting, named actively supervised clustering for
OpenRE. Our insight lies in that clustering learning and relation labeling can
be alternately performed, providing the necessary guidance for clustering
without a significant increase in human effort. The key to the setting is
selecting which instances to label. Instead of using classical active labeling
strategies designed for fixed known classes, we propose a new strategy, which
is applicable to dynamically discover clusters of unknown relations.
Experimental results show that our method is able to discover almost all
relational clusters in the data and improve the SOTA methods by 10.3\% and
5.2\%, on two datasets respectively.",Accepted by ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.04968v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.04968v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.04968v1,"{'id': 'http://arxiv.org/abs/2306.04968v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.04968v1', 'updated': '2023-06-08T06:55:02Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=6, tm_min=55, tm_sec=2, tm_wday=3, tm_yday=159, tm_isdst=0), 'published': '2023-06-08T06:55:02Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=6, tm_min=55, tm_sec=2, tm_wday=3, tm_yday=159, tm_isdst=0), 'title': 'Actively Supervised Clustering for Open Relation Extraction', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Actively Supervised Clustering for Open Relation Extraction'}, 'summary': 'Current clustering-based Open Relation Extraction (OpenRE) methods usually\nadopt a two-stage pipeline. The first stage simultaneously learns relation\nrepresentations and assignments. The second stage manually labels several\ninstances and thus names the relation for each cluster. However, unsupervised\nobjectives struggle to optimize the model to derive accurate clustering\nassignments, and the number of clusters has to be supplied in advance. In this\npaper, we present a novel setting, named actively supervised clustering for\nOpenRE. Our insight lies in that clustering learning and relation labeling can\nbe alternately performed, providing the necessary guidance for clustering\nwithout a significant increase in human effort. The key to the setting is\nselecting which instances to label. Instead of using classical active labeling\nstrategies designed for fixed known classes, we propose a new strategy, which\nis applicable to dynamically discover clusters of unknown relations.\nExperimental results show that our method is able to discover almost all\nrelational clusters in the data and improve the SOTA methods by 10.3\\% and\n5.2\\%, on two datasets respectively.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Current clustering-based Open Relation Extraction (OpenRE) methods usually\nadopt a two-stage pipeline. The first stage simultaneously learns relation\nrepresentations and assignments. The second stage manually labels several\ninstances and thus names the relation for each cluster. However, unsupervised\nobjectives struggle to optimize the model to derive accurate clustering\nassignments, and the number of clusters has to be supplied in advance. In this\npaper, we present a novel setting, named actively supervised clustering for\nOpenRE. Our insight lies in that clustering learning and relation labeling can\nbe alternately performed, providing the necessary guidance for clustering\nwithout a significant increase in human effort. The key to the setting is\nselecting which instances to label. Instead of using classical active labeling\nstrategies designed for fixed known classes, we propose a new strategy, which\nis applicable to dynamically discover clusters of unknown relations.\nExperimental results show that our method is able to discover almost all\nrelational clusters in the data and improve the SOTA methods by 10.3\\% and\n5.2\\%, on two datasets respectively.'}, 'authors': [{'name': 'Jun Zhao'}, {'name': 'Yongxin Zhang'}, {'name': 'Qi Zhang'}, {'name': 'Tao Gui'}, {'name': 'Zhongyu Wei'}, {'name': 'Minlong Peng'}, {'name': 'Mingming Sun'}], 'author_detail': {'name': 'Mingming Sun'}, 'author': 'Mingming Sun', 'arxiv_comment': 'Accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.04968v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.04968v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.05077v1,2023-06-08 10:00:19+00:00,2023-06-08 10:00:19+00:00,Improving Language Model Integration for Neural Machine Translation,"[arxiv.Result.Author('Christian Herold'), arxiv.Result.Author('Yingbo Gao'), arxiv.Result.Author('Mohammad Zeineldeen'), arxiv.Result.Author('Hermann Ney')]","The integration of language models for neural machine translation has been
extensively studied in the past. It has been shown that an external language
model, trained on additional target-side monolingual data, can help improve
translation quality. However, there has always been the assumption that the
translation model also learns an implicit target-side language model during
training, which interferes with the external language model at decoding time.
Recently, some works on automatic speech recognition have demonstrated that, if
the implicit language model is neutralized in decoding, further improvements
can be gained when integrating an external language model. In this work, we
transfer this concept to the task of machine translation and compare with the
most prominent way of including additional monolingual data - namely
back-translation. We find that accounting for the implicit language model
significantly boosts the performance of language model fusion, although this
approach is still outperformed by back-translation.",accepted at ACL2023 (Findings),,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2306.05077v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.05077v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.05077v1,"{'id': 'http://arxiv.org/abs/2306.05077v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.05077v1', 'updated': '2023-06-08T10:00:19Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=10, tm_min=0, tm_sec=19, tm_wday=3, tm_yday=159, tm_isdst=0), 'published': '2023-06-08T10:00:19Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=8, tm_hour=10, tm_min=0, tm_sec=19, tm_wday=3, tm_yday=159, tm_isdst=0), 'title': 'Improving Language Model Integration for Neural Machine Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Improving Language Model Integration for Neural Machine Translation'}, 'summary': 'The integration of language models for neural machine translation has been\nextensively studied in the past. It has been shown that an external language\nmodel, trained on additional target-side monolingual data, can help improve\ntranslation quality. However, there has always been the assumption that the\ntranslation model also learns an implicit target-side language model during\ntraining, which interferes with the external language model at decoding time.\nRecently, some works on automatic speech recognition have demonstrated that, if\nthe implicit language model is neutralized in decoding, further improvements\ncan be gained when integrating an external language model. In this work, we\ntransfer this concept to the task of machine translation and compare with the\nmost prominent way of including additional monolingual data - namely\nback-translation. We find that accounting for the implicit language model\nsignificantly boosts the performance of language model fusion, although this\napproach is still outperformed by back-translation.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The integration of language models for neural machine translation has been\nextensively studied in the past. It has been shown that an external language\nmodel, trained on additional target-side monolingual data, can help improve\ntranslation quality. However, there has always been the assumption that the\ntranslation model also learns an implicit target-side language model during\ntraining, which interferes with the external language model at decoding time.\nRecently, some works on automatic speech recognition have demonstrated that, if\nthe implicit language model is neutralized in decoding, further improvements\ncan be gained when integrating an external language model. In this work, we\ntransfer this concept to the task of machine translation and compare with the\nmost prominent way of including additional monolingual data - namely\nback-translation. We find that accounting for the implicit language model\nsignificantly boosts the performance of language model fusion, although this\napproach is still outperformed by back-translation.'}, 'authors': [{'name': 'Christian Herold'}, {'name': 'Yingbo Gao'}, {'name': 'Mohammad Zeineldeen'}, {'name': 'Hermann Ney'}], 'author_detail': {'name': 'Hermann Ney'}, 'author': 'Hermann Ney', 'arxiv_comment': 'accepted at ACL2023 (Findings)', 'links': [{'href': 'http://arxiv.org/abs/2306.05077v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.05077v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.06948v1,2023-06-12 08:32:04+00:00,2023-06-12 08:32:04+00:00,Rethinking Translation Memory Augmented Neural Machine Translation,"[arxiv.Result.Author('Hongkun Hao'), arxiv.Result.Author('Guoping Huang'), arxiv.Result.Author('Lemao Liu'), arxiv.Result.Author('Zhirui Zhang'), arxiv.Result.Author('Shuming Shi'), arxiv.Result.Author('Rui Wang')]","This paper rethinks translation memory augmented neural machine translation
(TM-augmented NMT) from two perspectives, i.e., a probabilistic view of
retrieval and the variance-bias decomposition principle. The finding
demonstrates that TM-augmented NMT is good at the ability of fitting data
(i.e., lower bias) but is more sensitive to the fluctuations in the training
data (i.e., higher variance), which provides an explanation to a recently
reported contradictory phenomenon on the same translation task: TM-augmented
NMT substantially advances vanilla NMT under the high-resource scenario whereas
it fails under the low-resource scenario. Then we propose a simple yet
effective TM-augmented NMT model to promote the variance and address the
contradictory phenomenon. Extensive experiments show that the proposed
TM-augmented NMT achieves consistent gains over both conventional NMT and
existing TM-augmented NMT under two variance-preferable (low-resource and
plug-and-play) scenarios as well as the high-resource scenario.","15 pages, 2 figures, accepted by ACL2023 findings",,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2306.06948v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.06948v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.06948v1,"{'id': 'http://arxiv.org/abs/2306.06948v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.06948v1', 'updated': '2023-06-12T08:32:04Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=12, tm_hour=8, tm_min=32, tm_sec=4, tm_wday=0, tm_yday=163, tm_isdst=0), 'published': '2023-06-12T08:32:04Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=12, tm_hour=8, tm_min=32, tm_sec=4, tm_wday=0, tm_yday=163, tm_isdst=0), 'title': 'Rethinking Translation Memory Augmented Neural Machine Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Rethinking Translation Memory Augmented Neural Machine Translation'}, 'summary': 'This paper rethinks translation memory augmented neural machine translation\n(TM-augmented NMT) from two perspectives, i.e., a probabilistic view of\nretrieval and the variance-bias decomposition principle. The finding\ndemonstrates that TM-augmented NMT is good at the ability of fitting data\n(i.e., lower bias) but is more sensitive to the fluctuations in the training\ndata (i.e., higher variance), which provides an explanation to a recently\nreported contradictory phenomenon on the same translation task: TM-augmented\nNMT substantially advances vanilla NMT under the high-resource scenario whereas\nit fails under the low-resource scenario. Then we propose a simple yet\neffective TM-augmented NMT model to promote the variance and address the\ncontradictory phenomenon. Extensive experiments show that the proposed\nTM-augmented NMT achieves consistent gains over both conventional NMT and\nexisting TM-augmented NMT under two variance-preferable (low-resource and\nplug-and-play) scenarios as well as the high-resource scenario.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'This paper rethinks translation memory augmented neural machine translation\n(TM-augmented NMT) from two perspectives, i.e., a probabilistic view of\nretrieval and the variance-bias decomposition principle. The finding\ndemonstrates that TM-augmented NMT is good at the ability of fitting data\n(i.e., lower bias) but is more sensitive to the fluctuations in the training\ndata (i.e., higher variance), which provides an explanation to a recently\nreported contradictory phenomenon on the same translation task: TM-augmented\nNMT substantially advances vanilla NMT under the high-resource scenario whereas\nit fails under the low-resource scenario. Then we propose a simple yet\neffective TM-augmented NMT model to promote the variance and address the\ncontradictory phenomenon. Extensive experiments show that the proposed\nTM-augmented NMT achieves consistent gains over both conventional NMT and\nexisting TM-augmented NMT under two variance-preferable (low-resource and\nplug-and-play) scenarios as well as the high-resource scenario.'}, 'authors': [{'name': 'Hongkun Hao'}, {'name': 'Guoping Huang'}, {'name': 'Lemao Liu'}, {'name': 'Zhirui Zhang'}, {'name': 'Shuming Shi'}, {'name': 'Rui Wang'}], 'author_detail': {'name': 'Rui Wang'}, 'author': 'Rui Wang', 'arxiv_comment': '15 pages, 2 figures, accepted by ACL2023 findings', 'links': [{'href': 'http://arxiv.org/abs/2306.06948v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.06948v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.08909v1,2023-06-15 07:23:44+00:00,2023-06-15 07:23:44+00:00,Bridging the Gap between Decision and Logits in Decision-based Knowledge Distillation for Pre-trained Language Models,"[arxiv.Result.Author('Qinhong Zhou'), arxiv.Result.Author('Zonghan Yang'), arxiv.Result.Author('Peng Li'), arxiv.Result.Author('Yang Liu')]","Conventional knowledge distillation (KD) methods require access to the
internal information of teachers, e.g., logits. However, such information may
not always be accessible for large pre-trained language models (PLMs). In this
work, we focus on decision-based KD for PLMs, where only teacher decisions
(i.e., top-1 labels) are accessible. Considering the information gap between
logits and decisions, we propose a novel method to estimate logits from the
decision distributions. Specifically, decision distributions can be both
derived as a function of logits theoretically and estimated with test-time data
augmentation empirically. By combining the theoretical and empirical
estimations of the decision distributions together, the estimation of logits
can be successfully reduced to a simple root-finding problem. Extensive
experiments show that our method significantly outperforms strong baselines on
both natural language understanding and machine reading comprehension datasets.",Accepted by ACL2023 main conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.08909v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.08909v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.08909v1,"{'id': 'http://arxiv.org/abs/2306.08909v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.08909v1', 'updated': '2023-06-15T07:23:44Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=15, tm_hour=7, tm_min=23, tm_sec=44, tm_wday=3, tm_yday=166, tm_isdst=0), 'published': '2023-06-15T07:23:44Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=15, tm_hour=7, tm_min=23, tm_sec=44, tm_wday=3, tm_yday=166, tm_isdst=0), 'title': 'Bridging the Gap between Decision and Logits in Decision-based Knowledge\n  Distillation for Pre-trained Language Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Bridging the Gap between Decision and Logits in Decision-based Knowledge\n  Distillation for Pre-trained Language Models'}, 'summary': 'Conventional knowledge distillation (KD) methods require access to the\ninternal information of teachers, e.g., logits. However, such information may\nnot always be accessible for large pre-trained language models (PLMs). In this\nwork, we focus on decision-based KD for PLMs, where only teacher decisions\n(i.e., top-1 labels) are accessible. Considering the information gap between\nlogits and decisions, we propose a novel method to estimate logits from the\ndecision distributions. Specifically, decision distributions can be both\nderived as a function of logits theoretically and estimated with test-time data\naugmentation empirically. By combining the theoretical and empirical\nestimations of the decision distributions together, the estimation of logits\ncan be successfully reduced to a simple root-finding problem. Extensive\nexperiments show that our method significantly outperforms strong baselines on\nboth natural language understanding and machine reading comprehension datasets.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Conventional knowledge distillation (KD) methods require access to the\ninternal information of teachers, e.g., logits. However, such information may\nnot always be accessible for large pre-trained language models (PLMs). In this\nwork, we focus on decision-based KD for PLMs, where only teacher decisions\n(i.e., top-1 labels) are accessible. Considering the information gap between\nlogits and decisions, we propose a novel method to estimate logits from the\ndecision distributions. Specifically, decision distributions can be both\nderived as a function of logits theoretically and estimated with test-time data\naugmentation empirically. By combining the theoretical and empirical\nestimations of the decision distributions together, the estimation of logits\ncan be successfully reduced to a simple root-finding problem. Extensive\nexperiments show that our method significantly outperforms strong baselines on\nboth natural language understanding and machine reading comprehension datasets.'}, 'authors': [{'name': 'Qinhong Zhou'}, {'name': 'Zonghan Yang'}, {'name': 'Peng Li'}, {'name': 'Yang Liu'}], 'author_detail': {'name': 'Yang Liu'}, 'author': 'Yang Liu', 'arxiv_comment': 'Accepted by ACL2023 main conference', 'links': [{'href': 'http://arxiv.org/abs/2306.08909v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.08909v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.11559v1,2023-06-20 14:23:32+00:00,2023-06-20 14:23:32+00:00,The Ecological Fallacy in Annotation: Modelling Human Label Variation goes beyond Sociodemographics,"[arxiv.Result.Author('Matthias Orlikowski'), arxiv.Result.Author('Paul Röttger'), arxiv.Result.Author('Philipp Cimiano'), arxiv.Result.Author('Dirk Hovy')]","Many NLP tasks exhibit human label variation, where different annotators give
different labels to the same texts. This variation is known to depend, at least
in part, on the sociodemographics of annotators. Recent research aims to model
individual annotator behaviour rather than predicting aggregated labels, and we
would expect that sociodemographic information is useful for these models. On
the other hand, the ecological fallacy states that aggregate group behaviour,
such as the behaviour of the average female annotator, does not necessarily
explain individual behaviour. To account for sociodemographics in models of
individual annotator behaviour, we introduce group-specific layers to
multi-annotator models. In a series of experiments for toxic content detection,
we find that explicitly accounting for sociodemographic attributes in this way
does not significantly improve model performance. This result shows that
individual annotation behaviour depends on much more than just
sociodemographics.",ACL2023 Camera-Ready,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.11559v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.11559v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.11559v1,"{'id': 'http://arxiv.org/abs/2306.11559v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.11559v1', 'updated': '2023-06-20T14:23:32Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=20, tm_hour=14, tm_min=23, tm_sec=32, tm_wday=1, tm_yday=171, tm_isdst=0), 'published': '2023-06-20T14:23:32Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=20, tm_hour=14, tm_min=23, tm_sec=32, tm_wday=1, tm_yday=171, tm_isdst=0), 'title': 'The Ecological Fallacy in Annotation: Modelling Human Label Variation\n  goes beyond Sociodemographics', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The Ecological Fallacy in Annotation: Modelling Human Label Variation\n  goes beyond Sociodemographics'}, 'summary': 'Many NLP tasks exhibit human label variation, where different annotators give\ndifferent labels to the same texts. This variation is known to depend, at least\nin part, on the sociodemographics of annotators. Recent research aims to model\nindividual annotator behaviour rather than predicting aggregated labels, and we\nwould expect that sociodemographic information is useful for these models. On\nthe other hand, the ecological fallacy states that aggregate group behaviour,\nsuch as the behaviour of the average female annotator, does not necessarily\nexplain individual behaviour. To account for sociodemographics in models of\nindividual annotator behaviour, we introduce group-specific layers to\nmulti-annotator models. In a series of experiments for toxic content detection,\nwe find that explicitly accounting for sociodemographic attributes in this way\ndoes not significantly improve model performance. This result shows that\nindividual annotation behaviour depends on much more than just\nsociodemographics.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Many NLP tasks exhibit human label variation, where different annotators give\ndifferent labels to the same texts. This variation is known to depend, at least\nin part, on the sociodemographics of annotators. Recent research aims to model\nindividual annotator behaviour rather than predicting aggregated labels, and we\nwould expect that sociodemographic information is useful for these models. On\nthe other hand, the ecological fallacy states that aggregate group behaviour,\nsuch as the behaviour of the average female annotator, does not necessarily\nexplain individual behaviour. To account for sociodemographics in models of\nindividual annotator behaviour, we introduce group-specific layers to\nmulti-annotator models. In a series of experiments for toxic content detection,\nwe find that explicitly accounting for sociodemographic attributes in this way\ndoes not significantly improve model performance. This result shows that\nindividual annotation behaviour depends on much more than just\nsociodemographics.'}, 'authors': [{'name': 'Matthias Orlikowski'}, {'name': 'Paul Röttger'}, {'name': 'Philipp Cimiano'}, {'name': 'Dirk Hovy'}], 'author_detail': {'name': 'Dirk Hovy'}, 'arxiv_affiliation': 'Computing Sciences Department, Bocconi University, Milan, Italy', 'author': 'Dirk Hovy', 'arxiv_comment': 'ACL2023 Camera-Ready', 'links': [{'href': 'http://arxiv.org/abs/2306.11559v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.11559v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.12089v1,2023-06-21 08:08:15+00:00,2023-06-21 08:08:15+00:00,Towards Accurate Translation via Semantically Appropriate Application of Lexical Constraints,"[arxiv.Result.Author('Yujin Baek'), arxiv.Result.Author('Koanho Lee'), arxiv.Result.Author('Dayeon Ki'), arxiv.Result.Author('Hyoung-Gyu Lee'), arxiv.Result.Author('Cheonbok Park'), arxiv.Result.Author('Jaegul Choo')]","Lexically-constrained NMT (LNMT) aims to incorporate user-provided
terminology into translations. Despite its practical advantages, existing work
has not evaluated LNMT models under challenging real-world conditions. In this
paper, we focus on two important but under-studied issues that lie in the
current evaluation process of LNMT studies. The model needs to cope with
challenging lexical constraints that are ""homographs"" or ""unseen"" during
training. To this end, we first design a homograph disambiguation module to
differentiate the meanings of homographs. Moreover, we propose PLUMCOT, which
integrates contextually rich information about unseen lexical constraints from
pre-trained language models and strengthens a copy mechanism of the pointer
network via direct supervision of a copying score. We also release HOLLY, an
evaluation benchmark for assessing the ability of a model to cope with
""homographic"" and ""unseen"" lexical constraints. Experiments on HOLLY and the
previous test setup show the effectiveness of our method. The effects of
PLUMCOT are shown to be remarkable in ""unseen"" constraints. Our dataset is
available at https://github.com/papago-lab/HOLLY-benchmark",Findings of ACL2023. 15 pages,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.12089v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.12089v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.12089v1,"{'id': 'http://arxiv.org/abs/2306.12089v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.12089v1', 'updated': '2023-06-21T08:08:15Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=21, tm_hour=8, tm_min=8, tm_sec=15, tm_wday=2, tm_yday=172, tm_isdst=0), 'published': '2023-06-21T08:08:15Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=21, tm_hour=8, tm_min=8, tm_sec=15, tm_wday=2, tm_yday=172, tm_isdst=0), 'title': 'Towards Accurate Translation via Semantically Appropriate Application of\n  Lexical Constraints', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Towards Accurate Translation via Semantically Appropriate Application of\n  Lexical Constraints'}, 'summary': 'Lexically-constrained NMT (LNMT) aims to incorporate user-provided\nterminology into translations. Despite its practical advantages, existing work\nhas not evaluated LNMT models under challenging real-world conditions. In this\npaper, we focus on two important but under-studied issues that lie in the\ncurrent evaluation process of LNMT studies. The model needs to cope with\nchallenging lexical constraints that are ""homographs"" or ""unseen"" during\ntraining. To this end, we first design a homograph disambiguation module to\ndifferentiate the meanings of homographs. Moreover, we propose PLUMCOT, which\nintegrates contextually rich information about unseen lexical constraints from\npre-trained language models and strengthens a copy mechanism of the pointer\nnetwork via direct supervision of a copying score. We also release HOLLY, an\nevaluation benchmark for assessing the ability of a model to cope with\n""homographic"" and ""unseen"" lexical constraints. Experiments on HOLLY and the\nprevious test setup show the effectiveness of our method. The effects of\nPLUMCOT are shown to be remarkable in ""unseen"" constraints. Our dataset is\navailable at https://github.com/papago-lab/HOLLY-benchmark', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Lexically-constrained NMT (LNMT) aims to incorporate user-provided\nterminology into translations. Despite its practical advantages, existing work\nhas not evaluated LNMT models under challenging real-world conditions. In this\npaper, we focus on two important but under-studied issues that lie in the\ncurrent evaluation process of LNMT studies. The model needs to cope with\nchallenging lexical constraints that are ""homographs"" or ""unseen"" during\ntraining. To this end, we first design a homograph disambiguation module to\ndifferentiate the meanings of homographs. Moreover, we propose PLUMCOT, which\nintegrates contextually rich information about unseen lexical constraints from\npre-trained language models and strengthens a copy mechanism of the pointer\nnetwork via direct supervision of a copying score. We also release HOLLY, an\nevaluation benchmark for assessing the ability of a model to cope with\n""homographic"" and ""unseen"" lexical constraints. Experiments on HOLLY and the\nprevious test setup show the effectiveness of our method. The effects of\nPLUMCOT are shown to be remarkable in ""unseen"" constraints. Our dataset is\navailable at https://github.com/papago-lab/HOLLY-benchmark'}, 'authors': [{'name': 'Yujin Baek'}, {'name': 'Koanho Lee'}, {'name': 'Dayeon Ki'}, {'name': 'Hyoung-Gyu Lee'}, {'name': 'Cheonbok Park'}, {'name': 'Jaegul Choo'}], 'author_detail': {'name': 'Jaegul Choo'}, 'arxiv_affiliation': 'KAIST', 'author': 'Jaegul Choo', 'arxiv_comment': 'Findings of ACL2023. 15 pages', 'links': [{'href': 'http://arxiv.org/abs/2306.12089v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.12089v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.14913v1,2023-06-19 15:59:28+00:00,2023-06-19 15:59:28+00:00,FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction,"[arxiv.Result.Author('Tianshuo Peng'), arxiv.Result.Author('Zuchao Li'), arxiv.Result.Author('Lefei Zhang'), arxiv.Result.Author('Bo Du'), arxiv.Result.Author('Hai Zhao')]","Universal Information Extraction (UIE) has been introduced as a unified
framework for various Information Extraction (IE) tasks and has achieved
widespread success. Despite this, UIE models have limitations. For example,
they rely heavily on span boundaries in the data during training, which does
not reflect the reality of span annotation challenges. Slight adjustments to
positions can also meet requirements. Additionally, UIE models lack attention
to the limited span length feature in IE. To address these deficiencies, we
propose the Fuzzy Span Universal Information Extraction (FSUIE) framework.
Specifically, our contribution consists of two concepts: fuzzy span loss and
fuzzy span attention. Our experimental results on a series of main IE tasks
show significant improvement compared to the baseline, especially in terms of
fast convergence and strong performance with small amounts of data and training
epochs. These results demonstrate the effectiveness and generalization of FSUIE
in different tasks, settings, and scenarios.",ACL2023,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2306.14913v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.14913v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.14913v1,"{'id': 'http://arxiv.org/abs/2306.14913v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.14913v1', 'updated': '2023-06-19T15:59:28Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=19, tm_hour=15, tm_min=59, tm_sec=28, tm_wday=0, tm_yday=170, tm_isdst=0), 'published': '2023-06-19T15:59:28Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=19, tm_hour=15, tm_min=59, tm_sec=28, tm_wday=0, tm_yday=170, tm_isdst=0), 'title': 'FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction'}, 'summary': 'Universal Information Extraction (UIE) has been introduced as a unified\nframework for various Information Extraction (IE) tasks and has achieved\nwidespread success. Despite this, UIE models have limitations. For example,\nthey rely heavily on span boundaries in the data during training, which does\nnot reflect the reality of span annotation challenges. Slight adjustments to\npositions can also meet requirements. Additionally, UIE models lack attention\nto the limited span length feature in IE. To address these deficiencies, we\npropose the Fuzzy Span Universal Information Extraction (FSUIE) framework.\nSpecifically, our contribution consists of two concepts: fuzzy span loss and\nfuzzy span attention. Our experimental results on a series of main IE tasks\nshow significant improvement compared to the baseline, especially in terms of\nfast convergence and strong performance with small amounts of data and training\nepochs. These results demonstrate the effectiveness and generalization of FSUIE\nin different tasks, settings, and scenarios.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Universal Information Extraction (UIE) has been introduced as a unified\nframework for various Information Extraction (IE) tasks and has achieved\nwidespread success. Despite this, UIE models have limitations. For example,\nthey rely heavily on span boundaries in the data during training, which does\nnot reflect the reality of span annotation challenges. Slight adjustments to\npositions can also meet requirements. Additionally, UIE models lack attention\nto the limited span length feature in IE. To address these deficiencies, we\npropose the Fuzzy Span Universal Information Extraction (FSUIE) framework.\nSpecifically, our contribution consists of two concepts: fuzzy span loss and\nfuzzy span attention. Our experimental results on a series of main IE tasks\nshow significant improvement compared to the baseline, especially in terms of\nfast convergence and strong performance with small amounts of data and training\nepochs. These results demonstrate the effectiveness and generalization of FSUIE\nin different tasks, settings, and scenarios.'}, 'authors': [{'name': 'Tianshuo Peng'}, {'name': 'Zuchao Li'}, {'name': 'Lefei Zhang'}, {'name': 'Bo Du'}, {'name': 'Hai Zhao'}], 'author_detail': {'name': 'Hai Zhao'}, 'author': 'Hai Zhao', 'arxiv_comment': 'ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.14913v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.14913v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.15164v1,2023-06-27 02:46:08+00:00,2023-06-27 02:46:08+00:00,DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization,"[arxiv.Result.Author('Songyang Gao'), arxiv.Result.Author('Shihan Dou'), arxiv.Result.Author('Yan Liu'), arxiv.Result.Author('Xiao Wang'), arxiv.Result.Author('Qi Zhang'), arxiv.Result.Author('Zhongyu Wei'), arxiv.Result.Author('Jin Ma'), arxiv.Result.Author('Ying Shan')]","Adversarial training is one of the best-performing methods in improving the
robustness of deep language models. However, robust models come at the cost of
high time consumption, as they require multi-step gradient ascents or word
substitutions to obtain adversarial samples. In addition, these generated
samples are deficient in grammatical quality and semantic consistency, which
impairs the effectiveness of adversarial training. To address these problems,
we introduce a novel, effective procedure for instead adversarial training with
only clean data. Our procedure, distribution shift risk minimization (DSRM),
estimates the adversarial loss by perturbing the input data's probability
distribution rather than their embeddings. This formulation results in a robust
model that minimizes the expected global loss under adversarial attacks. Our
approach requires zero adversarial samples for training and reduces time
consumption by up to 70\% compared to current best-performing adversarial
training methods. Experiments demonstrate that DSRM considerably improves
BERT's resistance to textual adversarial attacks and achieves state-of-the-art
robust accuracy on various benchmarks.",Accepted by ACL2023,,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2306.15164v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.15164v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.15164v1,"{'id': 'http://arxiv.org/abs/2306.15164v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.15164v1', 'updated': '2023-06-27T02:46:08Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=27, tm_hour=2, tm_min=46, tm_sec=8, tm_wday=1, tm_yday=178, tm_isdst=0), 'published': '2023-06-27T02:46:08Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=27, tm_hour=2, tm_min=46, tm_sec=8, tm_wday=1, tm_yday=178, tm_isdst=0), 'title': 'DSRM: Boost Textual Adversarial Training with Distribution Shift Risk\n  Minimization', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'DSRM: Boost Textual Adversarial Training with Distribution Shift Risk\n  Minimization'}, 'summary': ""Adversarial training is one of the best-performing methods in improving the\nrobustness of deep language models. However, robust models come at the cost of\nhigh time consumption, as they require multi-step gradient ascents or word\nsubstitutions to obtain adversarial samples. In addition, these generated\nsamples are deficient in grammatical quality and semantic consistency, which\nimpairs the effectiveness of adversarial training. To address these problems,\nwe introduce a novel, effective procedure for instead adversarial training with\nonly clean data. Our procedure, distribution shift risk minimization (DSRM),\nestimates the adversarial loss by perturbing the input data's probability\ndistribution rather than their embeddings. This formulation results in a robust\nmodel that minimizes the expected global loss under adversarial attacks. Our\napproach requires zero adversarial samples for training and reduces time\nconsumption by up to 70\\% compared to current best-performing adversarial\ntraining methods. Experiments demonstrate that DSRM considerably improves\nBERT's resistance to textual adversarial attacks and achieves state-of-the-art\nrobust accuracy on various benchmarks."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Adversarial training is one of the best-performing methods in improving the\nrobustness of deep language models. However, robust models come at the cost of\nhigh time consumption, as they require multi-step gradient ascents or word\nsubstitutions to obtain adversarial samples. In addition, these generated\nsamples are deficient in grammatical quality and semantic consistency, which\nimpairs the effectiveness of adversarial training. To address these problems,\nwe introduce a novel, effective procedure for instead adversarial training with\nonly clean data. Our procedure, distribution shift risk minimization (DSRM),\nestimates the adversarial loss by perturbing the input data's probability\ndistribution rather than their embeddings. This formulation results in a robust\nmodel that minimizes the expected global loss under adversarial attacks. Our\napproach requires zero adversarial samples for training and reduces time\nconsumption by up to 70\\% compared to current best-performing adversarial\ntraining methods. Experiments demonstrate that DSRM considerably improves\nBERT's resistance to textual adversarial attacks and achieves state-of-the-art\nrobust accuracy on various benchmarks.""}, 'authors': [{'name': 'Songyang Gao'}, {'name': 'Shihan Dou'}, {'name': 'Yan Liu'}, {'name': 'Xiao Wang'}, {'name': 'Qi Zhang'}, {'name': 'Zhongyu Wei'}, {'name': 'Jin Ma'}, {'name': 'Ying Shan'}], 'author_detail': {'name': 'Ying Shan'}, 'author': 'Ying Shan', 'arxiv_comment': 'Accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.15164v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.15164v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.15245v3,2023-09-01 16:11:40+00:00,2023-06-27 06:58:03+00:00,C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue Evaluation,"[arxiv.Result.Author('Liliang Ren'), arxiv.Result.Author('Mankeerat Sidhu'), arxiv.Result.Author('Qi Zeng'), arxiv.Result.Author('Revanth Gangi Reddy'), arxiv.Result.Author('Heng Ji'), arxiv.Result.Author('ChengXiang Zhai')]","Existing reference-free turn-level evaluation metrics for chatbots
inadequately capture the interaction between the user and the system.
Consequently, they often correlate poorly with human evaluations. To address
this issue, we propose a novel model-agnostic approach that leverages
Conditional Pointwise Mutual Information (C-PMI) to measure the turn-level
interaction between the system and the user based on a given evaluation
dimension. Experimental results on the widely used FED dialogue evaluation
dataset demonstrate that our approach significantly improves the correlation
with human judgment compared with existing evaluation systems. By replacing the
negative log-likelihood-based scorer with our proposed C-PMI scorer, we achieve
a relative 62.6% higher Spearman correlation on average for the FED evaluation
metric. Our code is publicly available at https://github.com/renll/C-PMI.",Published at ACL2023 DialDoc Workshop; Updated Results,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.15245v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.15245v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.15245v3,"{'id': 'http://arxiv.org/abs/2306.15245v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.15245v3', 'updated': '2023-09-01T16:11:40Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=1, tm_hour=16, tm_min=11, tm_sec=40, tm_wday=4, tm_yday=244, tm_isdst=0), 'published': '2023-06-27T06:58:03Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=27, tm_hour=6, tm_min=58, tm_sec=3, tm_wday=1, tm_yday=178, tm_isdst=0), 'title': 'C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue\n  Evaluation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue\n  Evaluation'}, 'summary': 'Existing reference-free turn-level evaluation metrics for chatbots\ninadequately capture the interaction between the user and the system.\nConsequently, they often correlate poorly with human evaluations. To address\nthis issue, we propose a novel model-agnostic approach that leverages\nConditional Pointwise Mutual Information (C-PMI) to measure the turn-level\ninteraction between the system and the user based on a given evaluation\ndimension. Experimental results on the widely used FED dialogue evaluation\ndataset demonstrate that our approach significantly improves the correlation\nwith human judgment compared with existing evaluation systems. By replacing the\nnegative log-likelihood-based scorer with our proposed C-PMI scorer, we achieve\na relative 62.6% higher Spearman correlation on average for the FED evaluation\nmetric. Our code is publicly available at https://github.com/renll/C-PMI.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Existing reference-free turn-level evaluation metrics for chatbots\ninadequately capture the interaction between the user and the system.\nConsequently, they often correlate poorly with human evaluations. To address\nthis issue, we propose a novel model-agnostic approach that leverages\nConditional Pointwise Mutual Information (C-PMI) to measure the turn-level\ninteraction between the system and the user based on a given evaluation\ndimension. Experimental results on the widely used FED dialogue evaluation\ndataset demonstrate that our approach significantly improves the correlation\nwith human judgment compared with existing evaluation systems. By replacing the\nnegative log-likelihood-based scorer with our proposed C-PMI scorer, we achieve\na relative 62.6% higher Spearman correlation on average for the FED evaluation\nmetric. Our code is publicly available at https://github.com/renll/C-PMI.'}, 'authors': [{'name': 'Liliang Ren'}, {'name': 'Mankeerat Sidhu'}, {'name': 'Qi Zeng'}, {'name': 'Revanth Gangi Reddy'}, {'name': 'Heng Ji'}, {'name': 'ChengXiang Zhai'}], 'author_detail': {'name': 'ChengXiang Zhai'}, 'author': 'ChengXiang Zhai', 'arxiv_comment': 'Published at ACL2023 DialDoc Workshop; Updated Results', 'links': [{'href': 'http://arxiv.org/abs/2306.15245v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.15245v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.15604v1,2023-06-27 16:42:36+00:00,2023-06-27 16:42:36+00:00,Constructing Multilingual Code Search Dataset Using Neural Machine Translation,"[arxiv.Result.Author('Ryo Sekizawa'), arxiv.Result.Author('Nan Duan'), arxiv.Result.Author('Shuai Lu'), arxiv.Result.Author('Hitomi Yanaka')]","Code search is a task to find programming codes that semantically match the
given natural language queries. Even though some of the existing datasets for
this task are multilingual on the programming language side, their query data
are only in English. In this research, we create a multilingual code search
dataset in four natural and four programming languages using a neural machine
translation model. Using our dataset, we pre-train and fine-tune the
Transformer-based models and then evaluate them on multiple code search test
sets. Our results show that the model pre-trained with all natural and
programming language data has performed best in most cases. By applying
back-translation data filtering to our dataset, we demonstrate that the
translation quality affects the model's performance to a certain extent, but
the data size matters more.","To appear in the Proceedings of the ACL2023 Student Research Workshop
  (SRW)",,,cs.CL,"['cs.CL', 'cs.SE']","[arxiv.Result.Link('http://arxiv.org/abs/2306.15604v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.15604v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.15604v1,"{'id': 'http://arxiv.org/abs/2306.15604v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.15604v1', 'updated': '2023-06-27T16:42:36Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=27, tm_hour=16, tm_min=42, tm_sec=36, tm_wday=1, tm_yday=178, tm_isdst=0), 'published': '2023-06-27T16:42:36Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=27, tm_hour=16, tm_min=42, tm_sec=36, tm_wday=1, tm_yday=178, tm_isdst=0), 'title': 'Constructing Multilingual Code Search Dataset Using Neural Machine\n  Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Constructing Multilingual Code Search Dataset Using Neural Machine\n  Translation'}, 'summary': ""Code search is a task to find programming codes that semantically match the\ngiven natural language queries. Even though some of the existing datasets for\nthis task are multilingual on the programming language side, their query data\nare only in English. In this research, we create a multilingual code search\ndataset in four natural and four programming languages using a neural machine\ntranslation model. Using our dataset, we pre-train and fine-tune the\nTransformer-based models and then evaluate them on multiple code search test\nsets. Our results show that the model pre-trained with all natural and\nprogramming language data has performed best in most cases. By applying\nback-translation data filtering to our dataset, we demonstrate that the\ntranslation quality affects the model's performance to a certain extent, but\nthe data size matters more."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Code search is a task to find programming codes that semantically match the\ngiven natural language queries. Even though some of the existing datasets for\nthis task are multilingual on the programming language side, their query data\nare only in English. In this research, we create a multilingual code search\ndataset in four natural and four programming languages using a neural machine\ntranslation model. Using our dataset, we pre-train and fine-tune the\nTransformer-based models and then evaluate them on multiple code search test\nsets. Our results show that the model pre-trained with all natural and\nprogramming language data has performed best in most cases. By applying\nback-translation data filtering to our dataset, we demonstrate that the\ntranslation quality affects the model's performance to a certain extent, but\nthe data size matters more.""}, 'authors': [{'name': 'Ryo Sekizawa'}, {'name': 'Nan Duan'}, {'name': 'Shuai Lu'}, {'name': 'Hitomi Yanaka'}], 'author_detail': {'name': 'Hitomi Yanaka'}, 'author': 'Hitomi Yanaka', 'arxiv_comment': 'To appear in the Proceedings of the ACL2023 Student Research Workshop\n  (SRW)', 'links': [{'href': 'http://arxiv.org/abs/2306.15604v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.15604v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.15705v1,2023-06-27 02:54:07+00:00,2023-06-27 02:54:07+00:00,On the Universal Adversarial Perturbations for Efficient Data-free Adversarial Detection,"[arxiv.Result.Author('Songyang Gao'), arxiv.Result.Author('Shihan Dou'), arxiv.Result.Author('Qi Zhang'), arxiv.Result.Author('Xuanjing Huang'), arxiv.Result.Author('Jin Ma'), arxiv.Result.Author('Ying Shan')]","Detecting adversarial samples that are carefully crafted to fool the model is
a critical step to socially-secure applications. However, existing adversarial
detection methods require access to sufficient training data, which brings
noteworthy concerns regarding privacy leakage and generalizability. In this
work, we validate that the adversarial sample generated by attack algorithms is
strongly related to a specific vector in the high-dimensional inputs. Such
vectors, namely UAPs (Universal Adversarial Perturbations), can be calculated
without original training data. Based on this discovery, we propose a
data-agnostic adversarial detection framework, which induces different
responses between normal and adversarial samples to UAPs. Experimental results
show that our method achieves competitive detection performance on various text
classification tasks, and maintains an equivalent time consumption to normal
inference.",Accepted by ACL2023 (Short Paper),,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2306.15705v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.15705v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.15705v1,"{'id': 'http://arxiv.org/abs/2306.15705v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.15705v1', 'updated': '2023-06-27T02:54:07Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=27, tm_hour=2, tm_min=54, tm_sec=7, tm_wday=1, tm_yday=178, tm_isdst=0), 'published': '2023-06-27T02:54:07Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=27, tm_hour=2, tm_min=54, tm_sec=7, tm_wday=1, tm_yday=178, tm_isdst=0), 'title': 'On the Universal Adversarial Perturbations for Efficient Data-free\n  Adversarial Detection', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'On the Universal Adversarial Perturbations for Efficient Data-free\n  Adversarial Detection'}, 'summary': 'Detecting adversarial samples that are carefully crafted to fool the model is\na critical step to socially-secure applications. However, existing adversarial\ndetection methods require access to sufficient training data, which brings\nnoteworthy concerns regarding privacy leakage and generalizability. In this\nwork, we validate that the adversarial sample generated by attack algorithms is\nstrongly related to a specific vector in the high-dimensional inputs. Such\nvectors, namely UAPs (Universal Adversarial Perturbations), can be calculated\nwithout original training data. Based on this discovery, we propose a\ndata-agnostic adversarial detection framework, which induces different\nresponses between normal and adversarial samples to UAPs. Experimental results\nshow that our method achieves competitive detection performance on various text\nclassification tasks, and maintains an equivalent time consumption to normal\ninference.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Detecting adversarial samples that are carefully crafted to fool the model is\na critical step to socially-secure applications. However, existing adversarial\ndetection methods require access to sufficient training data, which brings\nnoteworthy concerns regarding privacy leakage and generalizability. In this\nwork, we validate that the adversarial sample generated by attack algorithms is\nstrongly related to a specific vector in the high-dimensional inputs. Such\nvectors, namely UAPs (Universal Adversarial Perturbations), can be calculated\nwithout original training data. Based on this discovery, we propose a\ndata-agnostic adversarial detection framework, which induces different\nresponses between normal and adversarial samples to UAPs. Experimental results\nshow that our method achieves competitive detection performance on various text\nclassification tasks, and maintains an equivalent time consumption to normal\ninference.'}, 'authors': [{'name': 'Songyang Gao'}, {'name': 'Shihan Dou'}, {'name': 'Qi Zhang'}, {'name': 'Xuanjing Huang'}, {'name': 'Jin Ma'}, {'name': 'Ying Shan'}], 'author_detail': {'name': 'Ying Shan'}, 'author': 'Ying Shan', 'arxiv_comment': 'Accepted by ACL2023 (Short Paper)', 'links': [{'href': 'http://arxiv.org/abs/2306.15705v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.15705v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2307.00456v1,2023-07-02 02:34:57+00:00,2023-07-02 02:34:57+00:00,Make Text Unlearnable: Exploiting Effective Patterns to Protect Personal Data,"[arxiv.Result.Author('Xinzhe Li'), arxiv.Result.Author('Ming Liu'), arxiv.Result.Author('Shang Gao')]","This paper addresses the ethical concerns arising from the use of
unauthorized public data in deep learning models and proposes a novel solution.
Specifically, building on the work of Huang et al. (2021), we extend their
bi-level optimization approach to generate unlearnable text using a
gradient-based search technique. However, although effective, this approach
faces practical limitations, including the requirement of batches of instances
and model architecture knowledge that is not readily accessible to ordinary
users with limited access to their own data. Furthermore, even with
semantic-preserving constraints, unlearnable noise can alter the text's
semantics. To address these challenges, we extract simple patterns from
unlearnable text produced by bi-level optimization and demonstrate that the
data remains unlearnable for unknown models. Additionally, these patterns are
not instance- or dataset-specific, allowing users to readily apply them to text
classification and question-answering tasks, even if only a small proportion of
users implement them on their public content. We also open-source codes to
generate unlearnable text and assess unlearnable noise to benefit the public
and future studies.",,ACL2023 Third Workshop on Trustworthy Natural Language Processing,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2307.00456v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.00456v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2307.00456v1,"{'id': 'http://arxiv.org/abs/2307.00456v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2307.00456v1', 'updated': '2023-07-02T02:34:57Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=2, tm_hour=2, tm_min=34, tm_sec=57, tm_wday=6, tm_yday=183, tm_isdst=0), 'published': '2023-07-02T02:34:57Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=2, tm_hour=2, tm_min=34, tm_sec=57, tm_wday=6, tm_yday=183, tm_isdst=0), 'title': 'Make Text Unlearnable: Exploiting Effective Patterns to Protect Personal\n  Data', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Make Text Unlearnable: Exploiting Effective Patterns to Protect Personal\n  Data'}, 'summary': ""This paper addresses the ethical concerns arising from the use of\nunauthorized public data in deep learning models and proposes a novel solution.\nSpecifically, building on the work of Huang et al. (2021), we extend their\nbi-level optimization approach to generate unlearnable text using a\ngradient-based search technique. However, although effective, this approach\nfaces practical limitations, including the requirement of batches of instances\nand model architecture knowledge that is not readily accessible to ordinary\nusers with limited access to their own data. Furthermore, even with\nsemantic-preserving constraints, unlearnable noise can alter the text's\nsemantics. To address these challenges, we extract simple patterns from\nunlearnable text produced by bi-level optimization and demonstrate that the\ndata remains unlearnable for unknown models. Additionally, these patterns are\nnot instance- or dataset-specific, allowing users to readily apply them to text\nclassification and question-answering tasks, even if only a small proportion of\nusers implement them on their public content. We also open-source codes to\ngenerate unlearnable text and assess unlearnable noise to benefit the public\nand future studies."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""This paper addresses the ethical concerns arising from the use of\nunauthorized public data in deep learning models and proposes a novel solution.\nSpecifically, building on the work of Huang et al. (2021), we extend their\nbi-level optimization approach to generate unlearnable text using a\ngradient-based search technique. However, although effective, this approach\nfaces practical limitations, including the requirement of batches of instances\nand model architecture knowledge that is not readily accessible to ordinary\nusers with limited access to their own data. Furthermore, even with\nsemantic-preserving constraints, unlearnable noise can alter the text's\nsemantics. To address these challenges, we extract simple patterns from\nunlearnable text produced by bi-level optimization and demonstrate that the\ndata remains unlearnable for unknown models. Additionally, these patterns are\nnot instance- or dataset-specific, allowing users to readily apply them to text\nclassification and question-answering tasks, even if only a small proportion of\nusers implement them on their public content. We also open-source codes to\ngenerate unlearnable text and assess unlearnable noise to benefit the public\nand future studies.""}, 'authors': [{'name': 'Xinzhe Li'}, {'name': 'Ming Liu'}, {'name': 'Shang Gao'}], 'author_detail': {'name': 'Shang Gao'}, 'author': 'Shang Gao', 'arxiv_journal_ref': 'ACL2023 Third Workshop on Trustworthy Natural Language Processing', 'links': [{'href': 'http://arxiv.org/abs/2307.00456v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2307.00456v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2307.01900v1,2023-07-04 19:57:54+00:00,2023-07-04 19:57:54+00:00,Concept-Based Explanations to Test for False Causal Relationships Learned by Abusive Language Classifiers,"[arxiv.Result.Author('Isar Nejadgholi'), arxiv.Result.Author('Svetlana Kiritchenko'), arxiv.Result.Author('Kathleen C. Fraser'), arxiv.Result.Author('Esma Balkır')]","Classifiers tend to learn a false causal relationship between an
over-represented concept and a label, which can result in over-reliance on the
concept and compromised classification accuracy. It is imperative to have
methods in place that can compare different models and identify over-reliances
on specific concepts. We consider three well-known abusive language classifiers
trained on large English datasets and focus on the concept of negative
emotions, which is an important signal but should not be learned as a
sufficient feature for the label of abuse. Motivated by the definition of
global sufficiency, we first examine the unwanted dependencies learned by the
classifiers by assessing their accuracy on a challenge set across all decision
thresholds. Further, recognizing that a challenge set might not always be
available, we introduce concept-based explanation metrics to assess the
influence of the concept on the labels. These explanations allow us to compare
classifiers regarding the degree of false global sufficiency they have learned
between a concept and a label.",Published at WOAH2023 co-located with ACL2023,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2307.01900v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.01900v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2307.01900v1,"{'id': 'http://arxiv.org/abs/2307.01900v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2307.01900v1', 'updated': '2023-07-04T19:57:54Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=4, tm_hour=19, tm_min=57, tm_sec=54, tm_wday=1, tm_yday=185, tm_isdst=0), 'published': '2023-07-04T19:57:54Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=4, tm_hour=19, tm_min=57, tm_sec=54, tm_wday=1, tm_yday=185, tm_isdst=0), 'title': 'Concept-Based Explanations to Test for False Causal Relationships\n  Learned by Abusive Language Classifiers', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Concept-Based Explanations to Test for False Causal Relationships\n  Learned by Abusive Language Classifiers'}, 'summary': 'Classifiers tend to learn a false causal relationship between an\nover-represented concept and a label, which can result in over-reliance on the\nconcept and compromised classification accuracy. It is imperative to have\nmethods in place that can compare different models and identify over-reliances\non specific concepts. We consider three well-known abusive language classifiers\ntrained on large English datasets and focus on the concept of negative\nemotions, which is an important signal but should not be learned as a\nsufficient feature for the label of abuse. Motivated by the definition of\nglobal sufficiency, we first examine the unwanted dependencies learned by the\nclassifiers by assessing their accuracy on a challenge set across all decision\nthresholds. Further, recognizing that a challenge set might not always be\navailable, we introduce concept-based explanation metrics to assess the\ninfluence of the concept on the labels. These explanations allow us to compare\nclassifiers regarding the degree of false global sufficiency they have learned\nbetween a concept and a label.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Classifiers tend to learn a false causal relationship between an\nover-represented concept and a label, which can result in over-reliance on the\nconcept and compromised classification accuracy. It is imperative to have\nmethods in place that can compare different models and identify over-reliances\non specific concepts. We consider three well-known abusive language classifiers\ntrained on large English datasets and focus on the concept of negative\nemotions, which is an important signal but should not be learned as a\nsufficient feature for the label of abuse. Motivated by the definition of\nglobal sufficiency, we first examine the unwanted dependencies learned by the\nclassifiers by assessing their accuracy on a challenge set across all decision\nthresholds. Further, recognizing that a challenge set might not always be\navailable, we introduce concept-based explanation metrics to assess the\ninfluence of the concept on the labels. These explanations allow us to compare\nclassifiers regarding the degree of false global sufficiency they have learned\nbetween a concept and a label.'}, 'authors': [{'name': 'Isar Nejadgholi'}, {'name': 'Svetlana Kiritchenko'}, {'name': 'Kathleen C. Fraser'}, {'name': 'Esma Balkır'}], 'author_detail': {'name': 'Esma Balkır'}, 'author': 'Esma Balkır', 'arxiv_comment': 'Published at WOAH2023 co-located with ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2307.01900v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2307.01900v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2307.02716v1,2023-07-06 01:46:00+00:00,2023-07-06 01:46:00+00:00,CFSum: A Coarse-to-Fine Contribution Network for Multimodal Summarization,"[arxiv.Result.Author('Min Xiao'), arxiv.Result.Author('Junnan Zhu'), arxiv.Result.Author('Haitao Lin'), arxiv.Result.Author('Yu Zhou'), arxiv.Result.Author('Chengqing Zong')]","Multimodal summarization usually suffers from the problem that the
contribution of the visual modality is unclear. Existing multimodal
summarization approaches focus on designing the fusion methods of different
modalities, while ignoring the adaptive conditions under which visual
modalities are useful. Therefore, we propose a novel Coarse-to-Fine
contribution network for multimodal Summarization (CFSum) to consider different
contributions of images for summarization. First, to eliminate the interference
of useless images, we propose a pre-filter module to abandon useless images.
Second, to make accurate use of useful images, we propose two levels of visual
complement modules, word level and phrase level. Specifically, image
contributions are calculated and are adopted to guide the attention of both
textual and visual modalities. Experimental results have shown that CFSum
significantly outperforms multiple strong baselines on the standard benchmark.
Furthermore, the analysis verifies that useful images can even help generate
non-visual words which are implicitly represented in the image.",acl2023,,,cs.CL,"['cs.CL', 'cs.CV']","[arxiv.Result.Link('http://arxiv.org/abs/2307.02716v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.02716v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2307.02716v1,"{'id': 'http://arxiv.org/abs/2307.02716v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2307.02716v1', 'updated': '2023-07-06T01:46:00Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=6, tm_hour=1, tm_min=46, tm_sec=0, tm_wday=3, tm_yday=187, tm_isdst=0), 'published': '2023-07-06T01:46:00Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=6, tm_hour=1, tm_min=46, tm_sec=0, tm_wday=3, tm_yday=187, tm_isdst=0), 'title': 'CFSum: A Coarse-to-Fine Contribution Network for Multimodal\n  Summarization', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'CFSum: A Coarse-to-Fine Contribution Network for Multimodal\n  Summarization'}, 'summary': 'Multimodal summarization usually suffers from the problem that the\ncontribution of the visual modality is unclear. Existing multimodal\nsummarization approaches focus on designing the fusion methods of different\nmodalities, while ignoring the adaptive conditions under which visual\nmodalities are useful. Therefore, we propose a novel Coarse-to-Fine\ncontribution network for multimodal Summarization (CFSum) to consider different\ncontributions of images for summarization. First, to eliminate the interference\nof useless images, we propose a pre-filter module to abandon useless images.\nSecond, to make accurate use of useful images, we propose two levels of visual\ncomplement modules, word level and phrase level. Specifically, image\ncontributions are calculated and are adopted to guide the attention of both\ntextual and visual modalities. Experimental results have shown that CFSum\nsignificantly outperforms multiple strong baselines on the standard benchmark.\nFurthermore, the analysis verifies that useful images can even help generate\nnon-visual words which are implicitly represented in the image.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Multimodal summarization usually suffers from the problem that the\ncontribution of the visual modality is unclear. Existing multimodal\nsummarization approaches focus on designing the fusion methods of different\nmodalities, while ignoring the adaptive conditions under which visual\nmodalities are useful. Therefore, we propose a novel Coarse-to-Fine\ncontribution network for multimodal Summarization (CFSum) to consider different\ncontributions of images for summarization. First, to eliminate the interference\nof useless images, we propose a pre-filter module to abandon useless images.\nSecond, to make accurate use of useful images, we propose two levels of visual\ncomplement modules, word level and phrase level. Specifically, image\ncontributions are calculated and are adopted to guide the attention of both\ntextual and visual modalities. Experimental results have shown that CFSum\nsignificantly outperforms multiple strong baselines on the standard benchmark.\nFurthermore, the analysis verifies that useful images can even help generate\nnon-visual words which are implicitly represented in the image.'}, 'authors': [{'name': 'Min Xiao'}, {'name': 'Junnan Zhu'}, {'name': 'Haitao Lin'}, {'name': 'Yu Zhou'}, {'name': 'Chengqing Zong'}], 'author_detail': {'name': 'Chengqing Zong'}, 'author': 'Chengqing Zong', 'arxiv_comment': 'acl2023', 'links': [{'href': 'http://arxiv.org/abs/2307.02716v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2307.02716v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2307.02830v1,2023-07-06 07:53:46+00:00,2023-07-06 07:53:46+00:00,Generative Zero-Shot Prompt Learning for Cross-Domain Slot Filling with Inverse Prompting,"[arxiv.Result.Author('Xuefeng Li'), arxiv.Result.Author('Liwen Wang'), arxiv.Result.Author('Guanting Dong'), arxiv.Result.Author('Keqing He'), arxiv.Result.Author('Jinzheng Zhao'), arxiv.Result.Author('Hao Lei'), arxiv.Result.Author('Jiachi Liu'), arxiv.Result.Author('Weiran Xu')]","Zero-shot cross-domain slot filling aims to transfer knowledge from the
labeled source domain to the unlabeled target domain. Existing models either
encode slot descriptions and examples or design handcrafted question templates
using heuristic rules, suffering from poor generalization capability or
robustness. In this paper, we propose a generative zero-shot prompt learning
framework for cross-domain slot filling, both improving generalization and
robustness than previous work. Besides, we introduce a novel inverse prompting
strategy to distinguish different slot types to avoid the multiple prediction
problem, and an efficient prompt-tuning strategy to boost higher performance by
only training fewer prompt parameters. Experiments and analysis demonstrate the
effectiveness of our proposed framework, especially huge improvements (+13.44%
F1) on the unseen slots.",Accepted by the Findings of ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2307.02830v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.02830v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2307.02830v1,"{'id': 'http://arxiv.org/abs/2307.02830v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2307.02830v1', 'updated': '2023-07-06T07:53:46Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=6, tm_hour=7, tm_min=53, tm_sec=46, tm_wday=3, tm_yday=187, tm_isdst=0), 'published': '2023-07-06T07:53:46Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=6, tm_hour=7, tm_min=53, tm_sec=46, tm_wday=3, tm_yday=187, tm_isdst=0), 'title': 'Generative Zero-Shot Prompt Learning for Cross-Domain Slot Filling with\n  Inverse Prompting', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Generative Zero-Shot Prompt Learning for Cross-Domain Slot Filling with\n  Inverse Prompting'}, 'summary': 'Zero-shot cross-domain slot filling aims to transfer knowledge from the\nlabeled source domain to the unlabeled target domain. Existing models either\nencode slot descriptions and examples or design handcrafted question templates\nusing heuristic rules, suffering from poor generalization capability or\nrobustness. In this paper, we propose a generative zero-shot prompt learning\nframework for cross-domain slot filling, both improving generalization and\nrobustness than previous work. Besides, we introduce a novel inverse prompting\nstrategy to distinguish different slot types to avoid the multiple prediction\nproblem, and an efficient prompt-tuning strategy to boost higher performance by\nonly training fewer prompt parameters. Experiments and analysis demonstrate the\neffectiveness of our proposed framework, especially huge improvements (+13.44%\nF1) on the unseen slots.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Zero-shot cross-domain slot filling aims to transfer knowledge from the\nlabeled source domain to the unlabeled target domain. Existing models either\nencode slot descriptions and examples or design handcrafted question templates\nusing heuristic rules, suffering from poor generalization capability or\nrobustness. In this paper, we propose a generative zero-shot prompt learning\nframework for cross-domain slot filling, both improving generalization and\nrobustness than previous work. Besides, we introduce a novel inverse prompting\nstrategy to distinguish different slot types to avoid the multiple prediction\nproblem, and an efficient prompt-tuning strategy to boost higher performance by\nonly training fewer prompt parameters. Experiments and analysis demonstrate the\neffectiveness of our proposed framework, especially huge improvements (+13.44%\nF1) on the unseen slots.'}, 'authors': [{'name': 'Xuefeng Li'}, {'name': 'Liwen Wang'}, {'name': 'Guanting Dong'}, {'name': 'Keqing He'}, {'name': 'Jinzheng Zhao'}, {'name': 'Hao Lei'}, {'name': 'Jiachi Liu'}, {'name': 'Weiran Xu'}], 'author_detail': {'name': 'Weiran Xu'}, 'author': 'Weiran Xu', 'arxiv_comment': 'Accepted by the Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2307.02830v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2307.02830v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2307.04018v1,2023-07-08 17:20:56+00:00,2023-07-08 17:20:56+00:00,Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New Benchmark with Improved Annotation,"[arxiv.Result.Author('Yulong Chen'), arxiv.Result.Author('Huajian Zhang'), arxiv.Result.Author('Yijie Zhou'), arxiv.Result.Author('Xuefeng Bai'), arxiv.Result.Author('Yueguan Wang'), arxiv.Result.Author('Ming Zhong'), arxiv.Result.Author('Jianhao Yan'), arxiv.Result.Author('Yafu Li'), arxiv.Result.Author('Judy Li'), arxiv.Result.Author('Michael Zhu'), arxiv.Result.Author('Yue Zhang')]","Most existing cross-lingual summarization (CLS) work constructs CLS corpora
by simply and directly translating pre-annotated summaries from one language to
another, which can contain errors from both summarization and translation
processes. To address this issue, we propose ConvSumX, a cross-lingual
conversation summarization benchmark, through a new annotation schema that
explicitly considers source input context. ConvSumX consists of 2 sub-tasks
under different real-world scenarios, with each covering 3 language directions.
We conduct thorough analysis on ConvSumX and 3 widely-used manually annotated
CLS corpora and empirically find that ConvSumX is more faithful towards input
text. Additionally, based on the same intuition, we propose a 2-Step method,
which takes both conversation and summary as input to simulate human annotation
process. Experimental results show that 2-Step method surpasses strong
baselines on ConvSumX under both automatic and human evaluation. Analysis shows
that both source input text and summary are crucial for modeling cross-lingual
summaries.",ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2307.04018v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.04018v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2307.04018v1,"{'id': 'http://arxiv.org/abs/2307.04018v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2307.04018v1', 'updated': '2023-07-08T17:20:56Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=8, tm_hour=17, tm_min=20, tm_sec=56, tm_wday=5, tm_yday=189, tm_isdst=0), 'published': '2023-07-08T17:20:56Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=8, tm_hour=17, tm_min=20, tm_sec=56, tm_wday=5, tm_yday=189, tm_isdst=0), 'title': 'Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New\n  Benchmark with Improved Annotation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Revisiting Cross-Lingual Summarization: A Corpus-based Study and A New\n  Benchmark with Improved Annotation'}, 'summary': 'Most existing cross-lingual summarization (CLS) work constructs CLS corpora\nby simply and directly translating pre-annotated summaries from one language to\nanother, which can contain errors from both summarization and translation\nprocesses. To address this issue, we propose ConvSumX, a cross-lingual\nconversation summarization benchmark, through a new annotation schema that\nexplicitly considers source input context. ConvSumX consists of 2 sub-tasks\nunder different real-world scenarios, with each covering 3 language directions.\nWe conduct thorough analysis on ConvSumX and 3 widely-used manually annotated\nCLS corpora and empirically find that ConvSumX is more faithful towards input\ntext. Additionally, based on the same intuition, we propose a 2-Step method,\nwhich takes both conversation and summary as input to simulate human annotation\nprocess. Experimental results show that 2-Step method surpasses strong\nbaselines on ConvSumX under both automatic and human evaluation. Analysis shows\nthat both source input text and summary are crucial for modeling cross-lingual\nsummaries.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Most existing cross-lingual summarization (CLS) work constructs CLS corpora\nby simply and directly translating pre-annotated summaries from one language to\nanother, which can contain errors from both summarization and translation\nprocesses. To address this issue, we propose ConvSumX, a cross-lingual\nconversation summarization benchmark, through a new annotation schema that\nexplicitly considers source input context. ConvSumX consists of 2 sub-tasks\nunder different real-world scenarios, with each covering 3 language directions.\nWe conduct thorough analysis on ConvSumX and 3 widely-used manually annotated\nCLS corpora and empirically find that ConvSumX is more faithful towards input\ntext. Additionally, based on the same intuition, we propose a 2-Step method,\nwhich takes both conversation and summary as input to simulate human annotation\nprocess. Experimental results show that 2-Step method surpasses strong\nbaselines on ConvSumX under both automatic and human evaluation. Analysis shows\nthat both source input text and summary are crucial for modeling cross-lingual\nsummaries.'}, 'authors': [{'name': 'Yulong Chen'}, {'name': 'Huajian Zhang'}, {'name': 'Yijie Zhou'}, {'name': 'Xuefeng Bai'}, {'name': 'Yueguan Wang'}, {'name': 'Ming Zhong'}, {'name': 'Jianhao Yan'}, {'name': 'Yafu Li'}, {'name': 'Judy Li'}, {'name': 'Michael Zhu'}, {'name': 'Yue Zhang'}], 'author_detail': {'name': 'Yue Zhang'}, 'author': 'Yue Zhang', 'arxiv_comment': 'ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2307.04018v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2307.04018v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2307.07135v1,2023-07-14 03:22:51+00:00,2023-07-14 03:22:51+00:00,MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System,"[arxiv.Result.Author('Libo Qin'), arxiv.Result.Author('Shijue Huang'), arxiv.Result.Author('Qiguang Chen'), arxiv.Result.Author('Chenran Cai'), arxiv.Result.Author('Yudi Zhang'), arxiv.Result.Author('Bin Liang'), arxiv.Result.Author('Wanxiang Che'), arxiv.Result.Author('Ruifeng Xu')]","Multi-modal sarcasm detection has attracted much recent attention.
Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder
the development of reliable multi-modal sarcasm detection system: (1) There are
some spurious cues in MMSD, leading to the model bias learning; (2) The
negative samples in MMSD are not always reasonable. To solve the aforementioned
issues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings
of MMSD, by removing the spurious cues and re-annotating the unreasonable
samples. Meanwhile, we present a novel framework called multi-view CLIP that is
capable of leveraging multi-grained cues from multiple perspectives (i.e.,
text, image, and text-image interaction view) for multi-modal sarcasm
detection. Extensive experiments show that MMSD2.0 is a valuable benchmark for
building reliable multi-modal sarcasm detection systems and multi-view CLIP can
significantly outperform the previous best baselines.",Accepted by ACL2023 Findings,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2307.07135v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.07135v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2307.07135v1,"{'id': 'http://arxiv.org/abs/2307.07135v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2307.07135v1', 'updated': '2023-07-14T03:22:51Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=14, tm_hour=3, tm_min=22, tm_sec=51, tm_wday=4, tm_yday=195, tm_isdst=0), 'published': '2023-07-14T03:22:51Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=14, tm_hour=3, tm_min=22, tm_sec=51, tm_wday=4, tm_yday=195, tm_isdst=0), 'title': 'MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System'}, 'summary': 'Multi-modal sarcasm detection has attracted much recent attention.\nNevertheless, the existing benchmark (MMSD) has some shortcomings that hinder\nthe development of reliable multi-modal sarcasm detection system: (1) There are\nsome spurious cues in MMSD, leading to the model bias learning; (2) The\nnegative samples in MMSD are not always reasonable. To solve the aforementioned\nissues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings\nof MMSD, by removing the spurious cues and re-annotating the unreasonable\nsamples. Meanwhile, we present a novel framework called multi-view CLIP that is\ncapable of leveraging multi-grained cues from multiple perspectives (i.e.,\ntext, image, and text-image interaction view) for multi-modal sarcasm\ndetection. Extensive experiments show that MMSD2.0 is a valuable benchmark for\nbuilding reliable multi-modal sarcasm detection systems and multi-view CLIP can\nsignificantly outperform the previous best baselines.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Multi-modal sarcasm detection has attracted much recent attention.\nNevertheless, the existing benchmark (MMSD) has some shortcomings that hinder\nthe development of reliable multi-modal sarcasm detection system: (1) There are\nsome spurious cues in MMSD, leading to the model bias learning; (2) The\nnegative samples in MMSD are not always reasonable. To solve the aforementioned\nissues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings\nof MMSD, by removing the spurious cues and re-annotating the unreasonable\nsamples. Meanwhile, we present a novel framework called multi-view CLIP that is\ncapable of leveraging multi-grained cues from multiple perspectives (i.e.,\ntext, image, and text-image interaction view) for multi-modal sarcasm\ndetection. Extensive experiments show that MMSD2.0 is a valuable benchmark for\nbuilding reliable multi-modal sarcasm detection systems and multi-view CLIP can\nsignificantly outperform the previous best baselines.'}, 'authors': [{'name': 'Libo Qin'}, {'name': 'Shijue Huang'}, {'name': 'Qiguang Chen'}, {'name': 'Chenran Cai'}, {'name': 'Yudi Zhang'}, {'name': 'Bin Liang'}, {'name': 'Wanxiang Che'}, {'name': 'Ruifeng Xu'}], 'author_detail': {'name': 'Ruifeng Xu'}, 'author': 'Ruifeng Xu', 'arxiv_comment': 'Accepted by ACL2023 Findings', 'links': [{'href': 'http://arxiv.org/abs/2307.07135v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2307.07135v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2307.10543v1,2023-07-20 02:48:04+00:00,2023-07-20 02:48:04+00:00,TREA: Tree-Structure Reasoning Schema for Conversational Recommendation,"[arxiv.Result.Author('Wendi Li'), arxiv.Result.Author('Wei Wei'), arxiv.Result.Author('Xiaoye Qu'), arxiv.Result.Author('Xian-Ling Mao'), arxiv.Result.Author('Ye Yuan'), arxiv.Result.Author('Wenfeng Xie'), arxiv.Result.Author('Dangyang Chen')]","Conversational recommender systems (CRS) aim to timely trace the dynamic
interests of users through dialogues and generate relevant responses for item
recommendations. Recently, various external knowledge bases (especially
knowledge graphs) are incorporated into CRS to enhance the understanding of
conversation contexts. However, recent reasoning-based models heavily rely on
simplified structures such as linear structures or fixed-hierarchical
structures for causality reasoning, hence they cannot fully figure out
sophisticated relationships among utterances with external knowledge. To
address this, we propose a novel Tree structure Reasoning schEmA named TREA.
TREA constructs a multi-hierarchical scalable tree as the reasoning structure
to clarify the causal relationships between mentioned entities, and fully
utilizes historical conversations to generate more reasonable and suitable
responses for recommended results. Extensive experiments on two public CRS
datasets have demonstrated the effectiveness of our approach.",Accepted by ACL2023 main conference,,,cs.AI,['cs.AI'],"[arxiv.Result.Link('http://arxiv.org/abs/2307.10543v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.10543v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2307.10543v1,"{'id': 'http://arxiv.org/abs/2307.10543v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2307.10543v1', 'updated': '2023-07-20T02:48:04Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=20, tm_hour=2, tm_min=48, tm_sec=4, tm_wday=3, tm_yday=201, tm_isdst=0), 'published': '2023-07-20T02:48:04Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=20, tm_hour=2, tm_min=48, tm_sec=4, tm_wday=3, tm_yday=201, tm_isdst=0), 'title': 'TREA: Tree-Structure Reasoning Schema for Conversational Recommendation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'TREA: Tree-Structure Reasoning Schema for Conversational Recommendation'}, 'summary': 'Conversational recommender systems (CRS) aim to timely trace the dynamic\ninterests of users through dialogues and generate relevant responses for item\nrecommendations. Recently, various external knowledge bases (especially\nknowledge graphs) are incorporated into CRS to enhance the understanding of\nconversation contexts. However, recent reasoning-based models heavily rely on\nsimplified structures such as linear structures or fixed-hierarchical\nstructures for causality reasoning, hence they cannot fully figure out\nsophisticated relationships among utterances with external knowledge. To\naddress this, we propose a novel Tree structure Reasoning schEmA named TREA.\nTREA constructs a multi-hierarchical scalable tree as the reasoning structure\nto clarify the causal relationships between mentioned entities, and fully\nutilizes historical conversations to generate more reasonable and suitable\nresponses for recommended results. Extensive experiments on two public CRS\ndatasets have demonstrated the effectiveness of our approach.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Conversational recommender systems (CRS) aim to timely trace the dynamic\ninterests of users through dialogues and generate relevant responses for item\nrecommendations. Recently, various external knowledge bases (especially\nknowledge graphs) are incorporated into CRS to enhance the understanding of\nconversation contexts. However, recent reasoning-based models heavily rely on\nsimplified structures such as linear structures or fixed-hierarchical\nstructures for causality reasoning, hence they cannot fully figure out\nsophisticated relationships among utterances with external knowledge. To\naddress this, we propose a novel Tree structure Reasoning schEmA named TREA.\nTREA constructs a multi-hierarchical scalable tree as the reasoning structure\nto clarify the causal relationships between mentioned entities, and fully\nutilizes historical conversations to generate more reasonable and suitable\nresponses for recommended results. Extensive experiments on two public CRS\ndatasets have demonstrated the effectiveness of our approach.'}, 'authors': [{'name': 'Wendi Li'}, {'name': 'Wei Wei'}, {'name': 'Xiaoye Qu'}, {'name': 'Xian-Ling Mao'}, {'name': 'Ye Yuan'}, {'name': 'Wenfeng Xie'}, {'name': 'Dangyang Chen'}], 'author_detail': {'name': 'Dangyang Chen'}, 'author': 'Dangyang Chen', 'arxiv_comment': 'Accepted by ACL2023 main conference', 'links': [{'href': 'http://arxiv.org/abs/2307.10543v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2307.10543v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2308.16588v1,2023-08-31 09:35:52+00:00,2023-08-31 09:35:52+00:00,Interpreting Sentiment Composition with Latent Semantic Tree,"[arxiv.Result.Author('Zhongtao Jiang'), arxiv.Result.Author('Yuanzhe Zhang'), arxiv.Result.Author('Cao Liu'), arxiv.Result.Author('Jiansong Chen'), arxiv.Result.Author('Jun Zhao'), arxiv.Result.Author('Kang Liu')]","As the key to sentiment analysis, sentiment composition considers the
classification of a constituent via classifications of its contained
sub-constituents and rules operated on them. Such compositionality has been
widely studied previously in the form of hierarchical trees including untagged
and sentiment ones, which are intrinsically suboptimal in our view. To address
this, we propose semantic tree, a new tree form capable of interpreting the
sentiment composition in a principled way. Semantic tree is a derivation of a
context-free grammar (CFG) describing the specific composition rules on
difference semantic roles, which is designed carefully following previous
linguistic conclusions. However, semantic tree is a latent variable since there
is no its annotation in regular datasets. Thus, in our method, it is
marginalized out via inside algorithm and learned to optimize the
classification performance. Quantitative and qualitative results demonstrate
that our method not only achieves better or competitive results compared to
baselines in the setting of regular and domain adaptation classification, and
also generates plausible tree explanations.",Findings of ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2308.16588v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2308.16588v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2308.16588v1,"{'id': 'http://arxiv.org/abs/2308.16588v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2308.16588v1', 'updated': '2023-08-31T09:35:52Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=31, tm_hour=9, tm_min=35, tm_sec=52, tm_wday=3, tm_yday=243, tm_isdst=0), 'published': '2023-08-31T09:35:52Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=8, tm_mday=31, tm_hour=9, tm_min=35, tm_sec=52, tm_wday=3, tm_yday=243, tm_isdst=0), 'title': 'Interpreting Sentiment Composition with Latent Semantic Tree', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Interpreting Sentiment Composition with Latent Semantic Tree'}, 'summary': 'As the key to sentiment analysis, sentiment composition considers the\nclassification of a constituent via classifications of its contained\nsub-constituents and rules operated on them. Such compositionality has been\nwidely studied previously in the form of hierarchical trees including untagged\nand sentiment ones, which are intrinsically suboptimal in our view. To address\nthis, we propose semantic tree, a new tree form capable of interpreting the\nsentiment composition in a principled way. Semantic tree is a derivation of a\ncontext-free grammar (CFG) describing the specific composition rules on\ndifference semantic roles, which is designed carefully following previous\nlinguistic conclusions. However, semantic tree is a latent variable since there\nis no its annotation in regular datasets. Thus, in our method, it is\nmarginalized out via inside algorithm and learned to optimize the\nclassification performance. Quantitative and qualitative results demonstrate\nthat our method not only achieves better or competitive results compared to\nbaselines in the setting of regular and domain adaptation classification, and\nalso generates plausible tree explanations.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'As the key to sentiment analysis, sentiment composition considers the\nclassification of a constituent via classifications of its contained\nsub-constituents and rules operated on them. Such compositionality has been\nwidely studied previously in the form of hierarchical trees including untagged\nand sentiment ones, which are intrinsically suboptimal in our view. To address\nthis, we propose semantic tree, a new tree form capable of interpreting the\nsentiment composition in a principled way. Semantic tree is a derivation of a\ncontext-free grammar (CFG) describing the specific composition rules on\ndifference semantic roles, which is designed carefully following previous\nlinguistic conclusions. However, semantic tree is a latent variable since there\nis no its annotation in regular datasets. Thus, in our method, it is\nmarginalized out via inside algorithm and learned to optimize the\nclassification performance. Quantitative and qualitative results demonstrate\nthat our method not only achieves better or competitive results compared to\nbaselines in the setting of regular and domain adaptation classification, and\nalso generates plausible tree explanations.'}, 'authors': [{'name': 'Zhongtao Jiang'}, {'name': 'Yuanzhe Zhang'}, {'name': 'Cao Liu'}, {'name': 'Jiansong Chen'}, {'name': 'Jun Zhao'}, {'name': 'Kang Liu'}], 'author_detail': {'name': 'Kang Liu'}, 'author': 'Kang Liu', 'arxiv_comment': 'Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2308.16588v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2308.16588v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2309.08156v2,2023-09-18 00:43:47+00:00,2023-09-15 04:47:19+00:00,RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue,"[arxiv.Result.Author('Zhengliang Shi'), arxiv.Result.Author('Weiwei Sun'), arxiv.Result.Author('Shuo Zhang'), arxiv.Result.Author('Zhen Zhang'), arxiv.Result.Author('Pengjie Ren'), arxiv.Result.Author('Zhaochun Ren')]","Evaluating open-domain dialogue systems is challenging for reasons such as
the one-to-many problem, i.e., many appropriate responses other than just the
golden response. As of now, automatic evaluation methods need better
consistency with humans, while reliable human evaluation can be time- and
cost-intensive. To this end, we propose the Reference-Assisted Dialogue
Evaluation (RADE) approach under the multi-task learning framework, which
leverages the pre-created utterance as reference other than the gold response
to relief the one-to-many problem. Specifically, RADE explicitly compares
reference and the candidate response to predict their overall scores. Moreover,
an auxiliary response generation task enhances prediction via a shared encoder.
To support RADE, we extend three datasets with additional rated responses other
than just a golden response by human annotation. Experiments on our three
datasets and two existing benchmarks demonstrate the effectiveness of our
method, where Pearson, Spearman, and Kendall correlations with human evaluation
outperform state-of-the-art baselines.","19 pages, Accepted by ACL2023 main conference",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2309.08156v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2309.08156v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2309.08156v2,"{'id': 'http://arxiv.org/abs/2309.08156v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2309.08156v2', 'updated': '2023-09-18T00:43:47Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=18, tm_hour=0, tm_min=43, tm_sec=47, tm_wday=0, tm_yday=261, tm_isdst=0), 'published': '2023-09-15T04:47:19Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=15, tm_hour=4, tm_min=47, tm_sec=19, tm_wday=4, tm_yday=258, tm_isdst=0), 'title': 'RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'RADE: Reference-Assisted Dialogue Evaluation for Open-Domain Dialogue'}, 'summary': 'Evaluating open-domain dialogue systems is challenging for reasons such as\nthe one-to-many problem, i.e., many appropriate responses other than just the\ngolden response. As of now, automatic evaluation methods need better\nconsistency with humans, while reliable human evaluation can be time- and\ncost-intensive. To this end, we propose the Reference-Assisted Dialogue\nEvaluation (RADE) approach under the multi-task learning framework, which\nleverages the pre-created utterance as reference other than the gold response\nto relief the one-to-many problem. Specifically, RADE explicitly compares\nreference and the candidate response to predict their overall scores. Moreover,\nan auxiliary response generation task enhances prediction via a shared encoder.\nTo support RADE, we extend three datasets with additional rated responses other\nthan just a golden response by human annotation. Experiments on our three\ndatasets and two existing benchmarks demonstrate the effectiveness of our\nmethod, where Pearson, Spearman, and Kendall correlations with human evaluation\noutperform state-of-the-art baselines.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Evaluating open-domain dialogue systems is challenging for reasons such as\nthe one-to-many problem, i.e., many appropriate responses other than just the\ngolden response. As of now, automatic evaluation methods need better\nconsistency with humans, while reliable human evaluation can be time- and\ncost-intensive. To this end, we propose the Reference-Assisted Dialogue\nEvaluation (RADE) approach under the multi-task learning framework, which\nleverages the pre-created utterance as reference other than the gold response\nto relief the one-to-many problem. Specifically, RADE explicitly compares\nreference and the candidate response to predict their overall scores. Moreover,\nan auxiliary response generation task enhances prediction via a shared encoder.\nTo support RADE, we extend three datasets with additional rated responses other\nthan just a golden response by human annotation. Experiments on our three\ndatasets and two existing benchmarks demonstrate the effectiveness of our\nmethod, where Pearson, Spearman, and Kendall correlations with human evaluation\noutperform state-of-the-art baselines.'}, 'authors': [{'name': 'Zhengliang Shi'}, {'name': 'Weiwei Sun'}, {'name': 'Shuo Zhang'}, {'name': 'Zhen Zhang'}, {'name': 'Pengjie Ren'}, {'name': 'Zhaochun Ren'}], 'author_detail': {'name': 'Zhaochun Ren'}, 'author': 'Zhaochun Ren', 'arxiv_comment': '19 pages, Accepted by ACL2023 main conference', 'links': [{'href': 'http://arxiv.org/abs/2309.08156v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2309.08156v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2311.15211v1,2023-11-26 06:56:02+00:00,2023-11-26 06:56:02+00:00,Probabilistic Transformer: A Probabilistic Dependency Model for Contextual Word Representation,"[arxiv.Result.Author('Haoyi Wu'), arxiv.Result.Author('Kewei Tu')]","Syntactic structures used to play a vital role in natural language processing
(NLP), but since the deep learning revolution, NLP has been gradually dominated
by neural models that do not consider syntactic structures in their design. One
vastly successful class of neural models is transformers. When used as an
encoder, a transformer produces contextual representation of words in the input
sentence. In this work, we propose a new model of contextual word
representation, not from a neural perspective, but from a purely syntactic and
probabilistic perspective. Specifically, we design a conditional random field
that models discrete latent representations of all words in a sentence as well
as dependency arcs between them; and we use mean field variational inference
for approximate inference. Strikingly, we find that the computation graph of
our model resembles transformers, with correspondences between dependencies and
self-attention and between distributions over latent representations and
contextual embeddings of words. Experiments show that our model performs
competitively to transformers on small to medium sized datasets. We hope that
our work could help bridge the gap between traditional syntactic and
probabilistic approaches and cutting-edge neural approaches to NLP, and inspire
more linguistically-principled neural approaches in the future.",Accepted to ACL2023 Findings,,10.18653/v1/2023.findings-acl.482,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://dx.doi.org/10.18653/v1/2023.findings-acl.482', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2311.15211v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2311.15211v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2311.15211v1,"{'id': 'http://arxiv.org/abs/2311.15211v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2311.15211v1', 'updated': '2023-11-26T06:56:02Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=26, tm_hour=6, tm_min=56, tm_sec=2, tm_wday=6, tm_yday=330, tm_isdst=0), 'published': '2023-11-26T06:56:02Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=26, tm_hour=6, tm_min=56, tm_sec=2, tm_wday=6, tm_yday=330, tm_isdst=0), 'title': 'Probabilistic Transformer: A Probabilistic Dependency Model for\n  Contextual Word Representation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Probabilistic Transformer: A Probabilistic Dependency Model for\n  Contextual Word Representation'}, 'summary': 'Syntactic structures used to play a vital role in natural language processing\n(NLP), but since the deep learning revolution, NLP has been gradually dominated\nby neural models that do not consider syntactic structures in their design. One\nvastly successful class of neural models is transformers. When used as an\nencoder, a transformer produces contextual representation of words in the input\nsentence. In this work, we propose a new model of contextual word\nrepresentation, not from a neural perspective, but from a purely syntactic and\nprobabilistic perspective. Specifically, we design a conditional random field\nthat models discrete latent representations of all words in a sentence as well\nas dependency arcs between them; and we use mean field variational inference\nfor approximate inference. Strikingly, we find that the computation graph of\nour model resembles transformers, with correspondences between dependencies and\nself-attention and between distributions over latent representations and\ncontextual embeddings of words. Experiments show that our model performs\ncompetitively to transformers on small to medium sized datasets. We hope that\nour work could help bridge the gap between traditional syntactic and\nprobabilistic approaches and cutting-edge neural approaches to NLP, and inspire\nmore linguistically-principled neural approaches in the future.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Syntactic structures used to play a vital role in natural language processing\n(NLP), but since the deep learning revolution, NLP has been gradually dominated\nby neural models that do not consider syntactic structures in their design. One\nvastly successful class of neural models is transformers. When used as an\nencoder, a transformer produces contextual representation of words in the input\nsentence. In this work, we propose a new model of contextual word\nrepresentation, not from a neural perspective, but from a purely syntactic and\nprobabilistic perspective. Specifically, we design a conditional random field\nthat models discrete latent representations of all words in a sentence as well\nas dependency arcs between them; and we use mean field variational inference\nfor approximate inference. Strikingly, we find that the computation graph of\nour model resembles transformers, with correspondences between dependencies and\nself-attention and between distributions over latent representations and\ncontextual embeddings of words. Experiments show that our model performs\ncompetitively to transformers on small to medium sized datasets. We hope that\nour work could help bridge the gap between traditional syntactic and\nprobabilistic approaches and cutting-edge neural approaches to NLP, and inspire\nmore linguistically-principled neural approaches in the future.'}, 'authors': [{'name': 'Haoyi Wu'}, {'name': 'Kewei Tu'}], 'author_detail': {'name': 'Kewei Tu'}, 'author': 'Kewei Tu', 'arxiv_doi': '10.18653/v1/2023.findings-acl.482', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.18653/v1/2023.findings-acl.482', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2311.15211v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.15211v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': 'Accepted to ACL2023 Findings', 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2311.15941v1,2023-11-27 15:49:29+00:00,2023-11-27 15:49:29+00:00,Tell2Design: A Dataset for Language-Guided Floor Plan Generation,"[arxiv.Result.Author('Sicong Leng'), arxiv.Result.Author('Yang Zhou'), arxiv.Result.Author('Mohammed Haroon Dupty'), arxiv.Result.Author('Wee Sun Lee'), arxiv.Result.Author('Sam Conrad Joyce'), arxiv.Result.Author('Wei Lu')]","We consider the task of generating designs directly from natural language
descriptions, and consider floor plan generation as the initial research area.
Language conditional generative models have recently been very successful in
generating high-quality artistic images. However, designs must satisfy
different constraints that are not present in generating artistic images,
particularly spatial and relational constraints. We make multiple contributions
to initiate research on this task. First, we introduce a novel dataset,
\textit{Tell2Design} (T2D), which contains more than $80k$ floor plan designs
associated with natural language instructions. Second, we propose a
Sequence-to-Sequence model that can serve as a strong baseline for future
research. Third, we benchmark this task with several text-conditional image
generation models. We conclude by conducting human evaluations on the generated
samples and providing an analysis of human performance. We hope our
contributions will propel the research on language-guided design generation
forward.",Paper published in ACL2023; Area Chair Award; Best Paper Nomination,,,cs.CL,"['cs.CL', 'cs.CV']","[arxiv.Result.Link('http://arxiv.org/abs/2311.15941v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2311.15941v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2311.15941v1,"{'id': 'http://arxiv.org/abs/2311.15941v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2311.15941v1', 'updated': '2023-11-27T15:49:29Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=27, tm_hour=15, tm_min=49, tm_sec=29, tm_wday=0, tm_yday=331, tm_isdst=0), 'published': '2023-11-27T15:49:29Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=11, tm_mday=27, tm_hour=15, tm_min=49, tm_sec=29, tm_wday=0, tm_yday=331, tm_isdst=0), 'title': 'Tell2Design: A Dataset for Language-Guided Floor Plan Generation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Tell2Design: A Dataset for Language-Guided Floor Plan Generation'}, 'summary': 'We consider the task of generating designs directly from natural language\ndescriptions, and consider floor plan generation as the initial research area.\nLanguage conditional generative models have recently been very successful in\ngenerating high-quality artistic images. However, designs must satisfy\ndifferent constraints that are not present in generating artistic images,\nparticularly spatial and relational constraints. We make multiple contributions\nto initiate research on this task. First, we introduce a novel dataset,\n\\textit{Tell2Design} (T2D), which contains more than $80k$ floor plan designs\nassociated with natural language instructions. Second, we propose a\nSequence-to-Sequence model that can serve as a strong baseline for future\nresearch. Third, we benchmark this task with several text-conditional image\ngeneration models. We conclude by conducting human evaluations on the generated\nsamples and providing an analysis of human performance. We hope our\ncontributions will propel the research on language-guided design generation\nforward.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We consider the task of generating designs directly from natural language\ndescriptions, and consider floor plan generation as the initial research area.\nLanguage conditional generative models have recently been very successful in\ngenerating high-quality artistic images. However, designs must satisfy\ndifferent constraints that are not present in generating artistic images,\nparticularly spatial and relational constraints. We make multiple contributions\nto initiate research on this task. First, we introduce a novel dataset,\n\\textit{Tell2Design} (T2D), which contains more than $80k$ floor plan designs\nassociated with natural language instructions. Second, we propose a\nSequence-to-Sequence model that can serve as a strong baseline for future\nresearch. Third, we benchmark this task with several text-conditional image\ngeneration models. We conclude by conducting human evaluations on the generated\nsamples and providing an analysis of human performance. We hope our\ncontributions will propel the research on language-guided design generation\nforward.'}, 'authors': [{'name': 'Sicong Leng'}, {'name': 'Yang Zhou'}, {'name': 'Mohammed Haroon Dupty'}, {'name': 'Wee Sun Lee'}, {'name': 'Sam Conrad Joyce'}, {'name': 'Wei Lu'}], 'author_detail': {'name': 'Wei Lu'}, 'author': 'Wei Lu', 'arxiv_comment': 'Paper published in ACL2023; Area Chair Award; Best Paper Nomination', 'links': [{'href': 'http://arxiv.org/abs/2311.15941v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2311.15941v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2209.13359v2,2023-05-25 08:50:45+00:00,2022-09-26 08:11:19+00:00,Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding,"[arxiv.Result.Author('Erica K. Shimomoto'), arxiv.Result.Author('Edison Marrese-Taylor'), arxiv.Result.Author('Hiroya Takamura'), arxiv.Result.Author('Ichiro Kobayashi'), arxiv.Result.Author('Hideki Nakayama'), arxiv.Result.Author('Yusuke Miyao')]","This paper explores the task of Temporal Video Grounding (TVG) where, given
an untrimmed video and a natural language sentence query, the goal is to
recognize and determine temporal boundaries of action instances in the video
described by the query. Recent works tackled this task by improving query
inputs with large pre-trained language models (PLM) at the cost of more
expensive training. However, the effects of this integration are unclear, as
these works also propose improvements in the visual inputs. Therefore, this
paper studies the effects of PLMs in TVG and assesses the applicability of
parameter-efficient training with NLP adapters. We couple popular PLMs with a
selection of existing approaches and test different adapters to reduce the
impact of the additional parameters. Our results on three challenging datasets
show that, without changing the visual inputs, TVG models greatly benefited
from the PLM integration and fine-tuning, stressing the importance of sentence
query representation in this task. Furthermore, NLP adapters were an effective
alternative to full fine-tuning, even though they were not tailored to our
task, allowing PLM integration in larger TVG models and delivering results
comparable to SOTA models. Finally, our results shed light on which adapters
work best in different scenarios.",Accepted for Findings of ACL2023,,,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Link('http://arxiv.org/abs/2209.13359v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2209.13359v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2209.13359v2,"{'id': 'http://arxiv.org/abs/2209.13359v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2209.13359v2', 'updated': '2023-05-25T08:50:45Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=25, tm_hour=8, tm_min=50, tm_sec=45, tm_wday=3, tm_yday=145, tm_isdst=0), 'published': '2022-09-26T08:11:19Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=9, tm_mday=26, tm_hour=8, tm_min=11, tm_sec=19, tm_wday=0, tm_yday=269, tm_isdst=0), 'title': 'Towards Parameter-Efficient Integration of Pre-Trained Language Models\n  In Temporal Video Grounding', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Towards Parameter-Efficient Integration of Pre-Trained Language Models\n  In Temporal Video Grounding'}, 'summary': 'This paper explores the task of Temporal Video Grounding (TVG) where, given\nan untrimmed video and a natural language sentence query, the goal is to\nrecognize and determine temporal boundaries of action instances in the video\ndescribed by the query. Recent works tackled this task by improving query\ninputs with large pre-trained language models (PLM) at the cost of more\nexpensive training. However, the effects of this integration are unclear, as\nthese works also propose improvements in the visual inputs. Therefore, this\npaper studies the effects of PLMs in TVG and assesses the applicability of\nparameter-efficient training with NLP adapters. We couple popular PLMs with a\nselection of existing approaches and test different adapters to reduce the\nimpact of the additional parameters. Our results on three challenging datasets\nshow that, without changing the visual inputs, TVG models greatly benefited\nfrom the PLM integration and fine-tuning, stressing the importance of sentence\nquery representation in this task. Furthermore, NLP adapters were an effective\nalternative to full fine-tuning, even though they were not tailored to our\ntask, allowing PLM integration in larger TVG models and delivering results\ncomparable to SOTA models. Finally, our results shed light on which adapters\nwork best in different scenarios.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'This paper explores the task of Temporal Video Grounding (TVG) where, given\nan untrimmed video and a natural language sentence query, the goal is to\nrecognize and determine temporal boundaries of action instances in the video\ndescribed by the query. Recent works tackled this task by improving query\ninputs with large pre-trained language models (PLM) at the cost of more\nexpensive training. However, the effects of this integration are unclear, as\nthese works also propose improvements in the visual inputs. Therefore, this\npaper studies the effects of PLMs in TVG and assesses the applicability of\nparameter-efficient training with NLP adapters. We couple popular PLMs with a\nselection of existing approaches and test different adapters to reduce the\nimpact of the additional parameters. Our results on three challenging datasets\nshow that, without changing the visual inputs, TVG models greatly benefited\nfrom the PLM integration and fine-tuning, stressing the importance of sentence\nquery representation in this task. Furthermore, NLP adapters were an effective\nalternative to full fine-tuning, even though they were not tailored to our\ntask, allowing PLM integration in larger TVG models and delivering results\ncomparable to SOTA models. Finally, our results shed light on which adapters\nwork best in different scenarios.'}, 'authors': [{'name': 'Erica K. Shimomoto'}, {'name': 'Edison Marrese-Taylor'}, {'name': 'Hiroya Takamura'}, {'name': 'Ichiro Kobayashi'}, {'name': 'Hideki Nakayama'}, {'name': 'Yusuke Miyao'}], 'author_detail': {'name': 'Yusuke Miyao'}, 'author': 'Yusuke Miyao', 'arxiv_comment': 'Accepted for Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2209.13359v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2209.13359v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2209.14389v2,2023-05-26 13:46:47+00:00,2022-09-28 19:28:43+00:00,Downstream Datasets Make Surprisingly Good Pretraining Corpora,"[arxiv.Result.Author('Kundan Krishna'), arxiv.Result.Author('Saurabh Garg'), arxiv.Result.Author('Jeffrey P. Bigham'), arxiv.Result.Author('Zachary C. Lipton')]","For most natural language processing tasks, the dominant practice is to
finetune large pretrained transformer models (e.g., BERT) using smaller
downstream datasets. Despite the success of this approach, it remains unclear
to what extent these gains are attributable to the massive background corpora
employed for pretraining versus to the pretraining objectives themselves. This
paper introduces a large-scale study of self-pretraining, where the same
(downstream) training data is used for both pretraining and finetuning. In
experiments addressing both ELECTRA and RoBERTa models and 10 distinct
downstream classification datasets, we observe that self-pretraining rivals
standard pretraining on the BookWiki corpus (despite using around
$10\times$--$500\times$ less data), outperforming the latter on $7$ and $5$
datasets, respectively. Surprisingly, these task-specific pretrained models
often perform well on other tasks, including the GLUE benchmark. Besides
classification tasks, self-pretraining also provides benefits on structured
output prediction tasks such as span based question answering and commonsense
inference, often providing more than $50\%$ of the performance boosts provided
by pretraining on the BookWiki corpus. Our results hint that in many scenarios,
performance gains attributable to pretraining are driven primarily by the
pretraining objective itself and are not always attributable to the use of
external pretraining data in massive amounts. These findings are especially
relevant in light of concerns about intellectual property and offensive content
in web-scale pretraining data.",ACL2023 Camera Ready,,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2209.14389v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2209.14389v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2209.14389v2,"{'id': 'http://arxiv.org/abs/2209.14389v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2209.14389v2', 'updated': '2023-05-26T13:46:47Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=13, tm_min=46, tm_sec=47, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2022-09-28T19:28:43Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=9, tm_mday=28, tm_hour=19, tm_min=28, tm_sec=43, tm_wday=2, tm_yday=271, tm_isdst=0), 'title': 'Downstream Datasets Make Surprisingly Good Pretraining Corpora', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Downstream Datasets Make Surprisingly Good Pretraining Corpora'}, 'summary': 'For most natural language processing tasks, the dominant practice is to\nfinetune large pretrained transformer models (e.g., BERT) using smaller\ndownstream datasets. Despite the success of this approach, it remains unclear\nto what extent these gains are attributable to the massive background corpora\nemployed for pretraining versus to the pretraining objectives themselves. This\npaper introduces a large-scale study of self-pretraining, where the same\n(downstream) training data is used for both pretraining and finetuning. In\nexperiments addressing both ELECTRA and RoBERTa models and 10 distinct\ndownstream classification datasets, we observe that self-pretraining rivals\nstandard pretraining on the BookWiki corpus (despite using around\n$10\\times$--$500\\times$ less data), outperforming the latter on $7$ and $5$\ndatasets, respectively. Surprisingly, these task-specific pretrained models\noften perform well on other tasks, including the GLUE benchmark. Besides\nclassification tasks, self-pretraining also provides benefits on structured\noutput prediction tasks such as span based question answering and commonsense\ninference, often providing more than $50\\%$ of the performance boosts provided\nby pretraining on the BookWiki corpus. Our results hint that in many scenarios,\nperformance gains attributable to pretraining are driven primarily by the\npretraining objective itself and are not always attributable to the use of\nexternal pretraining data in massive amounts. These findings are especially\nrelevant in light of concerns about intellectual property and offensive content\nin web-scale pretraining data.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'For most natural language processing tasks, the dominant practice is to\nfinetune large pretrained transformer models (e.g., BERT) using smaller\ndownstream datasets. Despite the success of this approach, it remains unclear\nto what extent these gains are attributable to the massive background corpora\nemployed for pretraining versus to the pretraining objectives themselves. This\npaper introduces a large-scale study of self-pretraining, where the same\n(downstream) training data is used for both pretraining and finetuning. In\nexperiments addressing both ELECTRA and RoBERTa models and 10 distinct\ndownstream classification datasets, we observe that self-pretraining rivals\nstandard pretraining on the BookWiki corpus (despite using around\n$10\\times$--$500\\times$ less data), outperforming the latter on $7$ and $5$\ndatasets, respectively. Surprisingly, these task-specific pretrained models\noften perform well on other tasks, including the GLUE benchmark. Besides\nclassification tasks, self-pretraining also provides benefits on structured\noutput prediction tasks such as span based question answering and commonsense\ninference, often providing more than $50\\%$ of the performance boosts provided\nby pretraining on the BookWiki corpus. Our results hint that in many scenarios,\nperformance gains attributable to pretraining are driven primarily by the\npretraining objective itself and are not always attributable to the use of\nexternal pretraining data in massive amounts. These findings are especially\nrelevant in light of concerns about intellectual property and offensive content\nin web-scale pretraining data.'}, 'authors': [{'name': 'Kundan Krishna'}, {'name': 'Saurabh Garg'}, {'name': 'Jeffrey P. Bigham'}, {'name': 'Zachary C. Lipton'}], 'author_detail': {'name': 'Zachary C. Lipton'}, 'author': 'Zachary C. Lipton', 'arxiv_comment': 'ACL2023 Camera Ready', 'links': [{'href': 'http://arxiv.org/abs/2209.14389v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2209.14389v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2210.08303v2,2023-12-27 05:52:24+00:00,2022-10-15 14:05:03+00:00,Improving Radiology Summarization with Radiograph and Anatomy Prompts,"[arxiv.Result.Author('Jinpeng Hu'), arxiv.Result.Author('Zhihong Chen'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Xiang Wan'), arxiv.Result.Author('Tsung-Hui Chang')]","The impression is crucial for the referring physicians to grasp key
information since it is concluded from the findings and reasoning of
radiologists. To alleviate the workload of radiologists and reduce repetitive
human labor in impression writing, many researchers have focused on automatic
impression generation. However, recent works on this task mainly summarize the
corresponding findings and pay less attention to the radiology images. In
clinical, radiographs can provide more detailed valuable observations to
enhance radiologists' impression writing, especially for complicated cases.
Besides, each sentence in findings usually focuses on single anatomy, so they
only need to be matched to corresponding anatomical regions instead of the
whole image, which is beneficial for textual and visual features alignment.
Therefore, we propose a novel anatomy-enhanced multimodal model to promote
impression generation. In detail, we first construct a set of rules to extract
anatomies and put these prompts into each sentence to highlight anatomy
characteristics. Then, two separate encoders are applied to extract features
from the radiograph and findings. Afterward, we utilize a contrastive learning
module to align these two representations at the overall level and use a
co-attention to fuse them at the sentence level with the help of
anatomy-enhanced sentence representation. Finally, the decoder takes the fused
information as the input to generate impressions. The experimental results on
two benchmark datasets confirm the effectiveness of the proposed method, which
achieves state-of-the-art results.","11 pages, ACL2023 Findings",,,cs.CV,"['cs.CV', 'cs.AI', 'cs.CL']","[arxiv.Result.Link('http://arxiv.org/abs/2210.08303v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2210.08303v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2210.08303v2,"{'id': 'http://arxiv.org/abs/2210.08303v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2210.08303v2', 'updated': '2023-12-27T05:52:24Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=12, tm_mday=27, tm_hour=5, tm_min=52, tm_sec=24, tm_wday=2, tm_yday=361, tm_isdst=0), 'published': '2022-10-15T14:05:03Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=10, tm_mday=15, tm_hour=14, tm_min=5, tm_sec=3, tm_wday=5, tm_yday=288, tm_isdst=0), 'title': 'Improving Radiology Summarization with Radiograph and Anatomy Prompts', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Improving Radiology Summarization with Radiograph and Anatomy Prompts'}, 'summary': ""The impression is crucial for the referring physicians to grasp key\ninformation since it is concluded from the findings and reasoning of\nradiologists. To alleviate the workload of radiologists and reduce repetitive\nhuman labor in impression writing, many researchers have focused on automatic\nimpression generation. However, recent works on this task mainly summarize the\ncorresponding findings and pay less attention to the radiology images. In\nclinical, radiographs can provide more detailed valuable observations to\nenhance radiologists' impression writing, especially for complicated cases.\nBesides, each sentence in findings usually focuses on single anatomy, so they\nonly need to be matched to corresponding anatomical regions instead of the\nwhole image, which is beneficial for textual and visual features alignment.\nTherefore, we propose a novel anatomy-enhanced multimodal model to promote\nimpression generation. In detail, we first construct a set of rules to extract\nanatomies and put these prompts into each sentence to highlight anatomy\ncharacteristics. Then, two separate encoders are applied to extract features\nfrom the radiograph and findings. Afterward, we utilize a contrastive learning\nmodule to align these two representations at the overall level and use a\nco-attention to fuse them at the sentence level with the help of\nanatomy-enhanced sentence representation. Finally, the decoder takes the fused\ninformation as the input to generate impressions. The experimental results on\ntwo benchmark datasets confirm the effectiveness of the proposed method, which\nachieves state-of-the-art results."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""The impression is crucial for the referring physicians to grasp key\ninformation since it is concluded from the findings and reasoning of\nradiologists. To alleviate the workload of radiologists and reduce repetitive\nhuman labor in impression writing, many researchers have focused on automatic\nimpression generation. However, recent works on this task mainly summarize the\ncorresponding findings and pay less attention to the radiology images. In\nclinical, radiographs can provide more detailed valuable observations to\nenhance radiologists' impression writing, especially for complicated cases.\nBesides, each sentence in findings usually focuses on single anatomy, so they\nonly need to be matched to corresponding anatomical regions instead of the\nwhole image, which is beneficial for textual and visual features alignment.\nTherefore, we propose a novel anatomy-enhanced multimodal model to promote\nimpression generation. In detail, we first construct a set of rules to extract\nanatomies and put these prompts into each sentence to highlight anatomy\ncharacteristics. Then, two separate encoders are applied to extract features\nfrom the radiograph and findings. Afterward, we utilize a contrastive learning\nmodule to align these two representations at the overall level and use a\nco-attention to fuse them at the sentence level with the help of\nanatomy-enhanced sentence representation. Finally, the decoder takes the fused\ninformation as the input to generate impressions. The experimental results on\ntwo benchmark datasets confirm the effectiveness of the proposed method, which\nachieves state-of-the-art results.""}, 'authors': [{'name': 'Jinpeng Hu'}, {'name': 'Zhihong Chen'}, {'name': 'Yang Liu'}, {'name': 'Xiang Wan'}, {'name': 'Tsung-Hui Chang'}], 'author_detail': {'name': 'Tsung-Hui Chang'}, 'author': 'Tsung-Hui Chang', 'arxiv_comment': '11 pages, ACL2023 Findings', 'links': [{'href': 'http://arxiv.org/abs/2210.08303v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2210.08303v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2211.09783v6,2023-05-27 19:28:00+00:00,2022-11-17 18:54:47+00:00,UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot Summarization,"[arxiv.Result.Author('Yulong Chen'), arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Ruochen Xu'), arxiv.Result.Author('Ziyi Yang'), arxiv.Result.Author('Chenguang Zhu'), arxiv.Result.Author('Michael Zeng'), arxiv.Result.Author('Yue Zhang')]","The high annotation costs and diverse demands of various summarization tasks
motivate the development of few-shot summarization. However, despite the
emergence of many summarization tasks and datasets, the current training
paradigm for few-shot summarization systems ignores potentially shareable
knowledge in heterogeneous datasets. To this end, we propose \textsc{UniSumm},
a unified few-shot summarization model pre-trained with multiple summarization
tasks and can be prefix-tuned to excel at any few-shot summarization task.
Meanwhile, to better evaluate few-shot summarizers, under the principles of
diversity and robustness, we assemble and release a new benchmark
\textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of
few-shot samples for each task, covering diverse domains. Experimental results
and analysis show that \textsc{UniSumm} outperforms strong baselines by a large
margin across all sub-tasks in \textsc{SummZoo} under both automatic and human
evaluations and achieves comparable results in human evaluation compared with a
GPT-3.5 model.",ACL2023 main conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2211.09783v6', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2211.09783v6', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2211.09783v6,"{'id': 'http://arxiv.org/abs/2211.09783v6', 'guidislink': True, 'link': 'http://arxiv.org/abs/2211.09783v6', 'updated': '2023-05-27T19:28:00Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=27, tm_hour=19, tm_min=28, tm_sec=0, tm_wday=5, tm_yday=147, tm_isdst=0), 'published': '2022-11-17T18:54:47Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=11, tm_mday=17, tm_hour=18, tm_min=54, tm_sec=47, tm_wday=3, tm_yday=321, tm_isdst=0), 'title': 'UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot\n  Summarization', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'UniSumm and SummZoo: Unified Model and Diverse Benchmark for Few-Shot\n  Summarization'}, 'summary': 'The high annotation costs and diverse demands of various summarization tasks\nmotivate the development of few-shot summarization. However, despite the\nemergence of many summarization tasks and datasets, the current training\nparadigm for few-shot summarization systems ignores potentially shareable\nknowledge in heterogeneous datasets. To this end, we propose \\textsc{UniSumm},\na unified few-shot summarization model pre-trained with multiple summarization\ntasks and can be prefix-tuned to excel at any few-shot summarization task.\nMeanwhile, to better evaluate few-shot summarizers, under the principles of\ndiversity and robustness, we assemble and release a new benchmark\n\\textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of\nfew-shot samples for each task, covering diverse domains. Experimental results\nand analysis show that \\textsc{UniSumm} outperforms strong baselines by a large\nmargin across all sub-tasks in \\textsc{SummZoo} under both automatic and human\nevaluations and achieves comparable results in human evaluation compared with a\nGPT-3.5 model.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The high annotation costs and diverse demands of various summarization tasks\nmotivate the development of few-shot summarization. However, despite the\nemergence of many summarization tasks and datasets, the current training\nparadigm for few-shot summarization systems ignores potentially shareable\nknowledge in heterogeneous datasets. To this end, we propose \\textsc{UniSumm},\na unified few-shot summarization model pre-trained with multiple summarization\ntasks and can be prefix-tuned to excel at any few-shot summarization task.\nMeanwhile, to better evaluate few-shot summarizers, under the principles of\ndiversity and robustness, we assemble and release a new benchmark\n\\textsc{SummZoo}. It consists of $8$ summarization tasks with multiple sets of\nfew-shot samples for each task, covering diverse domains. Experimental results\nand analysis show that \\textsc{UniSumm} outperforms strong baselines by a large\nmargin across all sub-tasks in \\textsc{SummZoo} under both automatic and human\nevaluations and achieves comparable results in human evaluation compared with a\nGPT-3.5 model.'}, 'authors': [{'name': 'Yulong Chen'}, {'name': 'Yang Liu'}, {'name': 'Ruochen Xu'}, {'name': 'Ziyi Yang'}, {'name': 'Chenguang Zhu'}, {'name': 'Michael Zeng'}, {'name': 'Yue Zhang'}], 'author_detail': {'name': 'Yue Zhang'}, 'author': 'Yue Zhang', 'arxiv_comment': 'ACL2023 main conference', 'links': [{'href': 'http://arxiv.org/abs/2211.09783v6', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2211.09783v6', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2212.09741v3,2023-05-30 15:22:50+00:00,2022-12-19 18:57:05+00:00,"One Embedder, Any Task: Instruction-Finetuned Text Embeddings","[arxiv.Result.Author('Hongjin Su'), arxiv.Result.Author('Weijia Shi'), arxiv.Result.Author('Jungo Kasai'), arxiv.Result.Author('Yizhong Wang'), arxiv.Result.Author('Yushi Hu'), arxiv.Result.Author('Mari Ostendorf'), arxiv.Result.Author('Wen-tau Yih'), arxiv.Result.Author('Noah A. Smith'), arxiv.Result.Author('Luke Zettlemoyer'), arxiv.Result.Author('Tao Yu')]","We introduce INSTRUCTOR, a new method for computing text embeddings given
task instructions: every text input is embedded together with instructions
explaining the use case (e.g., task and domain descriptions). Unlike encoders
from prior work that are more specialized, INSTRUCTOR is a single embedder that
can generate text embeddings tailored to different downstream tasks and
domains, without any further training. We first annotate instructions for 330
diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive
loss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are
unseen during training), ranging from classification and information retrieval
to semantic textual similarity and text generation evaluation. INSTRUCTOR,
while having an order of magnitude fewer parameters than the previous best
model, achieves state-of-the-art performance, with an average improvement of
3.4% compared to the previous best results on the 70 diverse datasets. Our
analysis suggests that INSTRUCTOR is robust to changes in instructions, and
that instruction finetuning mitigates the challenge of training a single model
on diverse datasets. Our model, code, and data are available at
https://instructor-embedding.github.io.",Accepted in ACL2023 Findings,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2212.09741v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.09741v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2212.09741v3,"{'id': 'http://arxiv.org/abs/2212.09741v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2212.09741v3', 'updated': '2023-05-30T15:22:50Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=30, tm_hour=15, tm_min=22, tm_sec=50, tm_wday=1, tm_yday=150, tm_isdst=0), 'published': '2022-12-19T18:57:05Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=12, tm_mday=19, tm_hour=18, tm_min=57, tm_sec=5, tm_wday=0, tm_yday=353, tm_isdst=0), 'title': 'One Embedder, Any Task: Instruction-Finetuned Text Embeddings', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'One Embedder, Any Task: Instruction-Finetuned Text Embeddings'}, 'summary': 'We introduce INSTRUCTOR, a new method for computing text embeddings given\ntask instructions: every text input is embedded together with instructions\nexplaining the use case (e.g., task and domain descriptions). Unlike encoders\nfrom prior work that are more specialized, INSTRUCTOR is a single embedder that\ncan generate text embeddings tailored to different downstream tasks and\ndomains, without any further training. We first annotate instructions for 330\ndiverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive\nloss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are\nunseen during training), ranging from classification and information retrieval\nto semantic textual similarity and text generation evaluation. INSTRUCTOR,\nwhile having an order of magnitude fewer parameters than the previous best\nmodel, achieves state-of-the-art performance, with an average improvement of\n3.4% compared to the previous best results on the 70 diverse datasets. Our\nanalysis suggests that INSTRUCTOR is robust to changes in instructions, and\nthat instruction finetuning mitigates the challenge of training a single model\non diverse datasets. Our model, code, and data are available at\nhttps://instructor-embedding.github.io.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We introduce INSTRUCTOR, a new method for computing text embeddings given\ntask instructions: every text input is embedded together with instructions\nexplaining the use case (e.g., task and domain descriptions). Unlike encoders\nfrom prior work that are more specialized, INSTRUCTOR is a single embedder that\ncan generate text embeddings tailored to different downstream tasks and\ndomains, without any further training. We first annotate instructions for 330\ndiverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive\nloss. We evaluate INSTRUCTOR on 70 embedding evaluation tasks (66 of which are\nunseen during training), ranging from classification and information retrieval\nto semantic textual similarity and text generation evaluation. INSTRUCTOR,\nwhile having an order of magnitude fewer parameters than the previous best\nmodel, achieves state-of-the-art performance, with an average improvement of\n3.4% compared to the previous best results on the 70 diverse datasets. Our\nanalysis suggests that INSTRUCTOR is robust to changes in instructions, and\nthat instruction finetuning mitigates the challenge of training a single model\non diverse datasets. Our model, code, and data are available at\nhttps://instructor-embedding.github.io.'}, 'authors': [{'name': 'Hongjin Su'}, {'name': 'Weijia Shi'}, {'name': 'Jungo Kasai'}, {'name': 'Yizhong Wang'}, {'name': 'Yushi Hu'}, {'name': 'Mari Ostendorf'}, {'name': 'Wen-tau Yih'}, {'name': 'Noah A. Smith'}, {'name': 'Luke Zettlemoyer'}, {'name': 'Tao Yu'}], 'author_detail': {'name': 'Tao Yu'}, 'author': 'Tao Yu', 'arxiv_comment': 'Accepted in ACL2023 Findings', 'links': [{'href': 'http://arxiv.org/abs/2212.09741v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2212.09741v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2212.09864v2,2023-05-31 01:34:54+00:00,2022-12-19 21:34:00+00:00,Synthetic Pre-Training Tasks for Neural Machine Translation,"[arxiv.Result.Author('Zexue He'), arxiv.Result.Author('Graeme Blackwood'), arxiv.Result.Author('Rameswar Panda'), arxiv.Result.Author('Julian McAuley'), arxiv.Result.Author('Rogerio Feris')]","Pre-training models with large crawled corpora can lead to issues such as
toxicity and bias, as well as copyright and privacy concerns. A promising way
of alleviating such concerns is to conduct pre-training with synthetic tasks
and data, since no real-world information is ingested by the model. Our goal in
this paper is to understand the factors that contribute to the effectiveness of
pre-training models when using synthetic resources, particularly in the context
of neural machine translation. We propose several novel approaches to
pre-training translation models that involve different levels of lexical and
structural knowledge, including: 1) generating obfuscated data from a large
parallel corpus 2) concatenating phrase pairs extracted from a small
word-aligned corpus, and 3) generating synthetic parallel data without real
human language corpora. Our experiments on multiple language pairs reveal that
pre-training benefits can be realized even with high levels of obfuscation or
purely synthetic parallel data. We hope the findings from our comprehensive
empirical analysis will shed light on understanding what matters for NMT
pre-training, as well as pave the way for the development of more efficient and
less toxic models.","Accepted to ACL2023-Findings. New added Phrase-cat for synthetic
  pre-training. 17 pages including 5-page appendix",,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2212.09864v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2212.09864v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2212.09864v2,"{'id': 'http://arxiv.org/abs/2212.09864v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2212.09864v2', 'updated': '2023-05-31T01:34:54Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=31, tm_hour=1, tm_min=34, tm_sec=54, tm_wday=2, tm_yday=151, tm_isdst=0), 'published': '2022-12-19T21:34:00Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=12, tm_mday=19, tm_hour=21, tm_min=34, tm_sec=0, tm_wday=0, tm_yday=353, tm_isdst=0), 'title': 'Synthetic Pre-Training Tasks for Neural Machine Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Synthetic Pre-Training Tasks for Neural Machine Translation'}, 'summary': 'Pre-training models with large crawled corpora can lead to issues such as\ntoxicity and bias, as well as copyright and privacy concerns. A promising way\nof alleviating such concerns is to conduct pre-training with synthetic tasks\nand data, since no real-world information is ingested by the model. Our goal in\nthis paper is to understand the factors that contribute to the effectiveness of\npre-training models when using synthetic resources, particularly in the context\nof neural machine translation. We propose several novel approaches to\npre-training translation models that involve different levels of lexical and\nstructural knowledge, including: 1) generating obfuscated data from a large\nparallel corpus 2) concatenating phrase pairs extracted from a small\nword-aligned corpus, and 3) generating synthetic parallel data without real\nhuman language corpora. Our experiments on multiple language pairs reveal that\npre-training benefits can be realized even with high levels of obfuscation or\npurely synthetic parallel data. We hope the findings from our comprehensive\nempirical analysis will shed light on understanding what matters for NMT\npre-training, as well as pave the way for the development of more efficient and\nless toxic models.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Pre-training models with large crawled corpora can lead to issues such as\ntoxicity and bias, as well as copyright and privacy concerns. A promising way\nof alleviating such concerns is to conduct pre-training with synthetic tasks\nand data, since no real-world information is ingested by the model. Our goal in\nthis paper is to understand the factors that contribute to the effectiveness of\npre-training models when using synthetic resources, particularly in the context\nof neural machine translation. We propose several novel approaches to\npre-training translation models that involve different levels of lexical and\nstructural knowledge, including: 1) generating obfuscated data from a large\nparallel corpus 2) concatenating phrase pairs extracted from a small\nword-aligned corpus, and 3) generating synthetic parallel data without real\nhuman language corpora. Our experiments on multiple language pairs reveal that\npre-training benefits can be realized even with high levels of obfuscation or\npurely synthetic parallel data. We hope the findings from our comprehensive\nempirical analysis will shed light on understanding what matters for NMT\npre-training, as well as pave the way for the development of more efficient and\nless toxic models.'}, 'authors': [{'name': 'Zexue He'}, {'name': 'Graeme Blackwood'}, {'name': 'Rameswar Panda'}, {'name': 'Julian McAuley'}, {'name': 'Rogerio Feris'}], 'author_detail': {'name': 'Rogerio Feris'}, 'author': 'Rogerio Feris', 'arxiv_comment': 'Accepted to ACL2023-Findings. New added Phrase-cat for synthetic\n  pre-training. 17 pages including 5-page appendix', 'links': [{'href': 'http://arxiv.org/abs/2212.09864v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2212.09864v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.02468v3,2023-05-31 06:18:28+00:00,2023-05-04 00:17:49+00:00,Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System,"[arxiv.Result.Author('Namo Bang'), arxiv.Result.Author('Jeehyun Lee'), arxiv.Result.Author('Myoung-Wan Koo')]","Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks
by tracking dialogue states and generating appropriate responses to help users
achieve defined goals. Recently, end-to-end dialogue models pre-trained based
on large datasets have shown promising performance in the conversational
system. However, they share the same parameters to train tasks of the dialogue
system (NLU, DST, NLG), so debugging each task is challenging. Also, they
require a lot of effort to fine-tune large parameters to create a task-oriented
chatbot, making it difficult for non-experts to handle. Therefore, we intend to
train relatively lightweight and fast models compared to PLM. In this paper, we
propose an End-to-end TOD system with Task-Optimized Adapters which learn
independently per task, adding only small number of parameters after fixed
layers of pre-trained network. We also enhance the performance of the DST and
NLG modules through reinforcement learning, overcoming the learning curve that
has lacked at the adapter learning and enabling the natural and consistent
response generation that is appropriate for the goal. Our method is a
model-agnostic approach and does not require prompt-tuning as only input data
without a prompt. As results of the experiment, our method shows competitive
performance on the MultiWOZ benchmark compared to the existing end-to-end
models. In particular, we attain state-of-the-art performance on the DST task
of 2.2 dataset.",Accepted to Findings of ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.02468v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.02468v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.02468v3,"{'id': 'http://arxiv.org/abs/2305.02468v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.02468v3', 'updated': '2023-05-31T06:18:28Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=31, tm_hour=6, tm_min=18, tm_sec=28, tm_wday=2, tm_yday=151, tm_isdst=0), 'published': '2023-05-04T00:17:49Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=4, tm_hour=0, tm_min=17, tm_sec=49, tm_wday=3, tm_yday=124, tm_isdst=0), 'title': 'Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System'}, 'summary': 'Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks\nby tracking dialogue states and generating appropriate responses to help users\nachieve defined goals. Recently, end-to-end dialogue models pre-trained based\non large datasets have shown promising performance in the conversational\nsystem. However, they share the same parameters to train tasks of the dialogue\nsystem (NLU, DST, NLG), so debugging each task is challenging. Also, they\nrequire a lot of effort to fine-tune large parameters to create a task-oriented\nchatbot, making it difficult for non-experts to handle. Therefore, we intend to\ntrain relatively lightweight and fast models compared to PLM. In this paper, we\npropose an End-to-end TOD system with Task-Optimized Adapters which learn\nindependently per task, adding only small number of parameters after fixed\nlayers of pre-trained network. We also enhance the performance of the DST and\nNLG modules through reinforcement learning, overcoming the learning curve that\nhas lacked at the adapter learning and enabling the natural and consistent\nresponse generation that is appropriate for the goal. Our method is a\nmodel-agnostic approach and does not require prompt-tuning as only input data\nwithout a prompt. As results of the experiment, our method shows competitive\nperformance on the MultiWOZ benchmark compared to the existing end-to-end\nmodels. In particular, we attain state-of-the-art performance on the DST task\nof 2.2 dataset.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks\nby tracking dialogue states and generating appropriate responses to help users\nachieve defined goals. Recently, end-to-end dialogue models pre-trained based\non large datasets have shown promising performance in the conversational\nsystem. However, they share the same parameters to train tasks of the dialogue\nsystem (NLU, DST, NLG), so debugging each task is challenging. Also, they\nrequire a lot of effort to fine-tune large parameters to create a task-oriented\nchatbot, making it difficult for non-experts to handle. Therefore, we intend to\ntrain relatively lightweight and fast models compared to PLM. In this paper, we\npropose an End-to-end TOD system with Task-Optimized Adapters which learn\nindependently per task, adding only small number of parameters after fixed\nlayers of pre-trained network. We also enhance the performance of the DST and\nNLG modules through reinforcement learning, overcoming the learning curve that\nhas lacked at the adapter learning and enabling the natural and consistent\nresponse generation that is appropriate for the goal. Our method is a\nmodel-agnostic approach and does not require prompt-tuning as only input data\nwithout a prompt. As results of the experiment, our method shows competitive\nperformance on the MultiWOZ benchmark compared to the existing end-to-end\nmodels. In particular, we attain state-of-the-art performance on the DST task\nof 2.2 dataset.'}, 'authors': [{'name': 'Namo Bang'}, {'name': 'Jeehyun Lee'}, {'name': 'Myoung-Wan Koo'}], 'author_detail': {'name': 'Myoung-Wan Koo'}, 'author': 'Myoung-Wan Koo', 'arxiv_comment': 'Accepted to Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.02468v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.02468v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.04087v5,2023-09-11 06:27:53+00:00,2023-05-06 16:12:19+00:00,Self-Edit: Fault-Aware Code Editor for Code Generation,"[arxiv.Result.Author('Kechi Zhang'), arxiv.Result.Author('Zhuo Li'), arxiv.Result.Author('Jia Li'), arxiv.Result.Author('Ge Li'), arxiv.Result.Author('Zhi Jin')]","Large language models (LLMs) have demonstrated an impressive ability to
generate codes on competitive programming tasks. However, with limited sample
numbers, LLMs still suffer from poor accuracy. Inspired by the process of human
programming, we propose a generate-and-edit approach named Self-Edit that
utilizes execution results of the generated code from LLMs to improve the code
quality on the competitive programming task. We execute the generated code on
the example test case provided in the question and wrap execution results into
a supplementary comment. Utilizing this comment as guidance, our fault-aware
code editor is employed to correct errors in the generated code. We perform
extensive evaluations across two competitive programming datasets with nine
different LLMs. Compared to directly generating from LLMs, our approach can
improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\%
on HumanEval over nine popular code generation LLMs with parameter sizes
ranging from 110M to 175B. Compared to other post-processing methods, our
method demonstrates superior accuracy and efficiency.",Accepted by ACL2023,,,cs.SE,"['cs.SE', 'cs.CL']","[arxiv.Result.Link('http://arxiv.org/abs/2305.04087v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.04087v5', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.04087v5,"{'id': 'http://arxiv.org/abs/2305.04087v5', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.04087v5', 'updated': '2023-09-11T06:27:53Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=11, tm_hour=6, tm_min=27, tm_sec=53, tm_wday=0, tm_yday=254, tm_isdst=0), 'published': '2023-05-06T16:12:19Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=6, tm_hour=16, tm_min=12, tm_sec=19, tm_wday=5, tm_yday=126, tm_isdst=0), 'title': 'Self-Edit: Fault-Aware Code Editor for Code Generation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Self-Edit: Fault-Aware Code Editor for Code Generation'}, 'summary': 'Large language models (LLMs) have demonstrated an impressive ability to\ngenerate codes on competitive programming tasks. However, with limited sample\nnumbers, LLMs still suffer from poor accuracy. Inspired by the process of human\nprogramming, we propose a generate-and-edit approach named Self-Edit that\nutilizes execution results of the generated code from LLMs to improve the code\nquality on the competitive programming task. We execute the generated code on\nthe example test case provided in the question and wrap execution results into\na supplementary comment. Utilizing this comment as guidance, our fault-aware\ncode editor is employed to correct errors in the generated code. We perform\nextensive evaluations across two competitive programming datasets with nine\ndifferent LLMs. Compared to directly generating from LLMs, our approach can\nimprove the average of pass@1 by 89\\% on APPS-dev, 31\\% on APPS-test, and 48\\%\non HumanEval over nine popular code generation LLMs with parameter sizes\nranging from 110M to 175B. Compared to other post-processing methods, our\nmethod demonstrates superior accuracy and efficiency.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Large language models (LLMs) have demonstrated an impressive ability to\ngenerate codes on competitive programming tasks. However, with limited sample\nnumbers, LLMs still suffer from poor accuracy. Inspired by the process of human\nprogramming, we propose a generate-and-edit approach named Self-Edit that\nutilizes execution results of the generated code from LLMs to improve the code\nquality on the competitive programming task. We execute the generated code on\nthe example test case provided in the question and wrap execution results into\na supplementary comment. Utilizing this comment as guidance, our fault-aware\ncode editor is employed to correct errors in the generated code. We perform\nextensive evaluations across two competitive programming datasets with nine\ndifferent LLMs. Compared to directly generating from LLMs, our approach can\nimprove the average of pass@1 by 89\\% on APPS-dev, 31\\% on APPS-test, and 48\\%\non HumanEval over nine popular code generation LLMs with parameter sizes\nranging from 110M to 175B. Compared to other post-processing methods, our\nmethod demonstrates superior accuracy and efficiency.'}, 'authors': [{'name': 'Kechi Zhang'}, {'name': 'Zhuo Li'}, {'name': 'Jia Li'}, {'name': 'Ge Li'}, {'name': 'Zhi Jin'}], 'author_detail': {'name': 'Zhi Jin'}, 'author': 'Zhi Jin', 'arxiv_comment': 'Accepted by ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.04087v5', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.04087v5', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.SE', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.SE', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.06555v1,2023-05-11 04:19:08+00:00,2023-05-11 04:19:08+00:00,Domain Incremental Lifelong Learning in an Open World,"[arxiv.Result.Author('Yi Dai'), arxiv.Result.Author('Hao Lang'), arxiv.Result.Author('Yinhe Zheng'), arxiv.Result.Author('Bowen Yu'), arxiv.Result.Author('Fei Huang'), arxiv.Result.Author('Yongbin Li')]","Lifelong learning (LL) is an important ability for NLP models to learn new
tasks continuously. Architecture-based approaches are reported to be effective
implementations for LL models. However, it is non-trivial to extend previous
approaches to domain incremental LL scenarios since they either require access
to task identities in the testing phase or cannot handle samples from unseen
tasks. In this paper, we propose \textbf{Diana}: a
\underline{d}ynam\underline{i}c \underline{a}rchitecture-based
lifelo\underline{n}g le\underline{a}rning model that tries to learn a sequence
of tasks with a prompt-enhanced language model. Four types of hierarchically
organized prompts are used in Diana to capture knowledge from different
granularities. Specifically, we dedicate task-level prompts to capture
task-specific knowledge to retain high LL performances and maintain
instance-level prompts to learn knowledge shared across input samples to
improve the model's generalization performance. Moreover, we dedicate separate
prompts to explicitly model unseen tasks and introduce a set of prompt key
vectors to facilitate knowledge sharing between tasks. Extensive experiments
demonstrate that Diana outperforms state-of-the-art LL models, especially in
handling unseen tasks. We release the code and data at
\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/diana}.","ACL2023 Findings Long Paper. arXiv admin note: substantial text
  overlap with arXiv:2208.14602",,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.06555v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.06555v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.06555v1,"{'id': 'http://arxiv.org/abs/2305.06555v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.06555v1', 'updated': '2023-05-11T04:19:08Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=11, tm_hour=4, tm_min=19, tm_sec=8, tm_wday=3, tm_yday=131, tm_isdst=0), 'published': '2023-05-11T04:19:08Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=11, tm_hour=4, tm_min=19, tm_sec=8, tm_wday=3, tm_yday=131, tm_isdst=0), 'title': 'Domain Incremental Lifelong Learning in an Open World', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Domain Incremental Lifelong Learning in an Open World'}, 'summary': ""Lifelong learning (LL) is an important ability for NLP models to learn new\ntasks continuously. Architecture-based approaches are reported to be effective\nimplementations for LL models. However, it is non-trivial to extend previous\napproaches to domain incremental LL scenarios since they either require access\nto task identities in the testing phase or cannot handle samples from unseen\ntasks. In this paper, we propose \\textbf{Diana}: a\n\\underline{d}ynam\\underline{i}c \\underline{a}rchitecture-based\nlifelo\\underline{n}g le\\underline{a}rning model that tries to learn a sequence\nof tasks with a prompt-enhanced language model. Four types of hierarchically\norganized prompts are used in Diana to capture knowledge from different\ngranularities. Specifically, we dedicate task-level prompts to capture\ntask-specific knowledge to retain high LL performances and maintain\ninstance-level prompts to learn knowledge shared across input samples to\nimprove the model's generalization performance. Moreover, we dedicate separate\nprompts to explicitly model unseen tasks and introduce a set of prompt key\nvectors to facilitate knowledge sharing between tasks. Extensive experiments\ndemonstrate that Diana outperforms state-of-the-art LL models, especially in\nhandling unseen tasks. We release the code and data at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/diana}."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Lifelong learning (LL) is an important ability for NLP models to learn new\ntasks continuously. Architecture-based approaches are reported to be effective\nimplementations for LL models. However, it is non-trivial to extend previous\napproaches to domain incremental LL scenarios since they either require access\nto task identities in the testing phase or cannot handle samples from unseen\ntasks. In this paper, we propose \\textbf{Diana}: a\n\\underline{d}ynam\\underline{i}c \\underline{a}rchitecture-based\nlifelo\\underline{n}g le\\underline{a}rning model that tries to learn a sequence\nof tasks with a prompt-enhanced language model. Four types of hierarchically\norganized prompts are used in Diana to capture knowledge from different\ngranularities. Specifically, we dedicate task-level prompts to capture\ntask-specific knowledge to retain high LL performances and maintain\ninstance-level prompts to learn knowledge shared across input samples to\nimprove the model's generalization performance. Moreover, we dedicate separate\nprompts to explicitly model unseen tasks and introduce a set of prompt key\nvectors to facilitate knowledge sharing between tasks. Extensive experiments\ndemonstrate that Diana outperforms state-of-the-art LL models, especially in\nhandling unseen tasks. We release the code and data at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/diana}.""}, 'authors': [{'name': 'Yi Dai'}, {'name': 'Hao Lang'}, {'name': 'Yinhe Zheng'}, {'name': 'Bowen Yu'}, {'name': 'Fei Huang'}, {'name': 'Yongbin Li'}], 'author_detail': {'name': 'Yongbin Li'}, 'author': 'Yongbin Li', 'arxiv_comment': 'ACL2023 Findings Long Paper. arXiv admin note: substantial text\n  overlap with arXiv:2208.14602', 'links': [{'href': 'http://arxiv.org/abs/2305.06555v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.06555v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.06557v1,2023-05-11 04:28:58+00:00,2023-05-11 04:28:58+00:00,Long-Tailed Question Answering in an Open World,"[arxiv.Result.Author('Yi Dai'), arxiv.Result.Author('Hao Lang'), arxiv.Result.Author('Yinhe Zheng'), arxiv.Result.Author('Fei Huang'), arxiv.Result.Author('Yongbin Li')]","Real-world data often have an open long-tailed distribution, and building a
unified QA model supporting various tasks is vital for practical QA
applications. However, it is non-trivial to extend previous QA approaches since
they either require access to seen tasks of adequate samples or do not
explicitly model samples from unseen tasks. In this paper, we define Open
Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and
optimizing performance over seen and unseen QA tasks. We propose an OLTQA model
that encourages knowledge sharing between head, tail and unseen tasks, and
explicitly mines knowledge from a large pre-trained language model (LM).
Specifically, we organize our model through a pool of fine-grained components
and dynamically combine these components for an input to facilitate knowledge
sharing. A retrieve-then-rerank frame is further introduced to select
in-context examples, which guild the LM to generate text that express knowledge
for QA tasks. Moreover, a two-stage training approach is introduced to
pre-train the framework by knowledge distillation (KD) from the LM and then
jointly train the frame and a QA model through an adaptive mutual KD method. On
a large-scale OLTQA dataset we curate from 43 existing QA datasets, our model
consistently outperforms the state-of-the-art. We release the code and data at
\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/oltqa}.",ACL2023 Main Track Long Paper,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.06557v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.06557v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.06557v1,"{'id': 'http://arxiv.org/abs/2305.06557v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.06557v1', 'updated': '2023-05-11T04:28:58Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=11, tm_hour=4, tm_min=28, tm_sec=58, tm_wday=3, tm_yday=131, tm_isdst=0), 'published': '2023-05-11T04:28:58Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=11, tm_hour=4, tm_min=28, tm_sec=58, tm_wday=3, tm_yday=131, tm_isdst=0), 'title': 'Long-Tailed Question Answering in an Open World', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Long-Tailed Question Answering in an Open World'}, 'summary': 'Real-world data often have an open long-tailed distribution, and building a\nunified QA model supporting various tasks is vital for practical QA\napplications. However, it is non-trivial to extend previous QA approaches since\nthey either require access to seen tasks of adequate samples or do not\nexplicitly model samples from unseen tasks. In this paper, we define Open\nLong-Tailed QA (OLTQA) as learning from long-tailed distributed data and\noptimizing performance over seen and unseen QA tasks. We propose an OLTQA model\nthat encourages knowledge sharing between head, tail and unseen tasks, and\nexplicitly mines knowledge from a large pre-trained language model (LM).\nSpecifically, we organize our model through a pool of fine-grained components\nand dynamically combine these components for an input to facilitate knowledge\nsharing. A retrieve-then-rerank frame is further introduced to select\nin-context examples, which guild the LM to generate text that express knowledge\nfor QA tasks. Moreover, a two-stage training approach is introduced to\npre-train the framework by knowledge distillation (KD) from the LM and then\njointly train the frame and a QA model through an adaptive mutual KD method. On\na large-scale OLTQA dataset we curate from 43 existing QA datasets, our model\nconsistently outperforms the state-of-the-art. We release the code and data at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/oltqa}.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Real-world data often have an open long-tailed distribution, and building a\nunified QA model supporting various tasks is vital for practical QA\napplications. However, it is non-trivial to extend previous QA approaches since\nthey either require access to seen tasks of adequate samples or do not\nexplicitly model samples from unseen tasks. In this paper, we define Open\nLong-Tailed QA (OLTQA) as learning from long-tailed distributed data and\noptimizing performance over seen and unseen QA tasks. We propose an OLTQA model\nthat encourages knowledge sharing between head, tail and unseen tasks, and\nexplicitly mines knowledge from a large pre-trained language model (LM).\nSpecifically, we organize our model through a pool of fine-grained components\nand dynamically combine these components for an input to facilitate knowledge\nsharing. A retrieve-then-rerank frame is further introduced to select\nin-context examples, which guild the LM to generate text that express knowledge\nfor QA tasks. Moreover, a two-stage training approach is introduced to\npre-train the framework by knowledge distillation (KD) from the LM and then\njointly train the frame and a QA model through an adaptive mutual KD method. On\na large-scale OLTQA dataset we curate from 43 existing QA datasets, our model\nconsistently outperforms the state-of-the-art. We release the code and data at\n\\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/oltqa}.'}, 'authors': [{'name': 'Yi Dai'}, {'name': 'Hao Lang'}, {'name': 'Yinhe Zheng'}, {'name': 'Fei Huang'}, {'name': 'Yongbin Li'}], 'author_detail': {'name': 'Yongbin Li'}, 'author': 'Yongbin Li', 'arxiv_comment': 'ACL2023 Main Track Long Paper', 'links': [{'href': 'http://arxiv.org/abs/2305.06557v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.06557v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.08096v1,2023-05-14 08:23:03+00:00,2023-05-14 08:23:03+00:00,Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation,"[arxiv.Result.Author('Songming Zhang'), arxiv.Result.Author('Yunlong Liang'), arxiv.Result.Author('Shuaibo Wang'), arxiv.Result.Author('Wenjuan Han'), arxiv.Result.Author('Jian Liu'), arxiv.Result.Author('Jinan Xu'), arxiv.Result.Author('Yufeng Chen')]","Knowledge distillation (KD) is a promising technique for model compression in
neural machine translation. However, where the knowledge hides in KD is still
not clear, which may hinder the development of KD. In this work, we first
unravel this mystery from an empirical perspective and show that the knowledge
comes from the top-1 predictions of teachers, which also helps us build a
potential connection between word- and sequence-level KD. Further, we point out
two inherent issues in vanilla word-level KD based on this finding. Firstly,
the current objective of KD spreads its focus to whole distributions to learn
the knowledge, yet lacks special treatment on the most crucial top-1
information. Secondly, the knowledge is largely covered by the golden
information due to the fact that most top-1 predictions of teachers overlap
with ground-truth tokens, which further restricts the potential of KD. To
address these issues, we propose a novel method named \textbf{T}op-1
\textbf{I}nformation \textbf{E}nhanced \textbf{K}nowledge \textbf{D}istillation
(TIE-KD). Specifically, we design a hierarchical ranking loss to enforce the
learning of the top-1 information from the teacher. Additionally, we develop an
iterative KD procedure to infuse more additional knowledge by distilling on the
data without ground-truth targets. Experiments on WMT'14 English-German, WMT'14
English-French and WMT'16 English-Romanian demonstrate that our method can
respectively boost Transformer$_{base}$ students by +1.04, +0.60 and +1.11 BLEU
scores and significantly outperform the vanilla word-level KD baseline.
Besides, our method shows higher generalizability on different teacher-student
capacity gaps than existing KD techniques.","16 pages, 5 figures; accepted by the main conference of ACL2023",,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.08096v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.08096v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.08096v1,"{'id': 'http://arxiv.org/abs/2305.08096v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.08096v1', 'updated': '2023-05-14T08:23:03Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=14, tm_hour=8, tm_min=23, tm_sec=3, tm_wday=6, tm_yday=134, tm_isdst=0), 'published': '2023-05-14T08:23:03Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=14, tm_hour=8, tm_min=23, tm_sec=3, tm_wday=6, tm_yday=134, tm_isdst=0), 'title': 'Towards Understanding and Improving Knowledge Distillation for Neural\n  Machine Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Towards Understanding and Improving Knowledge Distillation for Neural\n  Machine Translation'}, 'summary': ""Knowledge distillation (KD) is a promising technique for model compression in\nneural machine translation. However, where the knowledge hides in KD is still\nnot clear, which may hinder the development of KD. In this work, we first\nunravel this mystery from an empirical perspective and show that the knowledge\ncomes from the top-1 predictions of teachers, which also helps us build a\npotential connection between word- and sequence-level KD. Further, we point out\ntwo inherent issues in vanilla word-level KD based on this finding. Firstly,\nthe current objective of KD spreads its focus to whole distributions to learn\nthe knowledge, yet lacks special treatment on the most crucial top-1\ninformation. Secondly, the knowledge is largely covered by the golden\ninformation due to the fact that most top-1 predictions of teachers overlap\nwith ground-truth tokens, which further restricts the potential of KD. To\naddress these issues, we propose a novel method named \\textbf{T}op-1\n\\textbf{I}nformation \\textbf{E}nhanced \\textbf{K}nowledge \\textbf{D}istillation\n(TIE-KD). Specifically, we design a hierarchical ranking loss to enforce the\nlearning of the top-1 information from the teacher. Additionally, we develop an\niterative KD procedure to infuse more additional knowledge by distilling on the\ndata without ground-truth targets. Experiments on WMT'14 English-German, WMT'14\nEnglish-French and WMT'16 English-Romanian demonstrate that our method can\nrespectively boost Transformer$_{base}$ students by +1.04, +0.60 and +1.11 BLEU\nscores and significantly outperform the vanilla word-level KD baseline.\nBesides, our method shows higher generalizability on different teacher-student\ncapacity gaps than existing KD techniques."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Knowledge distillation (KD) is a promising technique for model compression in\nneural machine translation. However, where the knowledge hides in KD is still\nnot clear, which may hinder the development of KD. In this work, we first\nunravel this mystery from an empirical perspective and show that the knowledge\ncomes from the top-1 predictions of teachers, which also helps us build a\npotential connection between word- and sequence-level KD. Further, we point out\ntwo inherent issues in vanilla word-level KD based on this finding. Firstly,\nthe current objective of KD spreads its focus to whole distributions to learn\nthe knowledge, yet lacks special treatment on the most crucial top-1\ninformation. Secondly, the knowledge is largely covered by the golden\ninformation due to the fact that most top-1 predictions of teachers overlap\nwith ground-truth tokens, which further restricts the potential of KD. To\naddress these issues, we propose a novel method named \\textbf{T}op-1\n\\textbf{I}nformation \\textbf{E}nhanced \\textbf{K}nowledge \\textbf{D}istillation\n(TIE-KD). Specifically, we design a hierarchical ranking loss to enforce the\nlearning of the top-1 information from the teacher. Additionally, we develop an\niterative KD procedure to infuse more additional knowledge by distilling on the\ndata without ground-truth targets. Experiments on WMT'14 English-German, WMT'14\nEnglish-French and WMT'16 English-Romanian demonstrate that our method can\nrespectively boost Transformer$_{base}$ students by +1.04, +0.60 and +1.11 BLEU\nscores and significantly outperform the vanilla word-level KD baseline.\nBesides, our method shows higher generalizability on different teacher-student\ncapacity gaps than existing KD techniques.""}, 'authors': [{'name': 'Songming Zhang'}, {'name': 'Yunlong Liang'}, {'name': 'Shuaibo Wang'}, {'name': 'Wenjuan Han'}, {'name': 'Jian Liu'}, {'name': 'Jinan Xu'}, {'name': 'Yufeng Chen'}], 'author_detail': {'name': 'Yufeng Chen'}, 'author': 'Yufeng Chen', 'arxiv_comment': '16 pages, 5 figures; accepted by the main conference of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.08096v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.08096v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.08135v2,2023-05-21 15:07:23+00:00,2023-05-14 12:12:24+00:00,Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering,"[arxiv.Result.Author('Qianglong Chen'), arxiv.Result.Author('Guohai Xu'), arxiv.Result.Author('Ming Yan'), arxiv.Result.Author('Ji Zhang'), arxiv.Result.Author('Fei Huang'), arxiv.Result.Author('Luo Si'), arxiv.Result.Author('Yin Zhang')]","Existing knowledge-enhanced methods have achieved remarkable results in
certain QA tasks via obtaining diverse knowledge from different knowledge
bases. However, limited by the properties of retrieved knowledge, they still
have trouble benefiting from both the knowledge relevance and distinguishment
simultaneously. To address the challenge, we propose CPACE, a Concept-centric
Prompt-bAsed Contrastive Explanation Generation model, which aims to convert
obtained symbolic knowledge into a contrastive explanation for better
distinguishing the differences among given candidates. Firstly, following
previous works, we retrieve different types of symbolic knowledge with a
concept-centric knowledge extraction module. After that, we generate
corresponding contrastive explanations using acquired symbolic knowledge and
explanation prompts as guidance for better modeling the knowledge
distinguishment and interpretability. Finally, we regard the generated
contrastive explanation as external knowledge for downstream task enhancement.
We conduct a series of experiments on three widely-used question-answering
datasets: CSQA, QASC, and OBQA. Experimental results demonstrate that with the
help of generated contrastive explanation, our CPACE model achieves new SOTA on
CSQA (89.8% on the testing set, 0.9% higher than human performance), and gains
impressive improvement on QASC and OBQA (4.2% and 3.5%, respectively).",Accepted to ACL2023(Findings). The Camera-ready Version,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.08135v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.08135v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.08135v2,"{'id': 'http://arxiv.org/abs/2305.08135v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.08135v2', 'updated': '2023-05-21T15:07:23Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=21, tm_hour=15, tm_min=7, tm_sec=23, tm_wday=6, tm_yday=141, tm_isdst=0), 'published': '2023-05-14T12:12:24Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=14, tm_hour=12, tm_min=12, tm_sec=24, tm_wday=6, tm_yday=134, tm_isdst=0), 'title': 'Distinguish Before Answer: Generating Contrastive Explanation as\n  Knowledge for Commonsense Question Answering', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Distinguish Before Answer: Generating Contrastive Explanation as\n  Knowledge for Commonsense Question Answering'}, 'summary': 'Existing knowledge-enhanced methods have achieved remarkable results in\ncertain QA tasks via obtaining diverse knowledge from different knowledge\nbases. However, limited by the properties of retrieved knowledge, they still\nhave trouble benefiting from both the knowledge relevance and distinguishment\nsimultaneously. To address the challenge, we propose CPACE, a Concept-centric\nPrompt-bAsed Contrastive Explanation Generation model, which aims to convert\nobtained symbolic knowledge into a contrastive explanation for better\ndistinguishing the differences among given candidates. Firstly, following\nprevious works, we retrieve different types of symbolic knowledge with a\nconcept-centric knowledge extraction module. After that, we generate\ncorresponding contrastive explanations using acquired symbolic knowledge and\nexplanation prompts as guidance for better modeling the knowledge\ndistinguishment and interpretability. Finally, we regard the generated\ncontrastive explanation as external knowledge for downstream task enhancement.\nWe conduct a series of experiments on three widely-used question-answering\ndatasets: CSQA, QASC, and OBQA. Experimental results demonstrate that with the\nhelp of generated contrastive explanation, our CPACE model achieves new SOTA on\nCSQA (89.8% on the testing set, 0.9% higher than human performance), and gains\nimpressive improvement on QASC and OBQA (4.2% and 3.5%, respectively).', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Existing knowledge-enhanced methods have achieved remarkable results in\ncertain QA tasks via obtaining diverse knowledge from different knowledge\nbases. However, limited by the properties of retrieved knowledge, they still\nhave trouble benefiting from both the knowledge relevance and distinguishment\nsimultaneously. To address the challenge, we propose CPACE, a Concept-centric\nPrompt-bAsed Contrastive Explanation Generation model, which aims to convert\nobtained symbolic knowledge into a contrastive explanation for better\ndistinguishing the differences among given candidates. Firstly, following\nprevious works, we retrieve different types of symbolic knowledge with a\nconcept-centric knowledge extraction module. After that, we generate\ncorresponding contrastive explanations using acquired symbolic knowledge and\nexplanation prompts as guidance for better modeling the knowledge\ndistinguishment and interpretability. Finally, we regard the generated\ncontrastive explanation as external knowledge for downstream task enhancement.\nWe conduct a series of experiments on three widely-used question-answering\ndatasets: CSQA, QASC, and OBQA. Experimental results demonstrate that with the\nhelp of generated contrastive explanation, our CPACE model achieves new SOTA on\nCSQA (89.8% on the testing set, 0.9% higher than human performance), and gains\nimpressive improvement on QASC and OBQA (4.2% and 3.5%, respectively).'}, 'authors': [{'name': 'Qianglong Chen'}, {'name': 'Guohai Xu'}, {'name': 'Ming Yan'}, {'name': 'Ji Zhang'}, {'name': 'Fei Huang'}, {'name': 'Luo Si'}, {'name': 'Yin Zhang'}], 'author_detail': {'name': 'Yin Zhang'}, 'author': 'Yin Zhang', 'arxiv_comment': 'Accepted to ACL2023(Findings). The Camera-ready Version', 'links': [{'href': 'http://arxiv.org/abs/2305.08135v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.08135v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.10196v1,2023-05-17 13:19:01+00:00,2023-05-17 13:19:01+00:00,A Survey on Zero Pronoun Translation,"[arxiv.Result.Author('Longyue Wang'), arxiv.Result.Author('Siyou Liu'), arxiv.Result.Author('Mingzhou Xu'), arxiv.Result.Author('Linfeng Song'), arxiv.Result.Author('Shuming Shi'), arxiv.Result.Author('Zhaopeng Tu')]","Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g.
Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop
languages (e.g. English). This phenomenon has been studied extensively in
machine translation (MT), as it poses a significant challenge for MT systems
due to the difficulty in determining the correct antecedent for the pronoun.
This survey paper highlights the major works that have been undertaken in zero
pronoun translation (ZPT) after the neural revolution, so that researchers can
recognise the current state and future directions of this field. We provide an
organisation of the literature based on evolution, dataset, method and
evaluation. In addition, we compare and analyze competing models and evaluation
metrics on different benchmarks. We uncover a number of insightful findings
such as: 1) ZPT is in line with the development trend of large language model;
2) data limitation causes learning bias in languages and domains; 3)
performance improvements are often reported on single benchmarks, but advanced
methods are still far from real-world use; 4) general-purpose metrics are not
reliable on nuances and complexities of ZPT, emphasizing the necessity of
targeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of
gender bias.","ACL2023 Main Conference Long Paper. Longyue Wang and Siyou Liu
  contributed equally to this work",,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.10196v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.10196v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.10196v1,"{'id': 'http://arxiv.org/abs/2305.10196v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.10196v1', 'updated': '2023-05-17T13:19:01Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=17, tm_hour=13, tm_min=19, tm_sec=1, tm_wday=2, tm_yday=137, tm_isdst=0), 'published': '2023-05-17T13:19:01Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=17, tm_hour=13, tm_min=19, tm_sec=1, tm_wday=2, tm_yday=137, tm_isdst=0), 'title': 'A Survey on Zero Pronoun Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A Survey on Zero Pronoun Translation'}, 'summary': 'Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g.\nChinese, Hungarian, and Hindi), but should be recalled in non-pro-drop\nlanguages (e.g. English). This phenomenon has been studied extensively in\nmachine translation (MT), as it poses a significant challenge for MT systems\ndue to the difficulty in determining the correct antecedent for the pronoun.\nThis survey paper highlights the major works that have been undertaken in zero\npronoun translation (ZPT) after the neural revolution, so that researchers can\nrecognise the current state and future directions of this field. We provide an\norganisation of the literature based on evolution, dataset, method and\nevaluation. In addition, we compare and analyze competing models and evaluation\nmetrics on different benchmarks. We uncover a number of insightful findings\nsuch as: 1) ZPT is in line with the development trend of large language model;\n2) data limitation causes learning bias in languages and domains; 3)\nperformance improvements are often reported on single benchmarks, but advanced\nmethods are still far from real-world use; 4) general-purpose metrics are not\nreliable on nuances and complexities of ZPT, emphasizing the necessity of\ntargeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of\ngender bias.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g.\nChinese, Hungarian, and Hindi), but should be recalled in non-pro-drop\nlanguages (e.g. English). This phenomenon has been studied extensively in\nmachine translation (MT), as it poses a significant challenge for MT systems\ndue to the difficulty in determining the correct antecedent for the pronoun.\nThis survey paper highlights the major works that have been undertaken in zero\npronoun translation (ZPT) after the neural revolution, so that researchers can\nrecognise the current state and future directions of this field. We provide an\norganisation of the literature based on evolution, dataset, method and\nevaluation. In addition, we compare and analyze competing models and evaluation\nmetrics on different benchmarks. We uncover a number of insightful findings\nsuch as: 1) ZPT is in line with the development trend of large language model;\n2) data limitation causes learning bias in languages and domains; 3)\nperformance improvements are often reported on single benchmarks, but advanced\nmethods are still far from real-world use; 4) general-purpose metrics are not\nreliable on nuances and complexities of ZPT, emphasizing the necessity of\ntargeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of\ngender bias.'}, 'authors': [{'name': 'Longyue Wang'}, {'name': 'Siyou Liu'}, {'name': 'Mingzhou Xu'}, {'name': 'Linfeng Song'}, {'name': 'Shuming Shi'}, {'name': 'Zhaopeng Tu'}], 'author_detail': {'name': 'Zhaopeng Tu'}, 'author': 'Zhaopeng Tu', 'arxiv_comment': 'ACL2023 Main Conference Long Paper. Longyue Wang and Siyou Liu\n  contributed equally to this work', 'links': [{'href': 'http://arxiv.org/abs/2305.10196v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.10196v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.10686v1,2023-05-18 03:57:51+00:00,2023-05-18 03:57:51+00:00,RMSSinger: Realistic-Music-Score based Singing Voice Synthesis,"[arxiv.Result.Author('Jinzheng He'), arxiv.Result.Author('Jinglin Liu'), arxiv.Result.Author('Zhenhui Ye'), arxiv.Result.Author('Rongjie Huang'), arxiv.Result.Author('Chenye Cui'), arxiv.Result.Author('Huadai Liu'), arxiv.Result.Author('Zhou Zhao')]","We are interested in a challenging task, Realistic-Music-Score based Singing
Voice Synthesis (RMS-SVS). RMS-SVS aims to generate high-quality singing voices
given realistic music scores with different note types (grace, slur, rest,
etc.). Though significant progress has been achieved, recent singing voice
synthesis (SVS) methods are limited to fine-grained music scores, which require
a complicated data collection pipeline with time-consuming manual annotation to
align music notes with phonemes. Furthermore, these manual annotation destroys
the regularity of note durations in music scores, making fine-grained music
scores inconvenient for composing. To tackle these challenges, we propose
RMSSinger, the first RMS-SVS method, which takes realistic music scores as
input, eliminating most of the tedious manual annotation and avoiding the
aforementioned inconvenience. Note that music scores are based on words rather
than phonemes, in RMSSinger, we introduce word-level modeling to avoid the
time-consuming phoneme duration annotation and the complicated phoneme-level
mel-note alignment. Furthermore, we propose the first diffusion-based pitch
modeling method, which ameliorates the naturalness of existing pitch-modeling
methods. To achieve these, we collect a new dataset containing realistic music
scores and singing voices according to these realistic music scores from
professional singers. Extensive experiments on the dataset demonstrate the
effectiveness of our methods. Audio samples are available at
https://rmssinger.github.io/.",Accepted by Finding of ACL2023,,,cs.SD,"['cs.SD', 'cs.CL', 'eess.AS']","[arxiv.Result.Link('http://arxiv.org/abs/2305.10686v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.10686v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.10686v1,"{'id': 'http://arxiv.org/abs/2305.10686v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.10686v1', 'updated': '2023-05-18T03:57:51Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=18, tm_hour=3, tm_min=57, tm_sec=51, tm_wday=3, tm_yday=138, tm_isdst=0), 'published': '2023-05-18T03:57:51Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=18, tm_hour=3, tm_min=57, tm_sec=51, tm_wday=3, tm_yday=138, tm_isdst=0), 'title': 'RMSSinger: Realistic-Music-Score based Singing Voice Synthesis', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'RMSSinger: Realistic-Music-Score based Singing Voice Synthesis'}, 'summary': 'We are interested in a challenging task, Realistic-Music-Score based Singing\nVoice Synthesis (RMS-SVS). RMS-SVS aims to generate high-quality singing voices\ngiven realistic music scores with different note types (grace, slur, rest,\netc.). Though significant progress has been achieved, recent singing voice\nsynthesis (SVS) methods are limited to fine-grained music scores, which require\na complicated data collection pipeline with time-consuming manual annotation to\nalign music notes with phonemes. Furthermore, these manual annotation destroys\nthe regularity of note durations in music scores, making fine-grained music\nscores inconvenient for composing. To tackle these challenges, we propose\nRMSSinger, the first RMS-SVS method, which takes realistic music scores as\ninput, eliminating most of the tedious manual annotation and avoiding the\naforementioned inconvenience. Note that music scores are based on words rather\nthan phonemes, in RMSSinger, we introduce word-level modeling to avoid the\ntime-consuming phoneme duration annotation and the complicated phoneme-level\nmel-note alignment. Furthermore, we propose the first diffusion-based pitch\nmodeling method, which ameliorates the naturalness of existing pitch-modeling\nmethods. To achieve these, we collect a new dataset containing realistic music\nscores and singing voices according to these realistic music scores from\nprofessional singers. Extensive experiments on the dataset demonstrate the\neffectiveness of our methods. Audio samples are available at\nhttps://rmssinger.github.io/.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We are interested in a challenging task, Realistic-Music-Score based Singing\nVoice Synthesis (RMS-SVS). RMS-SVS aims to generate high-quality singing voices\ngiven realistic music scores with different note types (grace, slur, rest,\netc.). Though significant progress has been achieved, recent singing voice\nsynthesis (SVS) methods are limited to fine-grained music scores, which require\na complicated data collection pipeline with time-consuming manual annotation to\nalign music notes with phonemes. Furthermore, these manual annotation destroys\nthe regularity of note durations in music scores, making fine-grained music\nscores inconvenient for composing. To tackle these challenges, we propose\nRMSSinger, the first RMS-SVS method, which takes realistic music scores as\ninput, eliminating most of the tedious manual annotation and avoiding the\naforementioned inconvenience. Note that music scores are based on words rather\nthan phonemes, in RMSSinger, we introduce word-level modeling to avoid the\ntime-consuming phoneme duration annotation and the complicated phoneme-level\nmel-note alignment. Furthermore, we propose the first diffusion-based pitch\nmodeling method, which ameliorates the naturalness of existing pitch-modeling\nmethods. To achieve these, we collect a new dataset containing realistic music\nscores and singing voices according to these realistic music scores from\nprofessional singers. Extensive experiments on the dataset demonstrate the\neffectiveness of our methods. Audio samples are available at\nhttps://rmssinger.github.io/.'}, 'authors': [{'name': 'Jinzheng He'}, {'name': 'Jinglin Liu'}, {'name': 'Zhenhui Ye'}, {'name': 'Rongjie Huang'}, {'name': 'Chenye Cui'}, {'name': 'Huadai Liu'}, {'name': 'Zhou Zhao'}], 'author_detail': {'name': 'Zhou Zhao'}, 'author': 'Zhou Zhao', 'arxiv_comment': 'Accepted by Finding of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.10686v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.10686v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.11564v2,2023-09-18 09:42:26+00:00,2023-05-19 10:01:55+00:00,Decouple knowledge from parameters for plug-and-play language modeling,"[arxiv.Result.Author('Xin Cheng'), arxiv.Result.Author('Yankai Lin'), arxiv.Result.Author('Xiuying Chen'), arxiv.Result.Author('Dongyan Zhao'), arxiv.Result.Author('Rui Yan')]","Pre-trained language models(PLM) have made impressive results in various NLP
tasks. It has been revealed that one of the key factors to their success is the
parameters of these models implicitly learn all kinds of knowledge during
pre-training. However, encoding knowledge implicitly in the model parameters
has two fundamental drawbacks. First, the knowledge is neither editable nor
scalable once the model is trained, which is especially problematic in that
knowledge is consistently evolving. Second, it lacks interpretability and
prevents humans from understanding which knowledge PLM requires for a certain
problem. In this paper, we introduce PlugLM, a pre-training model with
differentiable plug-in memory(DPM). The key intuition is to decouple the
knowledge storage from model parameters with an editable and scalable key-value
memory and leverage knowledge in an explainable manner by knowledge retrieval
in the DPM. To justify this design choice, we conduct evaluations in three
settings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements
across four domains on average without any in-domain pre-training. (2)
knowledge update. PlugLM could absorb new knowledge in a training-free way
after pre-training is done. (3) in-task knowledge learning. PlugLM could be
further improved by incorporating training samples into DPM with knowledge
prompting.",ACL2023 Findings,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.11564v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.11564v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.11564v2,"{'id': 'http://arxiv.org/abs/2305.11564v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.11564v2', 'updated': '2023-09-18T09:42:26Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=9, tm_mday=18, tm_hour=9, tm_min=42, tm_sec=26, tm_wday=0, tm_yday=261, tm_isdst=0), 'published': '2023-05-19T10:01:55Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=19, tm_hour=10, tm_min=1, tm_sec=55, tm_wday=4, tm_yday=139, tm_isdst=0), 'title': 'Decouple knowledge from parameters for plug-and-play language modeling', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Decouple knowledge from parameters for plug-and-play language modeling'}, 'summary': 'Pre-trained language models(PLM) have made impressive results in various NLP\ntasks. It has been revealed that one of the key factors to their success is the\nparameters of these models implicitly learn all kinds of knowledge during\npre-training. However, encoding knowledge implicitly in the model parameters\nhas two fundamental drawbacks. First, the knowledge is neither editable nor\nscalable once the model is trained, which is especially problematic in that\nknowledge is consistently evolving. Second, it lacks interpretability and\nprevents humans from understanding which knowledge PLM requires for a certain\nproblem. In this paper, we introduce PlugLM, a pre-training model with\ndifferentiable plug-in memory(DPM). The key intuition is to decouple the\nknowledge storage from model parameters with an editable and scalable key-value\nmemory and leverage knowledge in an explainable manner by knowledge retrieval\nin the DPM. To justify this design choice, we conduct evaluations in three\nsettings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements\nacross four domains on average without any in-domain pre-training. (2)\nknowledge update. PlugLM could absorb new knowledge in a training-free way\nafter pre-training is done. (3) in-task knowledge learning. PlugLM could be\nfurther improved by incorporating training samples into DPM with knowledge\nprompting.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Pre-trained language models(PLM) have made impressive results in various NLP\ntasks. It has been revealed that one of the key factors to their success is the\nparameters of these models implicitly learn all kinds of knowledge during\npre-training. However, encoding knowledge implicitly in the model parameters\nhas two fundamental drawbacks. First, the knowledge is neither editable nor\nscalable once the model is trained, which is especially problematic in that\nknowledge is consistently evolving. Second, it lacks interpretability and\nprevents humans from understanding which knowledge PLM requires for a certain\nproblem. In this paper, we introduce PlugLM, a pre-training model with\ndifferentiable plug-in memory(DPM). The key intuition is to decouple the\nknowledge storage from model parameters with an editable and scalable key-value\nmemory and leverage knowledge in an explainable manner by knowledge retrieval\nin the DPM. To justify this design choice, we conduct evaluations in three\nsettings including: (1) domain adaptation. PlugLM obtains 3.95 F1 improvements\nacross four domains on average without any in-domain pre-training. (2)\nknowledge update. PlugLM could absorb new knowledge in a training-free way\nafter pre-training is done. (3) in-task knowledge learning. PlugLM could be\nfurther improved by incorporating training samples into DPM with knowledge\nprompting.'}, 'authors': [{'name': 'Xin Cheng'}, {'name': 'Yankai Lin'}, {'name': 'Xiuying Chen'}, {'name': 'Dongyan Zhao'}, {'name': 'Rui Yan'}], 'author_detail': {'name': 'Rui Yan'}, 'author': 'Rui Yan', 'arxiv_comment': 'ACL2023 Findings', 'links': [{'href': 'http://arxiv.org/abs/2305.11564v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.11564v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.12376v1,2023-05-21 07:10:31+00:00,2023-05-21 07:10:31+00:00,Measuring Intersectional Biases in Historical Documents,"[arxiv.Result.Author('Nadav Borenstein'), arxiv.Result.Author('Karolina Stańczak'), arxiv.Result.Author('Thea Rolskov'), arxiv.Result.Author('Natália da Silva Perez'), arxiv.Result.Author('Natacha Klein Käfer'), arxiv.Result.Author('Isabelle Augenstein')]","Data-driven analyses of biases in historical texts can help illuminate the
origin and development of biases prevailing in modern society.
  However, digitised historical documents pose a challenge for NLP
practitioners as these corpora suffer from errors introduced by optical
character recognition (OCR) and are written in an archaic language. In this
paper, we investigate the continuities and transformations of bias in
historical newspapers published in the Caribbean during the colonial era (18th
to 19th centuries). Our analyses are performed along the axes of gender, race,
and their intersection. We examine these biases by conducting a temporal study
in which we measure the development of lexical associations using
distributional semantics models and word embeddings. Further, we evaluate the
effectiveness of techniques designed to process OCR-generated data and assess
their stability when trained on and applied to the noisy historical newspapers.
We find that there is a trade-off between the stability of the word embeddings
and their compatibility with the historical dataset. We provide evidence that
gender and racial biases are interdependent, and their intersection triggers
distinct effects. These findings align with the theory of intersectionality,
which stresses that biases affecting people with multiple marginalised
identities compound to more than the sum of their constituents.",Accepted to Findings of ACL2023,,,cs.CL,"['cs.CL', 'cs.CY', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.12376v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.12376v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.12376v1,"{'id': 'http://arxiv.org/abs/2305.12376v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.12376v1', 'updated': '2023-05-21T07:10:31Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=21, tm_hour=7, tm_min=10, tm_sec=31, tm_wday=6, tm_yday=141, tm_isdst=0), 'published': '2023-05-21T07:10:31Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=21, tm_hour=7, tm_min=10, tm_sec=31, tm_wday=6, tm_yday=141, tm_isdst=0), 'title': 'Measuring Intersectional Biases in Historical Documents', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Measuring Intersectional Biases in Historical Documents'}, 'summary': 'Data-driven analyses of biases in historical texts can help illuminate the\norigin and development of biases prevailing in modern society.\n  However, digitised historical documents pose a challenge for NLP\npractitioners as these corpora suffer from errors introduced by optical\ncharacter recognition (OCR) and are written in an archaic language. In this\npaper, we investigate the continuities and transformations of bias in\nhistorical newspapers published in the Caribbean during the colonial era (18th\nto 19th centuries). Our analyses are performed along the axes of gender, race,\nand their intersection. We examine these biases by conducting a temporal study\nin which we measure the development of lexical associations using\ndistributional semantics models and word embeddings. Further, we evaluate the\neffectiveness of techniques designed to process OCR-generated data and assess\ntheir stability when trained on and applied to the noisy historical newspapers.\nWe find that there is a trade-off between the stability of the word embeddings\nand their compatibility with the historical dataset. We provide evidence that\ngender and racial biases are interdependent, and their intersection triggers\ndistinct effects. These findings align with the theory of intersectionality,\nwhich stresses that biases affecting people with multiple marginalised\nidentities compound to more than the sum of their constituents.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Data-driven analyses of biases in historical texts can help illuminate the\norigin and development of biases prevailing in modern society.\n  However, digitised historical documents pose a challenge for NLP\npractitioners as these corpora suffer from errors introduced by optical\ncharacter recognition (OCR) and are written in an archaic language. In this\npaper, we investigate the continuities and transformations of bias in\nhistorical newspapers published in the Caribbean during the colonial era (18th\nto 19th centuries). Our analyses are performed along the axes of gender, race,\nand their intersection. We examine these biases by conducting a temporal study\nin which we measure the development of lexical associations using\ndistributional semantics models and word embeddings. Further, we evaluate the\neffectiveness of techniques designed to process OCR-generated data and assess\ntheir stability when trained on and applied to the noisy historical newspapers.\nWe find that there is a trade-off between the stability of the word embeddings\nand their compatibility with the historical dataset. We provide evidence that\ngender and racial biases are interdependent, and their intersection triggers\ndistinct effects. These findings align with the theory of intersectionality,\nwhich stresses that biases affecting people with multiple marginalised\nidentities compound to more than the sum of their constituents.'}, 'authors': [{'name': 'Nadav Borenstein'}, {'name': 'Karolina Stańczak'}, {'name': 'Thea Rolskov'}, {'name': 'Natália da Silva Perez'}, {'name': 'Natacha Klein Käfer'}, {'name': 'Isabelle Augenstein'}], 'author_detail': {'name': 'Isabelle Augenstein'}, 'author': 'Isabelle Augenstein', 'arxiv_comment': 'Accepted to Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.12376v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.12376v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CY', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.14652v3,2023-05-31 08:20:33+00:00,2023-05-24 02:39:43+00:00,Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion,"[arxiv.Result.Author('Shaoxiang Wu'), arxiv.Result.Author('Damai Dai'), arxiv.Result.Author('Ziwei Qin'), arxiv.Result.Author('Tianyu Liu'), arxiv.Result.Author('Binghuai Lin'), arxiv.Result.Author('Yunbo Cao'), arxiv.Result.Author('Zhifang Sui')]","Video multimodal fusion aims to integrate multimodal signals in videos, such
as visual, audio and text, to make a complementary prediction with multiple
modalities contents. However, unlike other image-text multimodal tasks, video
has longer multimodal sequences with more redundancy and noise in both visual
and audio modalities. Prior denoising methods like forget gate are coarse in
the granularity of noise filtering. They often suppress the redundant and noisy
information at the risk of losing critical information. Therefore, we propose a
denoising bottleneck fusion (DBF) model for fine-grained video multimodal
fusion. On the one hand, we employ a bottleneck mechanism to filter out noise
and redundancy with a restrained receptive field. On the other hand, we use a
mutual information maximization module to regulate the filter-out module to
preserve key information within different modalities. Our DBF model achieves
significant improvement over current state-of-the-art baselines on multiple
benchmarks covering multimodal sentiment analysis and multimodal summarization
tasks. It proves that our model can effectively capture salient features from
noisy and redundant video, audio, and text inputs. The code for this paper is
publicly available at https://github.com/WSXRHFG/DBF.",Accept at ACL2023,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.14652v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.14652v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.14652v3,"{'id': 'http://arxiv.org/abs/2305.14652v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.14652v3', 'updated': '2023-05-31T08:20:33Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=31, tm_hour=8, tm_min=20, tm_sec=33, tm_wday=2, tm_yday=151, tm_isdst=0), 'published': '2023-05-24T02:39:43Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=24, tm_hour=2, tm_min=39, tm_sec=43, tm_wday=2, tm_yday=144, tm_isdst=0), 'title': 'Denoising Bottleneck with Mutual Information Maximization for Video\n  Multimodal Fusion', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Denoising Bottleneck with Mutual Information Maximization for Video\n  Multimodal Fusion'}, 'summary': 'Video multimodal fusion aims to integrate multimodal signals in videos, such\nas visual, audio and text, to make a complementary prediction with multiple\nmodalities contents. However, unlike other image-text multimodal tasks, video\nhas longer multimodal sequences with more redundancy and noise in both visual\nand audio modalities. Prior denoising methods like forget gate are coarse in\nthe granularity of noise filtering. They often suppress the redundant and noisy\ninformation at the risk of losing critical information. Therefore, we propose a\ndenoising bottleneck fusion (DBF) model for fine-grained video multimodal\nfusion. On the one hand, we employ a bottleneck mechanism to filter out noise\nand redundancy with a restrained receptive field. On the other hand, we use a\nmutual information maximization module to regulate the filter-out module to\npreserve key information within different modalities. Our DBF model achieves\nsignificant improvement over current state-of-the-art baselines on multiple\nbenchmarks covering multimodal sentiment analysis and multimodal summarization\ntasks. It proves that our model can effectively capture salient features from\nnoisy and redundant video, audio, and text inputs. The code for this paper is\npublicly available at https://github.com/WSXRHFG/DBF.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Video multimodal fusion aims to integrate multimodal signals in videos, such\nas visual, audio and text, to make a complementary prediction with multiple\nmodalities contents. However, unlike other image-text multimodal tasks, video\nhas longer multimodal sequences with more redundancy and noise in both visual\nand audio modalities. Prior denoising methods like forget gate are coarse in\nthe granularity of noise filtering. They often suppress the redundant and noisy\ninformation at the risk of losing critical information. Therefore, we propose a\ndenoising bottleneck fusion (DBF) model for fine-grained video multimodal\nfusion. On the one hand, we employ a bottleneck mechanism to filter out noise\nand redundancy with a restrained receptive field. On the other hand, we use a\nmutual information maximization module to regulate the filter-out module to\npreserve key information within different modalities. Our DBF model achieves\nsignificant improvement over current state-of-the-art baselines on multiple\nbenchmarks covering multimodal sentiment analysis and multimodal summarization\ntasks. It proves that our model can effectively capture salient features from\nnoisy and redundant video, audio, and text inputs. The code for this paper is\npublicly available at https://github.com/WSXRHFG/DBF.'}, 'authors': [{'name': 'Shaoxiang Wu'}, {'name': 'Damai Dai'}, {'name': 'Ziwei Qin'}, {'name': 'Tianyu Liu'}, {'name': 'Binghuai Lin'}, {'name': 'Yunbo Cao'}, {'name': 'Zhifang Sui'}], 'author_detail': {'name': 'Zhifang Sui'}, 'author': 'Zhifang Sui', 'arxiv_comment': 'Accept at ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.14652v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.14652v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.15904v1,2023-05-25 10:06:08+00:00,2023-05-25 10:06:08+00:00,MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation,"[arxiv.Result.Author('Sebastian Vincent'), arxiv.Result.Author('Robert Flynn'), arxiv.Result.Author('Carolina Scarton')]","Efficient utilisation of both intra- and extra-textual context remains one of
the critical gaps between machine and human translation. Existing research has
primarily focused on providing individual, well-defined types of context in
translation, such as the surrounding text or discrete external variables like
the speaker's gender. This work introduces MTCue, a novel neural machine
translation (NMT) framework that interprets all context (including discrete
variables) as text. MTCue learns an abstract representation of context,
enabling transferability across different data settings and leveraging similar
attributes in low-resource scenarios. With a focus on a dialogue domain with
access to document and metadata context, we extensively evaluate MTCue in four
language pairs in both translation directions. Our framework demonstrates
significant improvements in translation quality over a parameter-matched
non-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58).
Moreover, MTCue significantly outperforms a ""tagging"" baseline at translating
English text. Analysis reveals that the context encoder of MTCue learns a
representation space that organises context based on specific attributes, such
as formality, enabling effective zero-shot control. Pre-training on context
embeddings also improves MTCue's few-shot performance compared to the ""tagging""
baseline. Finally, an ablation study conducted on model components and
contextual variables further supports the robustness of MTCue for context-based
NMT.",Accepted to Findings at ACL2023,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.15904v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.15904v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.15904v1,"{'id': 'http://arxiv.org/abs/2305.15904v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.15904v1', 'updated': '2023-05-25T10:06:08Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=25, tm_hour=10, tm_min=6, tm_sec=8, tm_wday=3, tm_yday=145, tm_isdst=0), 'published': '2023-05-25T10:06:08Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=25, tm_hour=10, tm_min=6, tm_sec=8, tm_wday=3, tm_yday=145, tm_isdst=0), 'title': 'MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by\n  Leveraging Unstructured Context in Neural Machine Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by\n  Leveraging Unstructured Context in Neural Machine Translation'}, 'summary': 'Efficient utilisation of both intra- and extra-textual context remains one of\nthe critical gaps between machine and human translation. Existing research has\nprimarily focused on providing individual, well-defined types of context in\ntranslation, such as the surrounding text or discrete external variables like\nthe speaker\'s gender. This work introduces MTCue, a novel neural machine\ntranslation (NMT) framework that interprets all context (including discrete\nvariables) as text. MTCue learns an abstract representation of context,\nenabling transferability across different data settings and leveraging similar\nattributes in low-resource scenarios. With a focus on a dialogue domain with\naccess to document and metadata context, we extensively evaluate MTCue in four\nlanguage pairs in both translation directions. Our framework demonstrates\nsignificant improvements in translation quality over a parameter-matched\nnon-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58).\nMoreover, MTCue significantly outperforms a ""tagging"" baseline at translating\nEnglish text. Analysis reveals that the context encoder of MTCue learns a\nrepresentation space that organises context based on specific attributes, such\nas formality, enabling effective zero-shot control. Pre-training on context\nembeddings also improves MTCue\'s few-shot performance compared to the ""tagging""\nbaseline. Finally, an ablation study conducted on model components and\ncontextual variables further supports the robustness of MTCue for context-based\nNMT.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Efficient utilisation of both intra- and extra-textual context remains one of\nthe critical gaps between machine and human translation. Existing research has\nprimarily focused on providing individual, well-defined types of context in\ntranslation, such as the surrounding text or discrete external variables like\nthe speaker\'s gender. This work introduces MTCue, a novel neural machine\ntranslation (NMT) framework that interprets all context (including discrete\nvariables) as text. MTCue learns an abstract representation of context,\nenabling transferability across different data settings and leveraging similar\nattributes in low-resource scenarios. With a focus on a dialogue domain with\naccess to document and metadata context, we extensively evaluate MTCue in four\nlanguage pairs in both translation directions. Our framework demonstrates\nsignificant improvements in translation quality over a parameter-matched\nnon-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58).\nMoreover, MTCue significantly outperforms a ""tagging"" baseline at translating\nEnglish text. Analysis reveals that the context encoder of MTCue learns a\nrepresentation space that organises context based on specific attributes, such\nas formality, enabling effective zero-shot control. Pre-training on context\nembeddings also improves MTCue\'s few-shot performance compared to the ""tagging""\nbaseline. Finally, an ablation study conducted on model components and\ncontextual variables further supports the robustness of MTCue for context-based\nNMT.'}, 'authors': [{'name': 'Sebastian Vincent'}, {'name': 'Robert Flynn'}, {'name': 'Carolina Scarton'}], 'author_detail': {'name': 'Carolina Scarton'}, 'author': 'Carolina Scarton', 'arxiv_comment': 'Accepted to Findings at ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.15904v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.15904v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.16371v1,2023-05-25 13:06:01+00:00,2023-05-25 13:06:01+00:00,INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced Non-Native Speech Recognition,"[arxiv.Result.Author('Eunseop Yoon'), arxiv.Result.Author('Hee Suk Yoon'), arxiv.Result.Author('John Harvill'), arxiv.Result.Author('Mark Hasegawa-Johnson'), arxiv.Result.Author('Chang D. Yoo')]","Automatic Speech Recognition (ASR) systems have attained unprecedented
performance with large speech models pre-trained based on self-supervised
speech representation learning. However, these pre-trained speech models suffer
from representational bias as they tend to better represent those prominent
accents (i.e., native (L1) English accent) in the pre-training speech corpus
than less represented accents, resulting in a deteriorated performance for
non-native (L2) English accents. Although there have been some approaches to
mitigate this issue, all of these methods require updating the pre-trained
model weights. In this paper, we propose Information Theoretic Adversarial
Prompt Tuning (INTapt), which introduces prompts concatenated to the original
input that can re-modulate the attention of the pre-trained model such that the
corresponding input resembles a native (L1) English speech without updating the
backbone weights. INTapt is trained simultaneously in the following two
manners: (1) adversarial training to reduce accent feature dependence between
the original input and the prompt-concatenated input and (2) training to
minimize CTC loss for improving ASR performance to a prompt-concatenated input.
Experimental results show that INTapt improves the performance of L2 English
and increases feature similarity between L2 and L1 accents.",ACL2023,,,cs.CL,"['cs.CL', 'cs.SD', 'eess.AS']","[arxiv.Result.Link('http://arxiv.org/abs/2305.16371v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.16371v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.16371v1,"{'id': 'http://arxiv.org/abs/2305.16371v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.16371v1', 'updated': '2023-05-25T13:06:01Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=25, tm_hour=13, tm_min=6, tm_sec=1, tm_wday=3, tm_yday=145, tm_isdst=0), 'published': '2023-05-25T13:06:01Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=25, tm_hour=13, tm_min=6, tm_sec=1, tm_wday=3, tm_yday=145, tm_isdst=0), 'title': 'INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced\n  Non-Native Speech Recognition', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'INTapt: Information-Theoretic Adversarial Prompt Tuning for Enhanced\n  Non-Native Speech Recognition'}, 'summary': 'Automatic Speech Recognition (ASR) systems have attained unprecedented\nperformance with large speech models pre-trained based on self-supervised\nspeech representation learning. However, these pre-trained speech models suffer\nfrom representational bias as they tend to better represent those prominent\naccents (i.e., native (L1) English accent) in the pre-training speech corpus\nthan less represented accents, resulting in a deteriorated performance for\nnon-native (L2) English accents. Although there have been some approaches to\nmitigate this issue, all of these methods require updating the pre-trained\nmodel weights. In this paper, we propose Information Theoretic Adversarial\nPrompt Tuning (INTapt), which introduces prompts concatenated to the original\ninput that can re-modulate the attention of the pre-trained model such that the\ncorresponding input resembles a native (L1) English speech without updating the\nbackbone weights. INTapt is trained simultaneously in the following two\nmanners: (1) adversarial training to reduce accent feature dependence between\nthe original input and the prompt-concatenated input and (2) training to\nminimize CTC loss for improving ASR performance to a prompt-concatenated input.\nExperimental results show that INTapt improves the performance of L2 English\nand increases feature similarity between L2 and L1 accents.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Automatic Speech Recognition (ASR) systems have attained unprecedented\nperformance with large speech models pre-trained based on self-supervised\nspeech representation learning. However, these pre-trained speech models suffer\nfrom representational bias as they tend to better represent those prominent\naccents (i.e., native (L1) English accent) in the pre-training speech corpus\nthan less represented accents, resulting in a deteriorated performance for\nnon-native (L2) English accents. Although there have been some approaches to\nmitigate this issue, all of these methods require updating the pre-trained\nmodel weights. In this paper, we propose Information Theoretic Adversarial\nPrompt Tuning (INTapt), which introduces prompts concatenated to the original\ninput that can re-modulate the attention of the pre-trained model such that the\ncorresponding input resembles a native (L1) English speech without updating the\nbackbone weights. INTapt is trained simultaneously in the following two\nmanners: (1) adversarial training to reduce accent feature dependence between\nthe original input and the prompt-concatenated input and (2) training to\nminimize CTC loss for improving ASR performance to a prompt-concatenated input.\nExperimental results show that INTapt improves the performance of L2 English\nand increases feature similarity between L2 and L1 accents.'}, 'authors': [{'name': 'Eunseop Yoon'}, {'name': 'Hee Suk Yoon'}, {'name': 'John Harvill'}, {'name': 'Mark Hasegawa-Johnson'}, {'name': 'Chang D. Yoo'}], 'author_detail': {'name': 'Chang D. Yoo'}, 'author': 'Chang D. Yoo', 'arxiv_comment': 'ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.16371v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.16371v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.16724v2,2023-07-07 07:51:38+00:00,2023-05-26 08:22:35+00:00,Code-Switched Text Synthesis in Unseen Language Pairs,"[arxiv.Result.Author('I-Hung Hsu'), arxiv.Result.Author('Avik Ray'), arxiv.Result.Author('Shubham Garg'), arxiv.Result.Author('Nanyun Peng'), arxiv.Result.Author('Jing Huang')]","Existing efforts on text synthesis for code-switching mostly require training
on code-switched texts in the target language pairs, limiting the deployment of
the models to cases lacking code-switched data. In this work, we study the
problem of synthesizing code-switched texts for language pairs absent from the
training data. We introduce GLOSS, a model built on top of a pre-trained
multilingual machine translation model (PMMTM) with an additional
code-switching module. This module, either an adapter or extra prefixes, learns
code-switching patterns from code-switched data during training, while the
primary component of GLOSS, i.e., the PMMTM, is frozen. The design of only
adjusting the code-switching module prevents our model from overfitting to the
constrained training data for code-switching. Hence, GLOSS exhibits the ability
to generalize and synthesize code-switched texts across a broader spectrum of
language pairs. Additionally, we develop a self-training algorithm on target
language pairs further to enhance the reliability of GLOSS. Automatic
evaluations on four language pairs show that GLOSS achieves at least 55%
relative BLEU and METEOR scores improvements compared to strong baselines.
Human evaluations on two language pairs further validate the success of GLOSS.",Paper accepted by ACL2023 as a Finding paper,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.16724v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.16724v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.16724v2,"{'id': 'http://arxiv.org/abs/2305.16724v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.16724v2', 'updated': '2023-07-07T07:51:38Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=7, tm_hour=7, tm_min=51, tm_sec=38, tm_wday=4, tm_yday=188, tm_isdst=0), 'published': '2023-05-26T08:22:35Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=8, tm_min=22, tm_sec=35, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'Code-Switched Text Synthesis in Unseen Language Pairs', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Code-Switched Text Synthesis in Unseen Language Pairs'}, 'summary': 'Existing efforts on text synthesis for code-switching mostly require training\non code-switched texts in the target language pairs, limiting the deployment of\nthe models to cases lacking code-switched data. In this work, we study the\nproblem of synthesizing code-switched texts for language pairs absent from the\ntraining data. We introduce GLOSS, a model built on top of a pre-trained\nmultilingual machine translation model (PMMTM) with an additional\ncode-switching module. This module, either an adapter or extra prefixes, learns\ncode-switching patterns from code-switched data during training, while the\nprimary component of GLOSS, i.e., the PMMTM, is frozen. The design of only\nadjusting the code-switching module prevents our model from overfitting to the\nconstrained training data for code-switching. Hence, GLOSS exhibits the ability\nto generalize and synthesize code-switched texts across a broader spectrum of\nlanguage pairs. Additionally, we develop a self-training algorithm on target\nlanguage pairs further to enhance the reliability of GLOSS. Automatic\nevaluations on four language pairs show that GLOSS achieves at least 55%\nrelative BLEU and METEOR scores improvements compared to strong baselines.\nHuman evaluations on two language pairs further validate the success of GLOSS.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Existing efforts on text synthesis for code-switching mostly require training\non code-switched texts in the target language pairs, limiting the deployment of\nthe models to cases lacking code-switched data. In this work, we study the\nproblem of synthesizing code-switched texts for language pairs absent from the\ntraining data. We introduce GLOSS, a model built on top of a pre-trained\nmultilingual machine translation model (PMMTM) with an additional\ncode-switching module. This module, either an adapter or extra prefixes, learns\ncode-switching patterns from code-switched data during training, while the\nprimary component of GLOSS, i.e., the PMMTM, is frozen. The design of only\nadjusting the code-switching module prevents our model from overfitting to the\nconstrained training data for code-switching. Hence, GLOSS exhibits the ability\nto generalize and synthesize code-switched texts across a broader spectrum of\nlanguage pairs. Additionally, we develop a self-training algorithm on target\nlanguage pairs further to enhance the reliability of GLOSS. Automatic\nevaluations on four language pairs show that GLOSS achieves at least 55%\nrelative BLEU and METEOR scores improvements compared to strong baselines.\nHuman evaluations on two language pairs further validate the success of GLOSS.'}, 'authors': [{'name': 'I-Hung Hsu'}, {'name': 'Avik Ray'}, {'name': 'Shubham Garg'}, {'name': 'Nanyun Peng'}, {'name': 'Jing Huang'}], 'author_detail': {'name': 'Jing Huang'}, 'author': 'Jing Huang', 'arxiv_comment': 'Paper accepted by ACL2023 as a Finding paper', 'links': [{'href': 'http://arxiv.org/abs/2305.16724v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.16724v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.16734v1,2023-05-26 08:38:25+00:00,2023-05-26 08:38:25+00:00,AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction Model,"[arxiv.Result.Author('I-Hung Hsu'), arxiv.Result.Author('Zhiyu Xie'), arxiv.Result.Author('Kuan-Hao Huang'), arxiv.Result.Author('Prem Natarajan'), arxiv.Result.Author('Nanyun Peng')]","Event argument extraction (EAE) identifies event arguments and their specific
roles for a given event. Recent advancement in generation-based EAE models has
shown great performance and generalizability over classification-based models.
However, existing generation-based EAE models mostly focus on problem
re-formulation and prompt design, without incorporating additional information
that has been shown to be effective for classification-based models, such as
the abstract meaning representation (AMR) of the input passages. Incorporating
such information into generation-based models is challenging due to the
heterogeneous nature of the natural language form prevalently used in
generation-based models and the structured form of AMRs. In this work, we study
strategies to incorporate AMR into generation-based EAE models. We propose
AMPERE, which generates AMR-aware prefixes for every layer of the generation
model. Thus, the prefix introduces AMR information to the generation-based EAE
model and then improves the generation. We also introduce an adjusted copy
mechanism to AMPERE to help overcome potential noises brought by the AMR graph.
Comprehensive experiments and analyses on ACE2005 and ERE datasets show that
AMPERE can get 4% - 10% absolute F1 score improvements with reduced training
data and it is in general powerful across different training sizes.","Paper accepted by ACL2023 as a main conference paper. The first two
  authors contribute equally. Code can be publicly accessible at
  https://github.com/PlusLabNLP/AMPERE",,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2305.16734v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.16734v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.16734v1,"{'id': 'http://arxiv.org/abs/2305.16734v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.16734v1', 'updated': '2023-05-26T08:38:25Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=8, tm_min=38, tm_sec=25, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2023-05-26T08:38:25Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=8, tm_min=38, tm_sec=25, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction\n  Model', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'AMPERE: AMR-Aware Prefix for Generation-Based Event Argument Extraction\n  Model'}, 'summary': 'Event argument extraction (EAE) identifies event arguments and their specific\nroles for a given event. Recent advancement in generation-based EAE models has\nshown great performance and generalizability over classification-based models.\nHowever, existing generation-based EAE models mostly focus on problem\nre-formulation and prompt design, without incorporating additional information\nthat has been shown to be effective for classification-based models, such as\nthe abstract meaning representation (AMR) of the input passages. Incorporating\nsuch information into generation-based models is challenging due to the\nheterogeneous nature of the natural language form prevalently used in\ngeneration-based models and the structured form of AMRs. In this work, we study\nstrategies to incorporate AMR into generation-based EAE models. We propose\nAMPERE, which generates AMR-aware prefixes for every layer of the generation\nmodel. Thus, the prefix introduces AMR information to the generation-based EAE\nmodel and then improves the generation. We also introduce an adjusted copy\nmechanism to AMPERE to help overcome potential noises brought by the AMR graph.\nComprehensive experiments and analyses on ACE2005 and ERE datasets show that\nAMPERE can get 4% - 10% absolute F1 score improvements with reduced training\ndata and it is in general powerful across different training sizes.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Event argument extraction (EAE) identifies event arguments and their specific\nroles for a given event. Recent advancement in generation-based EAE models has\nshown great performance and generalizability over classification-based models.\nHowever, existing generation-based EAE models mostly focus on problem\nre-formulation and prompt design, without incorporating additional information\nthat has been shown to be effective for classification-based models, such as\nthe abstract meaning representation (AMR) of the input passages. Incorporating\nsuch information into generation-based models is challenging due to the\nheterogeneous nature of the natural language form prevalently used in\ngeneration-based models and the structured form of AMRs. In this work, we study\nstrategies to incorporate AMR into generation-based EAE models. We propose\nAMPERE, which generates AMR-aware prefixes for every layer of the generation\nmodel. Thus, the prefix introduces AMR information to the generation-based EAE\nmodel and then improves the generation. We also introduce an adjusted copy\nmechanism to AMPERE to help overcome potential noises brought by the AMR graph.\nComprehensive experiments and analyses on ACE2005 and ERE datasets show that\nAMPERE can get 4% - 10% absolute F1 score improvements with reduced training\ndata and it is in general powerful across different training sizes.'}, 'authors': [{'name': 'I-Hung Hsu'}, {'name': 'Zhiyu Xie'}, {'name': 'Kuan-Hao Huang'}, {'name': 'Prem Natarajan'}, {'name': 'Nanyun Peng'}], 'author_detail': {'name': 'Nanyun Peng'}, 'author': 'Nanyun Peng', 'arxiv_comment': 'Paper accepted by ACL2023 as a main conference paper. The first two\n  authors contribute equally. Code can be publicly accessible at\n  https://github.com/PlusLabNLP/AMPERE', 'links': [{'href': 'http://arxiv.org/abs/2305.16734v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.16734v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.16742v1,2023-05-26 08:44:42+00:00,2023-05-26 08:44:42+00:00,Parameter-Efficient Fine-Tuning without Introducing New Latency,"[arxiv.Result.Author('Baohao Liao'), arxiv.Result.Author('Yan Meng'), arxiv.Result.Author('Christof Monz')]","Parameter-efficient fine-tuning (PEFT) of pre-trained language models has
recently demonstrated remarkable achievements, effectively matching the
performance of full fine-tuning while utilizing significantly fewer trainable
parameters, and consequently addressing the storage and communication
constraints. Nonetheless, various PEFT methods are limited by their inherent
characteristics. In the case of sparse fine-tuning, which involves modifying
only a small subset of the existing parameters, the selection of fine-tuned
parameters is task- and domain-specific, making it unsuitable for federated
learning. On the other hand, PEFT methods with adding new parameters typically
introduce additional inference latency. In this paper, we demonstrate the
feasibility of generating a sparse mask in a task-agnostic manner, wherein all
downstream tasks share a common mask. Our approach, which relies solely on the
magnitude information of pre-trained parameters, surpasses existing
methodologies by a significant margin when evaluated on the GLUE benchmark.
Additionally, we introduce a novel adapter technique that directly applies the
adapter to pre-trained parameters instead of the hidden representation, thereby
achieving identical inference speed to that of full fine-tuning. Through
extensive experiments, our proposed method attains a new state-of-the-art
outcome in terms of both performance and storage efficiency, storing only 0.03%
parameters of full fine-tuning.",ACL2023 camera-ready version,,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2305.16742v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.16742v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.16742v1,"{'id': 'http://arxiv.org/abs/2305.16742v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.16742v1', 'updated': '2023-05-26T08:44:42Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=8, tm_min=44, tm_sec=42, tm_wday=4, tm_yday=146, tm_isdst=0), 'published': '2023-05-26T08:44:42Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=26, tm_hour=8, tm_min=44, tm_sec=42, tm_wday=4, tm_yday=146, tm_isdst=0), 'title': 'Parameter-Efficient Fine-Tuning without Introducing New Latency', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Parameter-Efficient Fine-Tuning without Introducing New Latency'}, 'summary': 'Parameter-efficient fine-tuning (PEFT) of pre-trained language models has\nrecently demonstrated remarkable achievements, effectively matching the\nperformance of full fine-tuning while utilizing significantly fewer trainable\nparameters, and consequently addressing the storage and communication\nconstraints. Nonetheless, various PEFT methods are limited by their inherent\ncharacteristics. In the case of sparse fine-tuning, which involves modifying\nonly a small subset of the existing parameters, the selection of fine-tuned\nparameters is task- and domain-specific, making it unsuitable for federated\nlearning. On the other hand, PEFT methods with adding new parameters typically\nintroduce additional inference latency. In this paper, we demonstrate the\nfeasibility of generating a sparse mask in a task-agnostic manner, wherein all\ndownstream tasks share a common mask. Our approach, which relies solely on the\nmagnitude information of pre-trained parameters, surpasses existing\nmethodologies by a significant margin when evaluated on the GLUE benchmark.\nAdditionally, we introduce a novel adapter technique that directly applies the\nadapter to pre-trained parameters instead of the hidden representation, thereby\nachieving identical inference speed to that of full fine-tuning. Through\nextensive experiments, our proposed method attains a new state-of-the-art\noutcome in terms of both performance and storage efficiency, storing only 0.03%\nparameters of full fine-tuning.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Parameter-efficient fine-tuning (PEFT) of pre-trained language models has\nrecently demonstrated remarkable achievements, effectively matching the\nperformance of full fine-tuning while utilizing significantly fewer trainable\nparameters, and consequently addressing the storage and communication\nconstraints. Nonetheless, various PEFT methods are limited by their inherent\ncharacteristics. In the case of sparse fine-tuning, which involves modifying\nonly a small subset of the existing parameters, the selection of fine-tuned\nparameters is task- and domain-specific, making it unsuitable for federated\nlearning. On the other hand, PEFT methods with adding new parameters typically\nintroduce additional inference latency. In this paper, we demonstrate the\nfeasibility of generating a sparse mask in a task-agnostic manner, wherein all\ndownstream tasks share a common mask. Our approach, which relies solely on the\nmagnitude information of pre-trained parameters, surpasses existing\nmethodologies by a significant margin when evaluated on the GLUE benchmark.\nAdditionally, we introduce a novel adapter technique that directly applies the\nadapter to pre-trained parameters instead of the hidden representation, thereby\nachieving identical inference speed to that of full fine-tuning. Through\nextensive experiments, our proposed method attains a new state-of-the-art\noutcome in terms of both performance and storage efficiency, storing only 0.03%\nparameters of full fine-tuning.'}, 'authors': [{'name': 'Baohao Liao'}, {'name': 'Yan Meng'}, {'name': 'Christof Monz'}], 'author_detail': {'name': 'Christof Monz'}, 'author': 'Christof Monz', 'arxiv_comment': 'ACL2023 camera-ready version', 'links': [{'href': 'http://arxiv.org/abs/2305.16742v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.16742v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2305.18153v2,2023-05-30 15:14:06+00:00,2023-05-29 15:30:13+00:00,Do Large Language Models Know What They Don't Know?,"[arxiv.Result.Author('Zhangyue Yin'), arxiv.Result.Author('Qiushi Sun'), arxiv.Result.Author('Qipeng Guo'), arxiv.Result.Author('Jiawen Wu'), arxiv.Result.Author('Xipeng Qiu'), arxiv.Result.Author('Xuanjing Huang')]","Large language models (LLMs) have a wealth of knowledge that allows them to
excel in various Natural Language Processing (NLP) tasks. Current research
focuses on enhancing their performance within their existing knowledge. Despite
their vast knowledge, LLMs are still limited by the amount of information they
can accommodate and comprehend. Therefore, the ability to understand their own
limitations on the unknows, referred to as self-knowledge, is of paramount
importance. This study aims to evaluate LLMs' self-knowledge by assessing their
ability to identify unanswerable or unknowable questions. We introduce an
automated methodology to detect uncertainty in the responses of these models,
providing a novel measure of their self-knowledge. We further introduce a
unique dataset, SelfAware, consisting of unanswerable questions from five
diverse categories and their answerable counterparts. Our extensive analysis,
involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an
intrinsic capacity for self-knowledge within these models. Moreover, we
demonstrate that in-context learning and instruction tuning can further enhance
this self-knowledge. Despite this promising insight, our findings also
highlight a considerable gap between the capabilities of these models and human
proficiency in recognizing the limits of their knowledge.","10 pages, 9 figures, accepted by Findings of ACL2023",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2305.18153v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2305.18153v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2305.18153v2,"{'id': 'http://arxiv.org/abs/2305.18153v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2305.18153v2', 'updated': '2023-05-30T15:14:06Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=30, tm_hour=15, tm_min=14, tm_sec=6, tm_wday=1, tm_yday=150, tm_isdst=0), 'published': '2023-05-29T15:30:13Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=5, tm_mday=29, tm_hour=15, tm_min=30, tm_sec=13, tm_wday=0, tm_yday=149, tm_isdst=0), 'title': ""Do Large Language Models Know What They Don't Know?"", 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Do Large Language Models Know What They Don't Know?""}, 'summary': ""Large language models (LLMs) have a wealth of knowledge that allows them to\nexcel in various Natural Language Processing (NLP) tasks. Current research\nfocuses on enhancing their performance within their existing knowledge. Despite\ntheir vast knowledge, LLMs are still limited by the amount of information they\ncan accommodate and comprehend. Therefore, the ability to understand their own\nlimitations on the unknows, referred to as self-knowledge, is of paramount\nimportance. This study aims to evaluate LLMs' self-knowledge by assessing their\nability to identify unanswerable or unknowable questions. We introduce an\nautomated methodology to detect uncertainty in the responses of these models,\nproviding a novel measure of their self-knowledge. We further introduce a\nunique dataset, SelfAware, consisting of unanswerable questions from five\ndiverse categories and their answerable counterparts. Our extensive analysis,\ninvolving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an\nintrinsic capacity for self-knowledge within these models. Moreover, we\ndemonstrate that in-context learning and instruction tuning can further enhance\nthis self-knowledge. Despite this promising insight, our findings also\nhighlight a considerable gap between the capabilities of these models and human\nproficiency in recognizing the limits of their knowledge."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Large language models (LLMs) have a wealth of knowledge that allows them to\nexcel in various Natural Language Processing (NLP) tasks. Current research\nfocuses on enhancing their performance within their existing knowledge. Despite\ntheir vast knowledge, LLMs are still limited by the amount of information they\ncan accommodate and comprehend. Therefore, the ability to understand their own\nlimitations on the unknows, referred to as self-knowledge, is of paramount\nimportance. This study aims to evaluate LLMs' self-knowledge by assessing their\nability to identify unanswerable or unknowable questions. We introduce an\nautomated methodology to detect uncertainty in the responses of these models,\nproviding a novel measure of their self-knowledge. We further introduce a\nunique dataset, SelfAware, consisting of unanswerable questions from five\ndiverse categories and their answerable counterparts. Our extensive analysis,\ninvolving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an\nintrinsic capacity for self-knowledge within these models. Moreover, we\ndemonstrate that in-context learning and instruction tuning can further enhance\nthis self-knowledge. Despite this promising insight, our findings also\nhighlight a considerable gap between the capabilities of these models and human\nproficiency in recognizing the limits of their knowledge.""}, 'authors': [{'name': 'Zhangyue Yin'}, {'name': 'Qiushi Sun'}, {'name': 'Qipeng Guo'}, {'name': 'Jiawen Wu'}, {'name': 'Xipeng Qiu'}, {'name': 'Xuanjing Huang'}], 'author_detail': {'name': 'Xuanjing Huang'}, 'author': 'Xuanjing Huang', 'arxiv_comment': '10 pages, 9 figures, accepted by Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2305.18153v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2305.18153v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.06410v1,2023-06-10 11:04:10+00:00,2023-06-10 11:04:10+00:00,OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality Alignment,"[arxiv.Result.Author('Xize Cheng'), arxiv.Result.Author('Tao Jin'), arxiv.Result.Author('Linjun Li'), arxiv.Result.Author('Wang Lin'), arxiv.Result.Author('Xinyu Duan'), arxiv.Result.Author('Zhou Zhao')]","Speech Recognition builds a bridge between the multimedia streaming
(audio-only, visual-only or audio-visual) and the corresponding text
transcription. However, when training the specific model of new domain, it
often gets stuck in the lack of new-domain utterances, especially the labeled
visual utterances. To break through this restriction, we attempt to achieve
zero-shot modality transfer by maintaining the multi-modality alignment in
phoneme space learned with unlabeled multimedia utterances in the high resource
domain during the pre-training \cite{shi2022learning}, and propose a training
system Open-modality Speech Recognition (\textbf{OpenSR}) that enables the
models trained on a single modality (e.g., audio-only) applicable to more
modalities (e.g., visual-only and audio-visual). Furthermore, we employ a
cluster-based prompt tuning strategy to handle the domain shift for the
scenarios with only common words in the new domain utterances. We demonstrate
that OpenSR enables modality transfer from one to any in three different
settings (zero-, few- and full-shot), and achieves highly competitive zero-shot
performance compared to the existing few-shot and full-shot lip-reading
methods. To the best of our knowledge, OpenSR achieves the state-of-the-art
performance of word error rate in LRS2 on audio-visual speech recognition and
lip-reading with 2.7\% and 25.0\%, respectively. The code and demo are
available at https://github.com/Exgc/OpenSR.",Accepted to ACL2023 (Oral),,,cs.CL,"['cs.CL', 'cs.CV']","[arxiv.Result.Link('http://arxiv.org/abs/2306.06410v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.06410v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.06410v1,"{'id': 'http://arxiv.org/abs/2306.06410v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.06410v1', 'updated': '2023-06-10T11:04:10Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=10, tm_hour=11, tm_min=4, tm_sec=10, tm_wday=5, tm_yday=161, tm_isdst=0), 'published': '2023-06-10T11:04:10Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=10, tm_hour=11, tm_min=4, tm_sec=10, tm_wday=5, tm_yday=161, tm_isdst=0), 'title': 'OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality\n  Alignment', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'OpenSR: Open-Modality Speech Recognition via Maintaining Multi-Modality\n  Alignment'}, 'summary': 'Speech Recognition builds a bridge between the multimedia streaming\n(audio-only, visual-only or audio-visual) and the corresponding text\ntranscription. However, when training the specific model of new domain, it\noften gets stuck in the lack of new-domain utterances, especially the labeled\nvisual utterances. To break through this restriction, we attempt to achieve\nzero-shot modality transfer by maintaining the multi-modality alignment in\nphoneme space learned with unlabeled multimedia utterances in the high resource\ndomain during the pre-training \\cite{shi2022learning}, and propose a training\nsystem Open-modality Speech Recognition (\\textbf{OpenSR}) that enables the\nmodels trained on a single modality (e.g., audio-only) applicable to more\nmodalities (e.g., visual-only and audio-visual). Furthermore, we employ a\ncluster-based prompt tuning strategy to handle the domain shift for the\nscenarios with only common words in the new domain utterances. We demonstrate\nthat OpenSR enables modality transfer from one to any in three different\nsettings (zero-, few- and full-shot), and achieves highly competitive zero-shot\nperformance compared to the existing few-shot and full-shot lip-reading\nmethods. To the best of our knowledge, OpenSR achieves the state-of-the-art\nperformance of word error rate in LRS2 on audio-visual speech recognition and\nlip-reading with 2.7\\% and 25.0\\%, respectively. The code and demo are\navailable at https://github.com/Exgc/OpenSR.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Speech Recognition builds a bridge between the multimedia streaming\n(audio-only, visual-only or audio-visual) and the corresponding text\ntranscription. However, when training the specific model of new domain, it\noften gets stuck in the lack of new-domain utterances, especially the labeled\nvisual utterances. To break through this restriction, we attempt to achieve\nzero-shot modality transfer by maintaining the multi-modality alignment in\nphoneme space learned with unlabeled multimedia utterances in the high resource\ndomain during the pre-training \\cite{shi2022learning}, and propose a training\nsystem Open-modality Speech Recognition (\\textbf{OpenSR}) that enables the\nmodels trained on a single modality (e.g., audio-only) applicable to more\nmodalities (e.g., visual-only and audio-visual). Furthermore, we employ a\ncluster-based prompt tuning strategy to handle the domain shift for the\nscenarios with only common words in the new domain utterances. We demonstrate\nthat OpenSR enables modality transfer from one to any in three different\nsettings (zero-, few- and full-shot), and achieves highly competitive zero-shot\nperformance compared to the existing few-shot and full-shot lip-reading\nmethods. To the best of our knowledge, OpenSR achieves the state-of-the-art\nperformance of word error rate in LRS2 on audio-visual speech recognition and\nlip-reading with 2.7\\% and 25.0\\%, respectively. The code and demo are\navailable at https://github.com/Exgc/OpenSR.'}, 'authors': [{'name': 'Xize Cheng'}, {'name': 'Tao Jin'}, {'name': 'Linjun Li'}, {'name': 'Wang Lin'}, {'name': 'Xinyu Duan'}, {'name': 'Zhou Zhao'}], 'author_detail': {'name': 'Zhou Zhao'}, 'author': 'Zhou Zhao', 'arxiv_comment': 'Accepted to ACL2023 (Oral)', 'links': [{'href': 'http://arxiv.org/abs/2306.06410v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.06410v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.06625v1,2023-06-11 08:53:27+00:00,2023-06-11 08:53:27+00:00,Are Intermediate Layers and Labels Really Necessary? A General Language Model Distillation Method,"[arxiv.Result.Author('Shicheng Tan'), arxiv.Result.Author('Weng Lam Tam'), arxiv.Result.Author('Yuanchun Wang'), arxiv.Result.Author('Wenwen Gong'), arxiv.Result.Author('Shu Zhao'), arxiv.Result.Author('Peng Zhang'), arxiv.Result.Author('Jie Tang')]","The large scale of pre-trained language models poses a challenge for their
deployment on various devices, with a growing emphasis on methods to compress
these models, particularly knowledge distillation. However, current knowledge
distillation methods rely on the model's intermediate layer features and the
golden labels (also called hard labels), which usually require aligned model
architecture and enough labeled data respectively. Moreover, the parameters of
vocabulary are usually neglected in existing methods. To address these
problems, we propose a general language model distillation (GLMD) method that
performs two-stage word prediction distillation and vocabulary compression,
which is simple and surprisingly shows extremely strong performance.
Specifically, GLMD supports more general application scenarios by eliminating
the constraints of dimension and structure between models and the need for
labeled datasets through the absence of intermediate layers and golden labels.
Meanwhile, based on the long-tailed distribution of word frequencies in the
data, GLMD designs a strategy of vocabulary compression through decreasing
vocabulary size instead of dimensionality. Experimental results show that our
method outperforms 25 state-of-the-art methods on the SuperGLUE benchmark,
achieving an average score that surpasses the best method by 3%.",Accepted to Findings of ACL2023,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2306.06625v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.06625v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.06625v1,"{'id': 'http://arxiv.org/abs/2306.06625v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.06625v1', 'updated': '2023-06-11T08:53:27Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=11, tm_hour=8, tm_min=53, tm_sec=27, tm_wday=6, tm_yday=162, tm_isdst=0), 'published': '2023-06-11T08:53:27Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=11, tm_hour=8, tm_min=53, tm_sec=27, tm_wday=6, tm_yday=162, tm_isdst=0), 'title': 'Are Intermediate Layers and Labels Really Necessary? A General Language\n  Model Distillation Method', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Are Intermediate Layers and Labels Really Necessary? A General Language\n  Model Distillation Method'}, 'summary': ""The large scale of pre-trained language models poses a challenge for their\ndeployment on various devices, with a growing emphasis on methods to compress\nthese models, particularly knowledge distillation. However, current knowledge\ndistillation methods rely on the model's intermediate layer features and the\ngolden labels (also called hard labels), which usually require aligned model\narchitecture and enough labeled data respectively. Moreover, the parameters of\nvocabulary are usually neglected in existing methods. To address these\nproblems, we propose a general language model distillation (GLMD) method that\nperforms two-stage word prediction distillation and vocabulary compression,\nwhich is simple and surprisingly shows extremely strong performance.\nSpecifically, GLMD supports more general application scenarios by eliminating\nthe constraints of dimension and structure between models and the need for\nlabeled datasets through the absence of intermediate layers and golden labels.\nMeanwhile, based on the long-tailed distribution of word frequencies in the\ndata, GLMD designs a strategy of vocabulary compression through decreasing\nvocabulary size instead of dimensionality. Experimental results show that our\nmethod outperforms 25 state-of-the-art methods on the SuperGLUE benchmark,\nachieving an average score that surpasses the best method by 3%."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""The large scale of pre-trained language models poses a challenge for their\ndeployment on various devices, with a growing emphasis on methods to compress\nthese models, particularly knowledge distillation. However, current knowledge\ndistillation methods rely on the model's intermediate layer features and the\ngolden labels (also called hard labels), which usually require aligned model\narchitecture and enough labeled data respectively. Moreover, the parameters of\nvocabulary are usually neglected in existing methods. To address these\nproblems, we propose a general language model distillation (GLMD) method that\nperforms two-stage word prediction distillation and vocabulary compression,\nwhich is simple and surprisingly shows extremely strong performance.\nSpecifically, GLMD supports more general application scenarios by eliminating\nthe constraints of dimension and structure between models and the need for\nlabeled datasets through the absence of intermediate layers and golden labels.\nMeanwhile, based on the long-tailed distribution of word frequencies in the\ndata, GLMD designs a strategy of vocabulary compression through decreasing\nvocabulary size instead of dimensionality. Experimental results show that our\nmethod outperforms 25 state-of-the-art methods on the SuperGLUE benchmark,\nachieving an average score that surpasses the best method by 3%.""}, 'authors': [{'name': 'Shicheng Tan'}, {'name': 'Weng Lam Tam'}, {'name': 'Yuanchun Wang'}, {'name': 'Wenwen Gong'}, {'name': 'Shu Zhao'}, {'name': 'Peng Zhang'}, {'name': 'Jie Tang'}], 'author_detail': {'name': 'Jie Tang'}, 'author': 'Jie Tang', 'arxiv_comment': 'Accepted to Findings of ACL2023', 'links': [{'href': 'http://arxiv.org/abs/2306.06625v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.06625v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2306.09064v1,2023-06-15 11:47:07+00:00,2023-06-15 11:47:07+00:00,Learning by Analogy: Diverse Questions Generation in Math Word Problem,"[arxiv.Result.Author('Zihao Zhou'), arxiv.Result.Author('Maizhen Ning'), arxiv.Result.Author('Qiufeng Wang'), arxiv.Result.Author('Jie Yao'), arxiv.Result.Author('Wei Wang'), arxiv.Result.Author('Xiaowei Huang'), arxiv.Result.Author('Kaizhu Huang')]","Solving math word problem (MWP) with AI techniques has recently made great
progress with the success of deep neural networks (DNN), but it is far from
being solved. We argue that the ability of learning by analogy is essential for
an MWP solver to better understand same problems which may typically be
formulated in diverse ways. However most existing works exploit the shortcut
learning to train MWP solvers simply based on samples with a single question.
In lack of diverse questions, these methods merely learn shallow heuristics. In
this paper, we make a first attempt to solve MWPs by generating diverse yet
consistent questions/equations. Given a typical MWP including the scenario
description, question, and equation (i.e., answer), we first generate multiple
consistent equations via a group of heuristic rules. We then feed them to a
question generator together with the scenario to obtain the corresponding
diverse questions, forming a new MWP with a variety of questions and equations.
Finally we engage a data filter to remove those unreasonable MWPs, keeping the
high-quality augmented ones. To evaluate the ability of learning by analogy for
an MWP solver, we generate a new MWP dataset (called DiverseMath23K) with
diverse questions by extending the current benchmark Math23K. Extensive
experimental results demonstrate that our proposed method can generate
high-quality diverse questions with corresponding equations, further leading to
performance improvement on Diverse-Math23K. The code and dataset is available
at: https://github.com/zhouzihao501/DiverseMWP",ACL2023 Findings,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2306.09064v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2306.09064v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2306.09064v1,"{'id': 'http://arxiv.org/abs/2306.09064v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2306.09064v1', 'updated': '2023-06-15T11:47:07Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=15, tm_hour=11, tm_min=47, tm_sec=7, tm_wday=3, tm_yday=166, tm_isdst=0), 'published': '2023-06-15T11:47:07Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=6, tm_mday=15, tm_hour=11, tm_min=47, tm_sec=7, tm_wday=3, tm_yday=166, tm_isdst=0), 'title': 'Learning by Analogy: Diverse Questions Generation in Math Word Problem', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Learning by Analogy: Diverse Questions Generation in Math Word Problem'}, 'summary': 'Solving math word problem (MWP) with AI techniques has recently made great\nprogress with the success of deep neural networks (DNN), but it is far from\nbeing solved. We argue that the ability of learning by analogy is essential for\nan MWP solver to better understand same problems which may typically be\nformulated in diverse ways. However most existing works exploit the shortcut\nlearning to train MWP solvers simply based on samples with a single question.\nIn lack of diverse questions, these methods merely learn shallow heuristics. In\nthis paper, we make a first attempt to solve MWPs by generating diverse yet\nconsistent questions/equations. Given a typical MWP including the scenario\ndescription, question, and equation (i.e., answer), we first generate multiple\nconsistent equations via a group of heuristic rules. We then feed them to a\nquestion generator together with the scenario to obtain the corresponding\ndiverse questions, forming a new MWP with a variety of questions and equations.\nFinally we engage a data filter to remove those unreasonable MWPs, keeping the\nhigh-quality augmented ones. To evaluate the ability of learning by analogy for\nan MWP solver, we generate a new MWP dataset (called DiverseMath23K) with\ndiverse questions by extending the current benchmark Math23K. Extensive\nexperimental results demonstrate that our proposed method can generate\nhigh-quality diverse questions with corresponding equations, further leading to\nperformance improvement on Diverse-Math23K. The code and dataset is available\nat: https://github.com/zhouzihao501/DiverseMWP', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Solving math word problem (MWP) with AI techniques has recently made great\nprogress with the success of deep neural networks (DNN), but it is far from\nbeing solved. We argue that the ability of learning by analogy is essential for\nan MWP solver to better understand same problems which may typically be\nformulated in diverse ways. However most existing works exploit the shortcut\nlearning to train MWP solvers simply based on samples with a single question.\nIn lack of diverse questions, these methods merely learn shallow heuristics. In\nthis paper, we make a first attempt to solve MWPs by generating diverse yet\nconsistent questions/equations. Given a typical MWP including the scenario\ndescription, question, and equation (i.e., answer), we first generate multiple\nconsistent equations via a group of heuristic rules. We then feed them to a\nquestion generator together with the scenario to obtain the corresponding\ndiverse questions, forming a new MWP with a variety of questions and equations.\nFinally we engage a data filter to remove those unreasonable MWPs, keeping the\nhigh-quality augmented ones. To evaluate the ability of learning by analogy for\nan MWP solver, we generate a new MWP dataset (called DiverseMath23K) with\ndiverse questions by extending the current benchmark Math23K. Extensive\nexperimental results demonstrate that our proposed method can generate\nhigh-quality diverse questions with corresponding equations, further leading to\nperformance improvement on Diverse-Math23K. The code and dataset is available\nat: https://github.com/zhouzihao501/DiverseMWP'}, 'authors': [{'name': 'Zihao Zhou'}, {'name': 'Maizhen Ning'}, {'name': 'Qiufeng Wang'}, {'name': 'Jie Yao'}, {'name': 'Wei Wang'}, {'name': 'Xiaowei Huang'}, {'name': 'Kaizhu Huang'}], 'author_detail': {'name': 'Kaizhu Huang'}, 'author': 'Kaizhu Huang', 'arxiv_comment': 'ACL2023 Findings', 'links': [{'href': 'http://arxiv.org/abs/2306.09064v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2306.09064v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2307.01595v1,2023-07-04 09:35:03+00:00,2023-07-04 09:35:03+00:00,"Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases","[arxiv.Result.Author('Yingji Li'), arxiv.Result.Author('Mengnan Du'), arxiv.Result.Author('Xin Wang'), arxiv.Result.Author('Ying Wang')]","As the representation capability of Pre-trained Language Models (PLMs)
improve, there is growing concern that they will inherit social biases from
unprocessed corpora. Most previous debiasing techniques used Counterfactual
Data Augmentation (CDA) to balance the training corpus. However, CDA slightly
modifies the original corpus, limiting the representation distance between
different demographic groups to a narrow range. As a result, the debiasing
model easily fits the differences between counterfactual pairs, which affects
its debiasing performance with limited text resources. In this paper, we
propose an adversarial training-inspired two-stage debiasing model using
Contrastive learning with Continuous Prompt Augmentation (named CCPA) to
mitigate social biases in PLMs' encoding. In the first stage, we propose a data
augmentation method based on continuous prompt tuning to push farther the
representation distance between sample pairs along different demographic
groups. In the second stage, we utilize contrastive learning to pull closer the
representation distance between the augmented sample pairs and then fine-tune
PLMs' parameters to get debiased encoding. Our approach guides the model to
achieve stronger debiasing performance by adding difficulty to the training
process. Extensive experiments show that CCPA outperforms baselines in terms of
debiasing performance. Meanwhile, experimental results on the GLUE benchmark
show that CCPA retains the language modeling capability of PLMs.",has been accepted as ACL2023 main conference long paper,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2307.01595v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2307.01595v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2307.01595v1,"{'id': 'http://arxiv.org/abs/2307.01595v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2307.01595v1', 'updated': '2023-07-04T09:35:03Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=4, tm_hour=9, tm_min=35, tm_sec=3, tm_wday=1, tm_yday=185, tm_isdst=0), 'published': '2023-07-04T09:35:03Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=7, tm_mday=4, tm_hour=9, tm_min=35, tm_sec=3, tm_wday=1, tm_yday=185, tm_isdst=0), 'title': 'Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A\n  Two-Stage Approach to Mitigate Social Biases', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A\n  Two-Stage Approach to Mitigate Social Biases'}, 'summary': ""As the representation capability of Pre-trained Language Models (PLMs)\nimprove, there is growing concern that they will inherit social biases from\nunprocessed corpora. Most previous debiasing techniques used Counterfactual\nData Augmentation (CDA) to balance the training corpus. However, CDA slightly\nmodifies the original corpus, limiting the representation distance between\ndifferent demographic groups to a narrow range. As a result, the debiasing\nmodel easily fits the differences between counterfactual pairs, which affects\nits debiasing performance with limited text resources. In this paper, we\npropose an adversarial training-inspired two-stage debiasing model using\nContrastive learning with Continuous Prompt Augmentation (named CCPA) to\nmitigate social biases in PLMs' encoding. In the first stage, we propose a data\naugmentation method based on continuous prompt tuning to push farther the\nrepresentation distance between sample pairs along different demographic\ngroups. In the second stage, we utilize contrastive learning to pull closer the\nrepresentation distance between the augmented sample pairs and then fine-tune\nPLMs' parameters to get debiased encoding. Our approach guides the model to\nachieve stronger debiasing performance by adding difficulty to the training\nprocess. Extensive experiments show that CCPA outperforms baselines in terms of\ndebiasing performance. Meanwhile, experimental results on the GLUE benchmark\nshow that CCPA retains the language modeling capability of PLMs."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""As the representation capability of Pre-trained Language Models (PLMs)\nimprove, there is growing concern that they will inherit social biases from\nunprocessed corpora. Most previous debiasing techniques used Counterfactual\nData Augmentation (CDA) to balance the training corpus. However, CDA slightly\nmodifies the original corpus, limiting the representation distance between\ndifferent demographic groups to a narrow range. As a result, the debiasing\nmodel easily fits the differences between counterfactual pairs, which affects\nits debiasing performance with limited text resources. In this paper, we\npropose an adversarial training-inspired two-stage debiasing model using\nContrastive learning with Continuous Prompt Augmentation (named CCPA) to\nmitigate social biases in PLMs' encoding. In the first stage, we propose a data\naugmentation method based on continuous prompt tuning to push farther the\nrepresentation distance between sample pairs along different demographic\ngroups. In the second stage, we utilize contrastive learning to pull closer the\nrepresentation distance between the augmented sample pairs and then fine-tune\nPLMs' parameters to get debiased encoding. Our approach guides the model to\nachieve stronger debiasing performance by adding difficulty to the training\nprocess. Extensive experiments show that CCPA outperforms baselines in terms of\ndebiasing performance. Meanwhile, experimental results on the GLUE benchmark\nshow that CCPA retains the language modeling capability of PLMs.""}, 'authors': [{'name': 'Yingji Li'}, {'name': 'Mengnan Du'}, {'name': 'Xin Wang'}, {'name': 'Ying Wang'}], 'author_detail': {'name': 'Ying Wang'}, 'author': 'Ying Wang', 'arxiv_comment': 'has been accepted as ACL2023 main conference long paper', 'links': [{'href': 'http://arxiv.org/abs/2307.01595v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2307.01595v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2310.05991v3,2023-10-20 02:20:20+00:00,2023-10-08 11:29:10+00:00,Enhancing Document-level Event Argument Extraction with Contextual Clues and Role Relevance,"[arxiv.Result.Author('Wanlong Liu'), arxiv.Result.Author('Shaohuan Cheng'), arxiv.Result.Author('Dingyi Zeng'), arxiv.Result.Author('Hong Qu')]","Document-level event argument extraction poses new challenges of long input
and cross-sentence inference compared to its sentence-level counterpart.
However, most prior works focus on capturing the relations between candidate
arguments and the event trigger in each event, ignoring two crucial points: a)
non-argument contextual clue information; b) the relevance among argument
roles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling
and latent Role Guidance) model, which contains two novel and effective modules
for the above problem. The Span-Trigger-based Contextual Pooling(STCP)
adaptively selects and aggregates the information of non-argument clue words
based on the context attention weights of specific argument-trigger pairs from
pre-trained model. The Role-based Latent Information Guidance (RLIG) module
constructs latent role representations, makes them interact through
role-interactive encoding to capture semantic relevance, and merges them into
candidate arguments. Both STCP and RLIG introduce no more than 1% new
parameters compared with the base model and can be easily applied to other
event extraction models, which are compact and transplantable. Experiments on
two public datasets show that our SCPRG outperforms previous state-of-the-art
methods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents
respectively. Further analyses illustrate the interpretability of our model.","Findings of ACL2023, correct some mistakes. arXiv admin note: text
  overlap with arXiv:2310.05116",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2310.05991v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2310.05991v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2310.05991v3,"{'id': 'http://arxiv.org/abs/2310.05991v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2310.05991v3', 'updated': '2023-10-20T02:20:20Z', 'updated_parsed': time.struct_time(tm_year=2023, tm_mon=10, tm_mday=20, tm_hour=2, tm_min=20, tm_sec=20, tm_wday=4, tm_yday=293, tm_isdst=0), 'published': '2023-10-08T11:29:10Z', 'published_parsed': time.struct_time(tm_year=2023, tm_mon=10, tm_mday=8, tm_hour=11, tm_min=29, tm_sec=10, tm_wday=6, tm_yday=281, tm_isdst=0), 'title': 'Enhancing Document-level Event Argument Extraction with Contextual Clues\n  and Role Relevance', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Enhancing Document-level Event Argument Extraction with Contextual Clues\n  and Role Relevance'}, 'summary': 'Document-level event argument extraction poses new challenges of long input\nand cross-sentence inference compared to its sentence-level counterpart.\nHowever, most prior works focus on capturing the relations between candidate\narguments and the event trigger in each event, ignoring two crucial points: a)\nnon-argument contextual clue information; b) the relevance among argument\nroles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling\nand latent Role Guidance) model, which contains two novel and effective modules\nfor the above problem. The Span-Trigger-based Contextual Pooling(STCP)\nadaptively selects and aggregates the information of non-argument clue words\nbased on the context attention weights of specific argument-trigger pairs from\npre-trained model. The Role-based Latent Information Guidance (RLIG) module\nconstructs latent role representations, makes them interact through\nrole-interactive encoding to capture semantic relevance, and merges them into\ncandidate arguments. Both STCP and RLIG introduce no more than 1% new\nparameters compared with the base model and can be easily applied to other\nevent extraction models, which are compact and transplantable. Experiments on\ntwo public datasets show that our SCPRG outperforms previous state-of-the-art\nmethods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents\nrespectively. Further analyses illustrate the interpretability of our model.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Document-level event argument extraction poses new challenges of long input\nand cross-sentence inference compared to its sentence-level counterpart.\nHowever, most prior works focus on capturing the relations between candidate\narguments and the event trigger in each event, ignoring two crucial points: a)\nnon-argument contextual clue information; b) the relevance among argument\nroles. In this paper, we propose a SCPRG (Span-trigger-based Contextual Pooling\nand latent Role Guidance) model, which contains two novel and effective modules\nfor the above problem. The Span-Trigger-based Contextual Pooling(STCP)\nadaptively selects and aggregates the information of non-argument clue words\nbased on the context attention weights of specific argument-trigger pairs from\npre-trained model. The Role-based Latent Information Guidance (RLIG) module\nconstructs latent role representations, makes them interact through\nrole-interactive encoding to capture semantic relevance, and merges them into\ncandidate arguments. Both STCP and RLIG introduce no more than 1% new\nparameters compared with the base model and can be easily applied to other\nevent extraction models, which are compact and transplantable. Experiments on\ntwo public datasets show that our SCPRG outperforms previous state-of-the-art\nmethods, with 1.13 F1 and 2.64 F1 improvements on RAMS and WikiEvents\nrespectively. Further analyses illustrate the interpretability of our model.'}, 'authors': [{'name': 'Wanlong Liu'}, {'name': 'Shaohuan Cheng'}, {'name': 'Dingyi Zeng'}, {'name': 'Hong Qu'}], 'author_detail': {'name': 'Hong Qu'}, 'author': 'Hong Qu', 'arxiv_comment': 'Findings of ACL2023, correct some mistakes. arXiv admin note: text\n  overlap with arXiv:2310.05116', 'links': [{'href': 'http://arxiv.org/abs/2310.05991v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2310.05991v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
