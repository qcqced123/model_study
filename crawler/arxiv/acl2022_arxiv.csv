entry_id,updated,published,title,authors,summary,comment,journal_ref,doi,primary_category,categories,links,pdf_url,_raw
http://arxiv.org/abs/2204.06239v1,2022-04-13 08:16:03+00:00,2022-04-13 08:16:03+00:00,Can Question Rewriting Help Conversational Question Answering?,"[arxiv.Result.Author('Etsuko Ishii'), arxiv.Result.Author('Yan Xu'), arxiv.Result.Author('Samuel Cahyawijaya'), arxiv.Result.Author('Bryan Wilie')]","Question rewriting (QR) is a subtask of conversational question answering
(CQA) aiming to ease the challenges of understanding dependencies among
dialogue history by reformulating questions in a self-contained form. Despite
seeming plausible, little evidence is available to justify QR as a mitigation
method for CQA. To verify the effectiveness of QR in CQA, we investigate a
reinforcement learning approach that integrates QR and CQA tasks and does not
require corresponding QR datasets for targeted CQA. We find, however, that the
RL method is on par with the end-to-end baseline. We provide an analysis of the
failure and describe the difficulty of exploiting QR for CQA.","Accepted at Workshop on Insights from Negative Results in NLP at
  ACL2022",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2204.06239v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.06239v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2204.06239v1,"{'id': 'http://arxiv.org/abs/2204.06239v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2204.06239v1', 'updated': '2022-04-13T08:16:03Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=13, tm_hour=8, tm_min=16, tm_sec=3, tm_wday=2, tm_yday=103, tm_isdst=0), 'published': '2022-04-13T08:16:03Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=13, tm_hour=8, tm_min=16, tm_sec=3, tm_wday=2, tm_yday=103, tm_isdst=0), 'title': 'Can Question Rewriting Help Conversational Question Answering?', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Can Question Rewriting Help Conversational Question Answering?'}, 'summary': 'Question rewriting (QR) is a subtask of conversational question answering\n(CQA) aiming to ease the challenges of understanding dependencies among\ndialogue history by reformulating questions in a self-contained form. Despite\nseeming plausible, little evidence is available to justify QR as a mitigation\nmethod for CQA. To verify the effectiveness of QR in CQA, we investigate a\nreinforcement learning approach that integrates QR and CQA tasks and does not\nrequire corresponding QR datasets for targeted CQA. We find, however, that the\nRL method is on par with the end-to-end baseline. We provide an analysis of the\nfailure and describe the difficulty of exploiting QR for CQA.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Question rewriting (QR) is a subtask of conversational question answering\n(CQA) aiming to ease the challenges of understanding dependencies among\ndialogue history by reformulating questions in a self-contained form. Despite\nseeming plausible, little evidence is available to justify QR as a mitigation\nmethod for CQA. To verify the effectiveness of QR in CQA, we investigate a\nreinforcement learning approach that integrates QR and CQA tasks and does not\nrequire corresponding QR datasets for targeted CQA. We find, however, that the\nRL method is on par with the end-to-end baseline. We provide an analysis of the\nfailure and describe the difficulty of exploiting QR for CQA.'}, 'authors': [{'name': 'Etsuko Ishii'}, {'name': 'Yan Xu'}, {'name': 'Samuel Cahyawijaya'}, {'name': 'Bryan Wilie'}], 'author_detail': {'name': 'Bryan Wilie'}, 'author': 'Bryan Wilie', 'arxiv_comment': 'Accepted at Workshop on Insights from Negative Results in NLP at\n  ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2204.06239v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2204.06239v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2205.13108v1,2022-05-26 02:18:12+00:00,2022-05-26 02:18:12+00:00,Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV Conversion,"[arxiv.Result.Author('Seongmin Park'), arxiv.Result.Author('Jihwa Lee')]","We advance the state-of-the-art in unsupervised abstractive dialogue
summarization by utilizing multi-sentence compression graphs. Starting from
well-founded assumptions about word graphs, we present simple but reliable
path-reranking and topic segmentation schemes. Robustness of our method is
demonstrated on datasets across multiple domains, including meetings,
interviews, movie scripts, and day-to-day conversations. We also identify
possible avenues to augment our heuristic-based system with deep learning. We
open-source our code, to provide a strong, reproducible baseline for future
research into unsupervised dialogue summarization.",WIT Workshop @ ACL2022,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2205.13108v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.13108v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2205.13108v1,"{'id': 'http://arxiv.org/abs/2205.13108v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2205.13108v1', 'updated': '2022-05-26T02:18:12Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=26, tm_hour=2, tm_min=18, tm_sec=12, tm_wday=3, tm_yday=146, tm_isdst=0), 'published': '2022-05-26T02:18:12Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=26, tm_hour=2, tm_min=18, tm_sec=12, tm_wday=3, tm_yday=146, tm_isdst=0), 'title': 'Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV\n  Conversion', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV\n  Conversion'}, 'summary': 'We advance the state-of-the-art in unsupervised abstractive dialogue\nsummarization by utilizing multi-sentence compression graphs. Starting from\nwell-founded assumptions about word graphs, we present simple but reliable\npath-reranking and topic segmentation schemes. Robustness of our method is\ndemonstrated on datasets across multiple domains, including meetings,\ninterviews, movie scripts, and day-to-day conversations. We also identify\npossible avenues to augment our heuristic-based system with deep learning. We\nopen-source our code, to provide a strong, reproducible baseline for future\nresearch into unsupervised dialogue summarization.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We advance the state-of-the-art in unsupervised abstractive dialogue\nsummarization by utilizing multi-sentence compression graphs. Starting from\nwell-founded assumptions about word graphs, we present simple but reliable\npath-reranking and topic segmentation schemes. Robustness of our method is\ndemonstrated on datasets across multiple domains, including meetings,\ninterviews, movie scripts, and day-to-day conversations. We also identify\npossible avenues to augment our heuristic-based system with deep learning. We\nopen-source our code, to provide a strong, reproducible baseline for future\nresearch into unsupervised dialogue summarization.'}, 'authors': [{'name': 'Seongmin Park'}, {'name': 'Jihwa Lee'}], 'author_detail': {'name': 'Jihwa Lee'}, 'author': 'Jihwa Lee', 'arxiv_comment': 'WIT Workshop @ ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2205.13108v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.13108v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2104.07554v2,2022-03-07 14:00:40+00:00,2021-04-15 16:08:43+00:00,Zero-Shot Cross-lingual Semantic Parsing,"[arxiv.Result.Author('Tom Sherborne'), arxiv.Result.Author('Mirella Lapata')]","Recent work in cross-lingual semantic parsing has successfully applied
machine translation to localize parsers to new languages. However, these
advances assume access to high-quality machine translation systems and word
alignment tools. We remove these assumptions and study cross-lingual semantic
parsing as a zero-shot problem, without parallel data (i.e., utterance-logical
form pairs) for new languages. We propose a multi-task encoder-decoder model to
transfer parsing knowledge to additional languages using only English-logical
form paired data and in-domain natural language corpora in each new language.
Our model encourages language-agnostic encodings by jointly optimizing for
logical-form generation with auxiliary objectives designed for cross-lingual
latent representation alignment. Our parser performs significantly above
translation-based baselines and, in some cases, competes with the supervised
upper-bound.","Accepted to ACL2022 Main Conference. 19 pages, 3 figures, 12 tables",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2104.07554v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2104.07554v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2104.07554v2,"{'id': 'http://arxiv.org/abs/2104.07554v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2104.07554v2', 'updated': '2022-03-07T14:00:40Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=7, tm_hour=14, tm_min=0, tm_sec=40, tm_wday=0, tm_yday=66, tm_isdst=0), 'published': '2021-04-15T16:08:43Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=4, tm_mday=15, tm_hour=16, tm_min=8, tm_sec=43, tm_wday=3, tm_yday=105, tm_isdst=0), 'title': 'Zero-Shot Cross-lingual Semantic Parsing', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Zero-Shot Cross-lingual Semantic Parsing'}, 'summary': 'Recent work in cross-lingual semantic parsing has successfully applied\nmachine translation to localize parsers to new languages. However, these\nadvances assume access to high-quality machine translation systems and word\nalignment tools. We remove these assumptions and study cross-lingual semantic\nparsing as a zero-shot problem, without parallel data (i.e., utterance-logical\nform pairs) for new languages. We propose a multi-task encoder-decoder model to\ntransfer parsing knowledge to additional languages using only English-logical\nform paired data and in-domain natural language corpora in each new language.\nOur model encourages language-agnostic encodings by jointly optimizing for\nlogical-form generation with auxiliary objectives designed for cross-lingual\nlatent representation alignment. Our parser performs significantly above\ntranslation-based baselines and, in some cases, competes with the supervised\nupper-bound.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Recent work in cross-lingual semantic parsing has successfully applied\nmachine translation to localize parsers to new languages. However, these\nadvances assume access to high-quality machine translation systems and word\nalignment tools. We remove these assumptions and study cross-lingual semantic\nparsing as a zero-shot problem, without parallel data (i.e., utterance-logical\nform pairs) for new languages. We propose a multi-task encoder-decoder model to\ntransfer parsing knowledge to additional languages using only English-logical\nform paired data and in-domain natural language corpora in each new language.\nOur model encourages language-agnostic encodings by jointly optimizing for\nlogical-form generation with auxiliary objectives designed for cross-lingual\nlatent representation alignment. Our parser performs significantly above\ntranslation-based baselines and, in some cases, competes with the supervised\nupper-bound.'}, 'authors': [{'name': 'Tom Sherborne'}, {'name': 'Mirella Lapata'}], 'author_detail': {'name': 'Mirella Lapata'}, 'author': 'Mirella Lapata', 'arxiv_comment': 'Accepted to ACL2022 Main Conference. 19 pages, 3 figures, 12 tables', 'links': [{'href': 'http://arxiv.org/abs/2104.07554v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.07554v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2104.08704v2,2022-04-02 15:23:44+00:00,2021-04-18 04:09:48+00:00,A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation,"[arxiv.Result.Author('Tianyu Liu'), arxiv.Result.Author('Yizhe Zhang'), arxiv.Result.Author('Chris Brockett'), arxiv.Result.Author('Yi Mao'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Weizhu Chen'), arxiv.Result.Author('Bill Dolan')]","Large pretrained generative models like GPT-3 often suffer from hallucinating
non-existent or incorrect content, which undermines their potential merits in
real applications. Existing work usually attempts to detect these
hallucinations based on a corresponding oracle reference at a sentence or
document level. However ground-truth references may not be readily available
for many free-form text generation applications, and sentence- or
document-level detection may fail to provide the fine-grained signals that
would prevent fallacious content in real time. As a first step to addressing
these issues, we propose a novel token-level, reference-free hallucination
detection task and an associated annotated dataset named HaDes (HAllucination
DEtection dataSet). To create this dataset, we first perturb a large number of
text segments extracted from English language Wikipedia, and then verify these
with crowd-sourced annotations. To mitigate label imbalance during annotation,
we utilize an iterative model-in-loop strategy. We conduct comprehensive data
analyses and create multiple baseline models.",Accepted by ACL2022 main conference,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2104.08704v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2104.08704v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2104.08704v2,"{'id': 'http://arxiv.org/abs/2104.08704v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2104.08704v2', 'updated': '2022-04-02T15:23:44Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=2, tm_hour=15, tm_min=23, tm_sec=44, tm_wday=5, tm_yday=92, tm_isdst=0), 'published': '2021-04-18T04:09:48Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=4, tm_mday=18, tm_hour=4, tm_min=9, tm_sec=48, tm_wday=6, tm_yday=108, tm_isdst=0), 'title': 'A Token-level Reference-free Hallucination Detection Benchmark for\n  Free-form Text Generation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A Token-level Reference-free Hallucination Detection Benchmark for\n  Free-form Text Generation'}, 'summary': 'Large pretrained generative models like GPT-3 often suffer from hallucinating\nnon-existent or incorrect content, which undermines their potential merits in\nreal applications. Existing work usually attempts to detect these\nhallucinations based on a corresponding oracle reference at a sentence or\ndocument level. However ground-truth references may not be readily available\nfor many free-form text generation applications, and sentence- or\ndocument-level detection may fail to provide the fine-grained signals that\nwould prevent fallacious content in real time. As a first step to addressing\nthese issues, we propose a novel token-level, reference-free hallucination\ndetection task and an associated annotated dataset named HaDes (HAllucination\nDEtection dataSet). To create this dataset, we first perturb a large number of\ntext segments extracted from English language Wikipedia, and then verify these\nwith crowd-sourced annotations. To mitigate label imbalance during annotation,\nwe utilize an iterative model-in-loop strategy. We conduct comprehensive data\nanalyses and create multiple baseline models.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Large pretrained generative models like GPT-3 often suffer from hallucinating\nnon-existent or incorrect content, which undermines their potential merits in\nreal applications. Existing work usually attempts to detect these\nhallucinations based on a corresponding oracle reference at a sentence or\ndocument level. However ground-truth references may not be readily available\nfor many free-form text generation applications, and sentence- or\ndocument-level detection may fail to provide the fine-grained signals that\nwould prevent fallacious content in real time. As a first step to addressing\nthese issues, we propose a novel token-level, reference-free hallucination\ndetection task and an associated annotated dataset named HaDes (HAllucination\nDEtection dataSet). To create this dataset, we first perturb a large number of\ntext segments extracted from English language Wikipedia, and then verify these\nwith crowd-sourced annotations. To mitigate label imbalance during annotation,\nwe utilize an iterative model-in-loop strategy. We conduct comprehensive data\nanalyses and create multiple baseline models.'}, 'authors': [{'name': 'Tianyu Liu'}, {'name': 'Yizhe Zhang'}, {'name': 'Chris Brockett'}, {'name': 'Yi Mao'}, {'name': 'Zhifang Sui'}, {'name': 'Weizhu Chen'}, {'name': 'Bill Dolan'}], 'author_detail': {'name': 'Bill Dolan'}, 'author': 'Bill Dolan', 'arxiv_comment': 'Accepted by ACL2022 main conference', 'links': [{'href': 'http://arxiv.org/abs/2104.08704v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.08704v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2105.00828v2,2022-03-15 01:14:16+00:00,2021-04-16 18:53:19+00:00,Memorisation versus Generalisation in Pre-trained Language Models,"[arxiv.Result.Author('Michael Tänzer'), arxiv.Result.Author('Sebastian Ruder'), arxiv.Result.Author('Marek Rei')]","State-of-the-art pre-trained language models have been shown to memorise
facts and perform well with limited amounts of training data. To gain a better
understanding of how these models learn, we study their generalisation and
memorisation capabilities in noisy and low-resource scenarios. We find that the
training of these models is almost unaffected by label noise and that it is
possible to reach near-optimal results even on extremely noisy datasets.
However, our experiments also show that they mainly learn from high-frequency
patterns and largely fail when tested on low-resource tasks such as few-shot
learning and rare entity recognition. To mitigate such limitations, we propose
an extension based on prototypical networks that improves performance in
low-resource named entity recognition tasks.","15 pages, 25 figures. To be published in ACL2022",,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2105.00828v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2105.00828v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2105.00828v2,"{'id': 'http://arxiv.org/abs/2105.00828v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2105.00828v2', 'updated': '2022-03-15T01:14:16Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=15, tm_hour=1, tm_min=14, tm_sec=16, tm_wday=1, tm_yday=74, tm_isdst=0), 'published': '2021-04-16T18:53:19Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=4, tm_mday=16, tm_hour=18, tm_min=53, tm_sec=19, tm_wday=4, tm_yday=106, tm_isdst=0), 'title': 'Memorisation versus Generalisation in Pre-trained Language Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Memorisation versus Generalisation in Pre-trained Language Models'}, 'summary': 'State-of-the-art pre-trained language models have been shown to memorise\nfacts and perform well with limited amounts of training data. To gain a better\nunderstanding of how these models learn, we study their generalisation and\nmemorisation capabilities in noisy and low-resource scenarios. We find that the\ntraining of these models is almost unaffected by label noise and that it is\npossible to reach near-optimal results even on extremely noisy datasets.\nHowever, our experiments also show that they mainly learn from high-frequency\npatterns and largely fail when tested on low-resource tasks such as few-shot\nlearning and rare entity recognition. To mitigate such limitations, we propose\nan extension based on prototypical networks that improves performance in\nlow-resource named entity recognition tasks.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'State-of-the-art pre-trained language models have been shown to memorise\nfacts and perform well with limited amounts of training data. To gain a better\nunderstanding of how these models learn, we study their generalisation and\nmemorisation capabilities in noisy and low-resource scenarios. We find that the\ntraining of these models is almost unaffected by label noise and that it is\npossible to reach near-optimal results even on extremely noisy datasets.\nHowever, our experiments also show that they mainly learn from high-frequency\npatterns and largely fail when tested on low-resource tasks such as few-shot\nlearning and rare entity recognition. To mitigate such limitations, we propose\nan extension based on prototypical networks that improves performance in\nlow-resource named entity recognition tasks.'}, 'authors': [{'name': 'Michael Tänzer'}, {'name': 'Sebastian Ruder'}, {'name': 'Marek Rei'}], 'author_detail': {'name': 'Marek Rei'}, 'author': 'Marek Rei', 'arxiv_comment': '15 pages, 25 figures. To be published in ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2105.00828v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.00828v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2107.05377v2,2022-03-09 03:56:22+00:00,2021-07-12 12:42:39+00:00,A Flexible Multi-Task Model for BERT Serving,"[arxiv.Result.Author('Tianwen Wei'), arxiv.Result.Author('Jianwei Qi'), arxiv.Result.Author('Shenghuan He')]","In this demonstration, we present an efficient BERT-based multi-task (MT)
framework that is particularly suitable for iterative and incremental
development of the tasks. The proposed framework is based on the idea of
partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the
other layers frozen. For each task, we train independently a single-task (ST)
model using partial fine-tuning. Then we compress the task-specific layers in
each ST model using knowledge distillation. Those compressed ST models are
finally merged into one MT model so that the frozen layers of the former are
shared across the tasks. We exemplify our approach on eight GLUE tasks,
demonstrating that it is able to achieve both strong performance and
efficiency. We have implemented our method in the utterance understanding
system of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate
that our model reduces the overall serving cost by 86%.",To appear in ACL2022,,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2107.05377v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.05377v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2107.05377v2,"{'id': 'http://arxiv.org/abs/2107.05377v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2107.05377v2', 'updated': '2022-03-09T03:56:22Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=9, tm_hour=3, tm_min=56, tm_sec=22, tm_wday=2, tm_yday=68, tm_isdst=0), 'published': '2021-07-12T12:42:39Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=7, tm_mday=12, tm_hour=12, tm_min=42, tm_sec=39, tm_wday=0, tm_yday=193, tm_isdst=0), 'title': 'A Flexible Multi-Task Model for BERT Serving', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A Flexible Multi-Task Model for BERT Serving'}, 'summary': 'In this demonstration, we present an efficient BERT-based multi-task (MT)\nframework that is particularly suitable for iterative and incremental\ndevelopment of the tasks. The proposed framework is based on the idea of\npartial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the\nother layers frozen. For each task, we train independently a single-task (ST)\nmodel using partial fine-tuning. Then we compress the task-specific layers in\neach ST model using knowledge distillation. Those compressed ST models are\nfinally merged into one MT model so that the frozen layers of the former are\nshared across the tasks. We exemplify our approach on eight GLUE tasks,\ndemonstrating that it is able to achieve both strong performance and\nefficiency. We have implemented our method in the utterance understanding\nsystem of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate\nthat our model reduces the overall serving cost by 86%.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'In this demonstration, we present an efficient BERT-based multi-task (MT)\nframework that is particularly suitable for iterative and incremental\ndevelopment of the tasks. The proposed framework is based on the idea of\npartial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the\nother layers frozen. For each task, we train independently a single-task (ST)\nmodel using partial fine-tuning. Then we compress the task-specific layers in\neach ST model using knowledge distillation. Those compressed ST models are\nfinally merged into one MT model so that the frozen layers of the former are\nshared across the tasks. We exemplify our approach on eight GLUE tasks,\ndemonstrating that it is able to achieve both strong performance and\nefficiency. We have implemented our method in the utterance understanding\nsystem of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate\nthat our model reduces the overall serving cost by 86%.'}, 'authors': [{'name': 'Tianwen Wei'}, {'name': 'Jianwei Qi'}, {'name': 'Shenghuan He'}], 'author_detail': {'name': 'Shenghuan He'}, 'author': 'Shenghuan He', 'arxiv_comment': 'To appear in ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2107.05377v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.05377v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2108.04750v2,2022-03-09 11:09:29+00:00,2021-08-10 15:27:47+00:00,Headed-Span-Based Projective Dependency Parsing,"[arxiv.Result.Author('Songlin Yang'), arxiv.Result.Author('Kewei Tu')]","We propose a new method for projective dependency parsing based on headed
spans. In a projective dependency tree, the largest subtree rooted at each word
covers a contiguous sequence (i.e., a span) in the surface order. We call such
a span marked by a root word \textit{headed span}.
  A projective dependency tree can be represented as a collection of headed
spans. We decompose the score of a dependency tree into the scores of the
headed spans and design a novel $O(n^3)$ dynamic programming algorithm to
enable global training and exact inference. Our model achieves state-of-the-art
or competitive results on PTB, CTB, and UD. Our code is publicly available at
\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.",ACL2022 camera ready,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2108.04750v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2108.04750v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2108.04750v2,"{'id': 'http://arxiv.org/abs/2108.04750v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2108.04750v2', 'updated': '2022-03-09T11:09:29Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=9, tm_hour=11, tm_min=9, tm_sec=29, tm_wday=2, tm_yday=68, tm_isdst=0), 'published': '2021-08-10T15:27:47Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=8, tm_mday=10, tm_hour=15, tm_min=27, tm_sec=47, tm_wday=1, tm_yday=222, tm_isdst=0), 'title': 'Headed-Span-Based Projective Dependency Parsing', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Headed-Span-Based Projective Dependency Parsing'}, 'summary': 'We propose a new method for projective dependency parsing based on headed\nspans. In a projective dependency tree, the largest subtree rooted at each word\ncovers a contiguous sequence (i.e., a span) in the surface order. We call such\na span marked by a root word \\textit{headed span}.\n  A projective dependency tree can be represented as a collection of headed\nspans. We decompose the score of a dependency tree into the scores of the\nheaded spans and design a novel $O(n^3)$ dynamic programming algorithm to\nenable global training and exact inference. Our model achieves state-of-the-art\nor competitive results on PTB, CTB, and UD. Our code is publicly available at\n\\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We propose a new method for projective dependency parsing based on headed\nspans. In a projective dependency tree, the largest subtree rooted at each word\ncovers a contiguous sequence (i.e., a span) in the surface order. We call such\na span marked by a root word \\textit{headed span}.\n  A projective dependency tree can be represented as a collection of headed\nspans. We decompose the score of a dependency tree into the scores of the\nheaded spans and design a novel $O(n^3)$ dynamic programming algorithm to\nenable global training and exact inference. Our model achieves state-of-the-art\nor competitive results on PTB, CTB, and UD. Our code is publicly available at\n\\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.'}, 'authors': [{'name': 'Songlin Yang'}, {'name': 'Kewei Tu'}], 'author_detail': {'name': 'Kewei Tu'}, 'author': 'Kewei Tu', 'arxiv_comment': 'ACL2022 camera ready', 'links': [{'href': 'http://arxiv.org/abs/2108.04750v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2108.04750v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2109.14739v2,2022-03-01 10:58:42+00:00,2021-09-29 22:02:18+00:00,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System,"[arxiv.Result.Author('Yixuan Su'), arxiv.Result.Author('Lei Shu'), arxiv.Result.Author('Elman Mansimov'), arxiv.Result.Author('Arshit Gupta'), arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Yi-An Lai'), arxiv.Result.Author('Yi Zhang')]","Pre-trained language models have been recently shown to benefit task-oriented
dialogue (TOD) systems. Despite their success, existing methods often formulate
this task as a cascaded generation problem which can lead to error accumulation
across different sub-tasks and greater data annotation overhead. In this study,
we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In
addition, we introduce a new dialogue multi-task pre-training strategy that
allows the model to learn the primary TOD task completion skills from
heterogeneous dialog corpora. We extensively test our model on three benchmark
TOD tasks, including end-to-end dialogue modelling, dialogue state tracking,
and intent classification. Experimental results show that PPTOD achieves new
state of the art on all evaluated tasks in both high-resource and low-resource
scenarios. Furthermore, comparisons against previous SOTA methods show that the
responses generated by PPTOD are more factually correct and semantically
coherent as judged by human annotators.",Camera-ready for ACL2022 main conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2109.14739v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2109.14739v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2109.14739v2,"{'id': 'http://arxiv.org/abs/2109.14739v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2109.14739v2', 'updated': '2022-03-01T10:58:42Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=1, tm_hour=10, tm_min=58, tm_sec=42, tm_wday=1, tm_yday=60, tm_isdst=0), 'published': '2021-09-29T22:02:18Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=9, tm_mday=29, tm_hour=22, tm_min=2, tm_sec=18, tm_wday=2, tm_yday=272, tm_isdst=0), 'title': 'Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System'}, 'summary': 'Pre-trained language models have been recently shown to benefit task-oriented\ndialogue (TOD) systems. Despite their success, existing methods often formulate\nthis task as a cascaded generation problem which can lead to error accumulation\nacross different sub-tasks and greater data annotation overhead. In this study,\nwe present PPTOD, a unified plug-and-play model for task-oriented dialogue. In\naddition, we introduce a new dialogue multi-task pre-training strategy that\nallows the model to learn the primary TOD task completion skills from\nheterogeneous dialog corpora. We extensively test our model on three benchmark\nTOD tasks, including end-to-end dialogue modelling, dialogue state tracking,\nand intent classification. Experimental results show that PPTOD achieves new\nstate of the art on all evaluated tasks in both high-resource and low-resource\nscenarios. Furthermore, comparisons against previous SOTA methods show that the\nresponses generated by PPTOD are more factually correct and semantically\ncoherent as judged by human annotators.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Pre-trained language models have been recently shown to benefit task-oriented\ndialogue (TOD) systems. Despite their success, existing methods often formulate\nthis task as a cascaded generation problem which can lead to error accumulation\nacross different sub-tasks and greater data annotation overhead. In this study,\nwe present PPTOD, a unified plug-and-play model for task-oriented dialogue. In\naddition, we introduce a new dialogue multi-task pre-training strategy that\nallows the model to learn the primary TOD task completion skills from\nheterogeneous dialog corpora. We extensively test our model on three benchmark\nTOD tasks, including end-to-end dialogue modelling, dialogue state tracking,\nand intent classification. Experimental results show that PPTOD achieves new\nstate of the art on all evaluated tasks in both high-resource and low-resource\nscenarios. Furthermore, comparisons against previous SOTA methods show that the\nresponses generated by PPTOD are more factually correct and semantically\ncoherent as judged by human annotators.'}, 'authors': [{'name': 'Yixuan Su'}, {'name': 'Lei Shu'}, {'name': 'Elman Mansimov'}, {'name': 'Arshit Gupta'}, {'name': 'Deng Cai'}, {'name': 'Yi-An Lai'}, {'name': 'Yi Zhang'}], 'author_detail': {'name': 'Yi Zhang'}, 'author': 'Yi Zhang', 'arxiv_comment': 'Camera-ready for ACL2022 main conference', 'links': [{'href': 'http://arxiv.org/abs/2109.14739v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.14739v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2110.07855v5,2022-04-26 08:31:33+00:00,2021-10-15 04:45:15+00:00,Hierarchical Curriculum Learning for AMR Parsing,"[arxiv.Result.Author('Peiyi Wang'), arxiv.Result.Author('Liang Chen'), arxiv.Result.Author('Tianyu Liu'), arxiv.Result.Author('Damai Dai'), arxiv.Result.Author('Yunbo Cao'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Zhifang Sui')]","Abstract Meaning Representation (AMR) parsing aims to translate sentences to
semantic representation with a hierarchical structure, and is recently
empowered by pretrained sequence-to-sequence models. However, there exists a
gap between their flat training objective (i.e., equally treats all output
tokens) and the hierarchical AMR structure, which limits the model
generalization. To bridge this gap, we propose a Hierarchical Curriculum
Learning (HCL) framework with Structure-level (SC) and Instance-level Curricula
(IC). SC switches progressively from core to detail AMR semantic elements while
IC transits from structure-simple to -complex AMR instances during training.
Through these two warming-up processes, HCL reduces the difficulty of learning
complex structures, thus the flat model can better adapt to the AMR hierarchy.
Extensive experiments on AMR2.0, AMR3.0, structure-complex and
out-of-distribution situations verify the effectiveness of HCL.","ACL2022 short paper; Code and model are available a
  https://github.com/Wangpeiyi9979/HCL-Text2AMR",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2110.07855v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2110.07855v5', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2110.07855v5,"{'id': 'http://arxiv.org/abs/2110.07855v5', 'guidislink': True, 'link': 'http://arxiv.org/abs/2110.07855v5', 'updated': '2022-04-26T08:31:33Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=26, tm_hour=8, tm_min=31, tm_sec=33, tm_wday=1, tm_yday=116, tm_isdst=0), 'published': '2021-10-15T04:45:15Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=10, tm_mday=15, tm_hour=4, tm_min=45, tm_sec=15, tm_wday=4, tm_yday=288, tm_isdst=0), 'title': 'Hierarchical Curriculum Learning for AMR Parsing', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Hierarchical Curriculum Learning for AMR Parsing'}, 'summary': 'Abstract Meaning Representation (AMR) parsing aims to translate sentences to\nsemantic representation with a hierarchical structure, and is recently\nempowered by pretrained sequence-to-sequence models. However, there exists a\ngap between their flat training objective (i.e., equally treats all output\ntokens) and the hierarchical AMR structure, which limits the model\ngeneralization. To bridge this gap, we propose a Hierarchical Curriculum\nLearning (HCL) framework with Structure-level (SC) and Instance-level Curricula\n(IC). SC switches progressively from core to detail AMR semantic elements while\nIC transits from structure-simple to -complex AMR instances during training.\nThrough these two warming-up processes, HCL reduces the difficulty of learning\ncomplex structures, thus the flat model can better adapt to the AMR hierarchy.\nExtensive experiments on AMR2.0, AMR3.0, structure-complex and\nout-of-distribution situations verify the effectiveness of HCL.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Abstract Meaning Representation (AMR) parsing aims to translate sentences to\nsemantic representation with a hierarchical structure, and is recently\nempowered by pretrained sequence-to-sequence models. However, there exists a\ngap between their flat training objective (i.e., equally treats all output\ntokens) and the hierarchical AMR structure, which limits the model\ngeneralization. To bridge this gap, we propose a Hierarchical Curriculum\nLearning (HCL) framework with Structure-level (SC) and Instance-level Curricula\n(IC). SC switches progressively from core to detail AMR semantic elements while\nIC transits from structure-simple to -complex AMR instances during training.\nThrough these two warming-up processes, HCL reduces the difficulty of learning\ncomplex structures, thus the flat model can better adapt to the AMR hierarchy.\nExtensive experiments on AMR2.0, AMR3.0, structure-complex and\nout-of-distribution situations verify the effectiveness of HCL.'}, 'authors': [{'name': 'Peiyi Wang'}, {'name': 'Liang Chen'}, {'name': 'Tianyu Liu'}, {'name': 'Damai Dai'}, {'name': 'Yunbo Cao'}, {'name': 'Baobao Chang'}, {'name': 'Zhifang Sui'}], 'author_detail': {'name': 'Zhifang Sui'}, 'author': 'Zhifang Sui', 'arxiv_comment': 'ACL2022 short paper; Code and model are available a\n  https://github.com/Wangpeiyi9979/HCL-Text2AMR', 'links': [{'href': 'http://arxiv.org/abs/2110.07855v5', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2110.07855v5', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2110.08018v2,2022-03-15 07:55:14+00:00,2021-10-15 11:28:43+00:00,Structural Characterization for Dialogue Disentanglement,"[arxiv.Result.Author('Xinbei Ma'), arxiv.Result.Author('Zhuosheng Zhang'), arxiv.Result.Author('Hai Zhao')]","Tangled multi-party dialogue contexts lead to challenges for dialogue reading
comprehension, where multiple dialogue threads flow simultaneously within a
common dialogue record, increasing difficulties in understanding the dialogue
history for both human and machine. Previous studies mainly focus on utterance
encoding methods with carefully designed features but pay inadequate attention
to characteristic features of the structure of dialogues. We specially take
structure factors into account and design a novel model for dialogue
disentangling. Based on the fact that dialogues are constructed on successive
participation and interactions between speakers, we model structural
information of dialogues in two aspects: 1)speaker property that indicates whom
a message is from, and 2) reference dependency that shows whom a message may
refer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC
benchmark dataset and contributes to dialogue-related comprehension.",Accepted by ACL2022,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2110.08018v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2110.08018v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2110.08018v2,"{'id': 'http://arxiv.org/abs/2110.08018v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2110.08018v2', 'updated': '2022-03-15T07:55:14Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=15, tm_hour=7, tm_min=55, tm_sec=14, tm_wday=1, tm_yday=74, tm_isdst=0), 'published': '2021-10-15T11:28:43Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=10, tm_mday=15, tm_hour=11, tm_min=28, tm_sec=43, tm_wday=4, tm_yday=288, tm_isdst=0), 'title': 'Structural Characterization for Dialogue Disentanglement', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Structural Characterization for Dialogue Disentanglement'}, 'summary': 'Tangled multi-party dialogue contexts lead to challenges for dialogue reading\ncomprehension, where multiple dialogue threads flow simultaneously within a\ncommon dialogue record, increasing difficulties in understanding the dialogue\nhistory for both human and machine. Previous studies mainly focus on utterance\nencoding methods with carefully designed features but pay inadequate attention\nto characteristic features of the structure of dialogues. We specially take\nstructure factors into account and design a novel model for dialogue\ndisentangling. Based on the fact that dialogues are constructed on successive\nparticipation and interactions between speakers, we model structural\ninformation of dialogues in two aspects: 1)speaker property that indicates whom\na message is from, and 2) reference dependency that shows whom a message may\nrefer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC\nbenchmark dataset and contributes to dialogue-related comprehension.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Tangled multi-party dialogue contexts lead to challenges for dialogue reading\ncomprehension, where multiple dialogue threads flow simultaneously within a\ncommon dialogue record, increasing difficulties in understanding the dialogue\nhistory for both human and machine. Previous studies mainly focus on utterance\nencoding methods with carefully designed features but pay inadequate attention\nto characteristic features of the structure of dialogues. We specially take\nstructure factors into account and design a novel model for dialogue\ndisentangling. Based on the fact that dialogues are constructed on successive\nparticipation and interactions between speakers, we model structural\ninformation of dialogues in two aspects: 1)speaker property that indicates whom\na message is from, and 2) reference dependency that shows whom a message may\nrefer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC\nbenchmark dataset and contributes to dialogue-related comprehension.'}, 'authors': [{'name': 'Xinbei Ma'}, {'name': 'Zhuosheng Zhang'}, {'name': 'Hai Zhao'}], 'author_detail': {'name': 'Hai Zhao'}, 'author': 'Hai Zhao', 'arxiv_comment': 'Accepted by ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2110.08018v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2110.08018v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
