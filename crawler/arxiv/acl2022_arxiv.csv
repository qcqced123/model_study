entry_id,updated,published,title,authors,summary,comment,journal_ref,doi,primary_category,categories,links,pdf_url,_raw
http://arxiv.org/abs/2204.06239v1,2022-04-13 08:16:03+00:00,2022-04-13 08:16:03+00:00,Can Question Rewriting Help Conversational Question Answering?,"[arxiv.Result.Author('Etsuko Ishii'), arxiv.Result.Author('Yan Xu'), arxiv.Result.Author('Samuel Cahyawijaya'), arxiv.Result.Author('Bryan Wilie')]","Question rewriting (QR) is a subtask of conversational question answering
(CQA) aiming to ease the challenges of understanding dependencies among
dialogue history by reformulating questions in a self-contained form. Despite
seeming plausible, little evidence is available to justify QR as a mitigation
method for CQA. To verify the effectiveness of QR in CQA, we investigate a
reinforcement learning approach that integrates QR and CQA tasks and does not
require corresponding QR datasets for targeted CQA. We find, however, that the
RL method is on par with the end-to-end baseline. We provide an analysis of the
failure and describe the difficulty of exploiting QR for CQA.","Accepted at Workshop on Insights from Negative Results in NLP at
  ACL2022",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2204.06239v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.06239v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2204.06239v1,"{'id': 'http://arxiv.org/abs/2204.06239v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2204.06239v1', 'updated': '2022-04-13T08:16:03Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=13, tm_hour=8, tm_min=16, tm_sec=3, tm_wday=2, tm_yday=103, tm_isdst=0), 'published': '2022-04-13T08:16:03Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=13, tm_hour=8, tm_min=16, tm_sec=3, tm_wday=2, tm_yday=103, tm_isdst=0), 'title': 'Can Question Rewriting Help Conversational Question Answering?', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Can Question Rewriting Help Conversational Question Answering?'}, 'summary': 'Question rewriting (QR) is a subtask of conversational question answering\n(CQA) aiming to ease the challenges of understanding dependencies among\ndialogue history by reformulating questions in a self-contained form. Despite\nseeming plausible, little evidence is available to justify QR as a mitigation\nmethod for CQA. To verify the effectiveness of QR in CQA, we investigate a\nreinforcement learning approach that integrates QR and CQA tasks and does not\nrequire corresponding QR datasets for targeted CQA. We find, however, that the\nRL method is on par with the end-to-end baseline. We provide an analysis of the\nfailure and describe the difficulty of exploiting QR for CQA.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Question rewriting (QR) is a subtask of conversational question answering\n(CQA) aiming to ease the challenges of understanding dependencies among\ndialogue history by reformulating questions in a self-contained form. Despite\nseeming plausible, little evidence is available to justify QR as a mitigation\nmethod for CQA. To verify the effectiveness of QR in CQA, we investigate a\nreinforcement learning approach that integrates QR and CQA tasks and does not\nrequire corresponding QR datasets for targeted CQA. We find, however, that the\nRL method is on par with the end-to-end baseline. We provide an analysis of the\nfailure and describe the difficulty of exploiting QR for CQA.'}, 'authors': [{'name': 'Etsuko Ishii'}, {'name': 'Yan Xu'}, {'name': 'Samuel Cahyawijaya'}, {'name': 'Bryan Wilie'}], 'author_detail': {'name': 'Bryan Wilie'}, 'author': 'Bryan Wilie', 'arxiv_comment': 'Accepted at Workshop on Insights from Negative Results in NLP at\n  ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2204.06239v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2204.06239v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2205.13108v1,2022-05-26 02:18:12+00:00,2022-05-26 02:18:12+00:00,Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV Conversion,"[arxiv.Result.Author('Seongmin Park'), arxiv.Result.Author('Jihwa Lee')]","We advance the state-of-the-art in unsupervised abstractive dialogue
summarization by utilizing multi-sentence compression graphs. Starting from
well-founded assumptions about word graphs, we present simple but reliable
path-reranking and topic segmentation schemes. Robustness of our method is
demonstrated on datasets across multiple domains, including meetings,
interviews, movie scripts, and day-to-day conversations. We also identify
possible avenues to augment our heuristic-based system with deep learning. We
open-source our code, to provide a strong, reproducible baseline for future
research into unsupervised dialogue summarization.",WIT Workshop @ ACL2022,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2205.13108v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.13108v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2205.13108v1,"{'id': 'http://arxiv.org/abs/2205.13108v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2205.13108v1', 'updated': '2022-05-26T02:18:12Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=26, tm_hour=2, tm_min=18, tm_sec=12, tm_wday=3, tm_yday=146, tm_isdst=0), 'published': '2022-05-26T02:18:12Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=26, tm_hour=2, tm_min=18, tm_sec=12, tm_wday=3, tm_yday=146, tm_isdst=0), 'title': 'Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV\n  Conversion', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Unsupervised Abstractive Dialogue Summarization with Word Graphs and POV\n  Conversion'}, 'summary': 'We advance the state-of-the-art in unsupervised abstractive dialogue\nsummarization by utilizing multi-sentence compression graphs. Starting from\nwell-founded assumptions about word graphs, we present simple but reliable\npath-reranking and topic segmentation schemes. Robustness of our method is\ndemonstrated on datasets across multiple domains, including meetings,\ninterviews, movie scripts, and day-to-day conversations. We also identify\npossible avenues to augment our heuristic-based system with deep learning. We\nopen-source our code, to provide a strong, reproducible baseline for future\nresearch into unsupervised dialogue summarization.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We advance the state-of-the-art in unsupervised abstractive dialogue\nsummarization by utilizing multi-sentence compression graphs. Starting from\nwell-founded assumptions about word graphs, we present simple but reliable\npath-reranking and topic segmentation schemes. Robustness of our method is\ndemonstrated on datasets across multiple domains, including meetings,\ninterviews, movie scripts, and day-to-day conversations. We also identify\npossible avenues to augment our heuristic-based system with deep learning. We\nopen-source our code, to provide a strong, reproducible baseline for future\nresearch into unsupervised dialogue summarization.'}, 'authors': [{'name': 'Seongmin Park'}, {'name': 'Jihwa Lee'}], 'author_detail': {'name': 'Jihwa Lee'}, 'author': 'Jihwa Lee', 'arxiv_comment': 'WIT Workshop @ ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2205.13108v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.13108v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2104.07554v2,2022-03-07 14:00:40+00:00,2021-04-15 16:08:43+00:00,Zero-Shot Cross-lingual Semantic Parsing,"[arxiv.Result.Author('Tom Sherborne'), arxiv.Result.Author('Mirella Lapata')]","Recent work in cross-lingual semantic parsing has successfully applied
machine translation to localize parsers to new languages. However, these
advances assume access to high-quality machine translation systems and word
alignment tools. We remove these assumptions and study cross-lingual semantic
parsing as a zero-shot problem, without parallel data (i.e., utterance-logical
form pairs) for new languages. We propose a multi-task encoder-decoder model to
transfer parsing knowledge to additional languages using only English-logical
form paired data and in-domain natural language corpora in each new language.
Our model encourages language-agnostic encodings by jointly optimizing for
logical-form generation with auxiliary objectives designed for cross-lingual
latent representation alignment. Our parser performs significantly above
translation-based baselines and, in some cases, competes with the supervised
upper-bound.","Accepted to ACL2022 Main Conference. 19 pages, 3 figures, 12 tables",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2104.07554v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2104.07554v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2104.07554v2,"{'id': 'http://arxiv.org/abs/2104.07554v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2104.07554v2', 'updated': '2022-03-07T14:00:40Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=7, tm_hour=14, tm_min=0, tm_sec=40, tm_wday=0, tm_yday=66, tm_isdst=0), 'published': '2021-04-15T16:08:43Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=4, tm_mday=15, tm_hour=16, tm_min=8, tm_sec=43, tm_wday=3, tm_yday=105, tm_isdst=0), 'title': 'Zero-Shot Cross-lingual Semantic Parsing', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Zero-Shot Cross-lingual Semantic Parsing'}, 'summary': 'Recent work in cross-lingual semantic parsing has successfully applied\nmachine translation to localize parsers to new languages. However, these\nadvances assume access to high-quality machine translation systems and word\nalignment tools. We remove these assumptions and study cross-lingual semantic\nparsing as a zero-shot problem, without parallel data (i.e., utterance-logical\nform pairs) for new languages. We propose a multi-task encoder-decoder model to\ntransfer parsing knowledge to additional languages using only English-logical\nform paired data and in-domain natural language corpora in each new language.\nOur model encourages language-agnostic encodings by jointly optimizing for\nlogical-form generation with auxiliary objectives designed for cross-lingual\nlatent representation alignment. Our parser performs significantly above\ntranslation-based baselines and, in some cases, competes with the supervised\nupper-bound.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Recent work in cross-lingual semantic parsing has successfully applied\nmachine translation to localize parsers to new languages. However, these\nadvances assume access to high-quality machine translation systems and word\nalignment tools. We remove these assumptions and study cross-lingual semantic\nparsing as a zero-shot problem, without parallel data (i.e., utterance-logical\nform pairs) for new languages. We propose a multi-task encoder-decoder model to\ntransfer parsing knowledge to additional languages using only English-logical\nform paired data and in-domain natural language corpora in each new language.\nOur model encourages language-agnostic encodings by jointly optimizing for\nlogical-form generation with auxiliary objectives designed for cross-lingual\nlatent representation alignment. Our parser performs significantly above\ntranslation-based baselines and, in some cases, competes with the supervised\nupper-bound.'}, 'authors': [{'name': 'Tom Sherborne'}, {'name': 'Mirella Lapata'}], 'author_detail': {'name': 'Mirella Lapata'}, 'author': 'Mirella Lapata', 'arxiv_comment': 'Accepted to ACL2022 Main Conference. 19 pages, 3 figures, 12 tables', 'links': [{'href': 'http://arxiv.org/abs/2104.07554v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.07554v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2104.08704v2,2022-04-02 15:23:44+00:00,2021-04-18 04:09:48+00:00,A Token-level Reference-free Hallucination Detection Benchmark for Free-form Text Generation,"[arxiv.Result.Author('Tianyu Liu'), arxiv.Result.Author('Yizhe Zhang'), arxiv.Result.Author('Chris Brockett'), arxiv.Result.Author('Yi Mao'), arxiv.Result.Author('Zhifang Sui'), arxiv.Result.Author('Weizhu Chen'), arxiv.Result.Author('Bill Dolan')]","Large pretrained generative models like GPT-3 often suffer from hallucinating
non-existent or incorrect content, which undermines their potential merits in
real applications. Existing work usually attempts to detect these
hallucinations based on a corresponding oracle reference at a sentence or
document level. However ground-truth references may not be readily available
for many free-form text generation applications, and sentence- or
document-level detection may fail to provide the fine-grained signals that
would prevent fallacious content in real time. As a first step to addressing
these issues, we propose a novel token-level, reference-free hallucination
detection task and an associated annotated dataset named HaDes (HAllucination
DEtection dataSet). To create this dataset, we first perturb a large number of
text segments extracted from English language Wikipedia, and then verify these
with crowd-sourced annotations. To mitigate label imbalance during annotation,
we utilize an iterative model-in-loop strategy. We conduct comprehensive data
analyses and create multiple baseline models.",Accepted by ACL2022 main conference,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2104.08704v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2104.08704v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2104.08704v2,"{'id': 'http://arxiv.org/abs/2104.08704v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2104.08704v2', 'updated': '2022-04-02T15:23:44Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=2, tm_hour=15, tm_min=23, tm_sec=44, tm_wday=5, tm_yday=92, tm_isdst=0), 'published': '2021-04-18T04:09:48Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=4, tm_mday=18, tm_hour=4, tm_min=9, tm_sec=48, tm_wday=6, tm_yday=108, tm_isdst=0), 'title': 'A Token-level Reference-free Hallucination Detection Benchmark for\n  Free-form Text Generation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A Token-level Reference-free Hallucination Detection Benchmark for\n  Free-form Text Generation'}, 'summary': 'Large pretrained generative models like GPT-3 often suffer from hallucinating\nnon-existent or incorrect content, which undermines their potential merits in\nreal applications. Existing work usually attempts to detect these\nhallucinations based on a corresponding oracle reference at a sentence or\ndocument level. However ground-truth references may not be readily available\nfor many free-form text generation applications, and sentence- or\ndocument-level detection may fail to provide the fine-grained signals that\nwould prevent fallacious content in real time. As a first step to addressing\nthese issues, we propose a novel token-level, reference-free hallucination\ndetection task and an associated annotated dataset named HaDes (HAllucination\nDEtection dataSet). To create this dataset, we first perturb a large number of\ntext segments extracted from English language Wikipedia, and then verify these\nwith crowd-sourced annotations. To mitigate label imbalance during annotation,\nwe utilize an iterative model-in-loop strategy. We conduct comprehensive data\nanalyses and create multiple baseline models.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Large pretrained generative models like GPT-3 often suffer from hallucinating\nnon-existent or incorrect content, which undermines their potential merits in\nreal applications. Existing work usually attempts to detect these\nhallucinations based on a corresponding oracle reference at a sentence or\ndocument level. However ground-truth references may not be readily available\nfor many free-form text generation applications, and sentence- or\ndocument-level detection may fail to provide the fine-grained signals that\nwould prevent fallacious content in real time. As a first step to addressing\nthese issues, we propose a novel token-level, reference-free hallucination\ndetection task and an associated annotated dataset named HaDes (HAllucination\nDEtection dataSet). To create this dataset, we first perturb a large number of\ntext segments extracted from English language Wikipedia, and then verify these\nwith crowd-sourced annotations. To mitigate label imbalance during annotation,\nwe utilize an iterative model-in-loop strategy. We conduct comprehensive data\nanalyses and create multiple baseline models.'}, 'authors': [{'name': 'Tianyu Liu'}, {'name': 'Yizhe Zhang'}, {'name': 'Chris Brockett'}, {'name': 'Yi Mao'}, {'name': 'Zhifang Sui'}, {'name': 'Weizhu Chen'}, {'name': 'Bill Dolan'}], 'author_detail': {'name': 'Bill Dolan'}, 'author': 'Bill Dolan', 'arxiv_comment': 'Accepted by ACL2022 main conference', 'links': [{'href': 'http://arxiv.org/abs/2104.08704v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2104.08704v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2105.00828v2,2022-03-15 01:14:16+00:00,2021-04-16 18:53:19+00:00,Memorisation versus Generalisation in Pre-trained Language Models,"[arxiv.Result.Author('Michael Tänzer'), arxiv.Result.Author('Sebastian Ruder'), arxiv.Result.Author('Marek Rei')]","State-of-the-art pre-trained language models have been shown to memorise
facts and perform well with limited amounts of training data. To gain a better
understanding of how these models learn, we study their generalisation and
memorisation capabilities in noisy and low-resource scenarios. We find that the
training of these models is almost unaffected by label noise and that it is
possible to reach near-optimal results even on extremely noisy datasets.
However, our experiments also show that they mainly learn from high-frequency
patterns and largely fail when tested on low-resource tasks such as few-shot
learning and rare entity recognition. To mitigate such limitations, we propose
an extension based on prototypical networks that improves performance in
low-resource named entity recognition tasks.","15 pages, 25 figures. To be published in ACL2022",,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2105.00828v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2105.00828v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2105.00828v2,"{'id': 'http://arxiv.org/abs/2105.00828v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2105.00828v2', 'updated': '2022-03-15T01:14:16Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=15, tm_hour=1, tm_min=14, tm_sec=16, tm_wday=1, tm_yday=74, tm_isdst=0), 'published': '2021-04-16T18:53:19Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=4, tm_mday=16, tm_hour=18, tm_min=53, tm_sec=19, tm_wday=4, tm_yday=106, tm_isdst=0), 'title': 'Memorisation versus Generalisation in Pre-trained Language Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Memorisation versus Generalisation in Pre-trained Language Models'}, 'summary': 'State-of-the-art pre-trained language models have been shown to memorise\nfacts and perform well with limited amounts of training data. To gain a better\nunderstanding of how these models learn, we study their generalisation and\nmemorisation capabilities in noisy and low-resource scenarios. We find that the\ntraining of these models is almost unaffected by label noise and that it is\npossible to reach near-optimal results even on extremely noisy datasets.\nHowever, our experiments also show that they mainly learn from high-frequency\npatterns and largely fail when tested on low-resource tasks such as few-shot\nlearning and rare entity recognition. To mitigate such limitations, we propose\nan extension based on prototypical networks that improves performance in\nlow-resource named entity recognition tasks.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'State-of-the-art pre-trained language models have been shown to memorise\nfacts and perform well with limited amounts of training data. To gain a better\nunderstanding of how these models learn, we study their generalisation and\nmemorisation capabilities in noisy and low-resource scenarios. We find that the\ntraining of these models is almost unaffected by label noise and that it is\npossible to reach near-optimal results even on extremely noisy datasets.\nHowever, our experiments also show that they mainly learn from high-frequency\npatterns and largely fail when tested on low-resource tasks such as few-shot\nlearning and rare entity recognition. To mitigate such limitations, we propose\nan extension based on prototypical networks that improves performance in\nlow-resource named entity recognition tasks.'}, 'authors': [{'name': 'Michael Tänzer'}, {'name': 'Sebastian Ruder'}, {'name': 'Marek Rei'}], 'author_detail': {'name': 'Marek Rei'}, 'author': 'Marek Rei', 'arxiv_comment': '15 pages, 25 figures. To be published in ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2105.00828v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2105.00828v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2107.05377v2,2022-03-09 03:56:22+00:00,2021-07-12 12:42:39+00:00,A Flexible Multi-Task Model for BERT Serving,"[arxiv.Result.Author('Tianwen Wei'), arxiv.Result.Author('Jianwei Qi'), arxiv.Result.Author('Shenghuan He')]","In this demonstration, we present an efficient BERT-based multi-task (MT)
framework that is particularly suitable for iterative and incremental
development of the tasks. The proposed framework is based on the idea of
partial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the
other layers frozen. For each task, we train independently a single-task (ST)
model using partial fine-tuning. Then we compress the task-specific layers in
each ST model using knowledge distillation. Those compressed ST models are
finally merged into one MT model so that the frozen layers of the former are
shared across the tasks. We exemplify our approach on eight GLUE tasks,
demonstrating that it is able to achieve both strong performance and
efficiency. We have implemented our method in the utterance understanding
system of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate
that our model reduces the overall serving cost by 86%.",To appear in ACL2022,,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2107.05377v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2107.05377v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2107.05377v2,"{'id': 'http://arxiv.org/abs/2107.05377v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2107.05377v2', 'updated': '2022-03-09T03:56:22Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=9, tm_hour=3, tm_min=56, tm_sec=22, tm_wday=2, tm_yday=68, tm_isdst=0), 'published': '2021-07-12T12:42:39Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=7, tm_mday=12, tm_hour=12, tm_min=42, tm_sec=39, tm_wday=0, tm_yday=193, tm_isdst=0), 'title': 'A Flexible Multi-Task Model for BERT Serving', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A Flexible Multi-Task Model for BERT Serving'}, 'summary': 'In this demonstration, we present an efficient BERT-based multi-task (MT)\nframework that is particularly suitable for iterative and incremental\ndevelopment of the tasks. The proposed framework is based on the idea of\npartial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the\nother layers frozen. For each task, we train independently a single-task (ST)\nmodel using partial fine-tuning. Then we compress the task-specific layers in\neach ST model using knowledge distillation. Those compressed ST models are\nfinally merged into one MT model so that the frozen layers of the former are\nshared across the tasks. We exemplify our approach on eight GLUE tasks,\ndemonstrating that it is able to achieve both strong performance and\nefficiency. We have implemented our method in the utterance understanding\nsystem of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate\nthat our model reduces the overall serving cost by 86%.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'In this demonstration, we present an efficient BERT-based multi-task (MT)\nframework that is particularly suitable for iterative and incremental\ndevelopment of the tasks. The proposed framework is based on the idea of\npartial fine-tuning, i.e. only fine-tune some top layers of BERT while keep the\nother layers frozen. For each task, we train independently a single-task (ST)\nmodel using partial fine-tuning. Then we compress the task-specific layers in\neach ST model using knowledge distillation. Those compressed ST models are\nfinally merged into one MT model so that the frozen layers of the former are\nshared across the tasks. We exemplify our approach on eight GLUE tasks,\ndemonstrating that it is able to achieve both strong performance and\nefficiency. We have implemented our method in the utterance understanding\nsystem of XiaoAI, a commercial AI assistant developed by Xiaomi. We estimate\nthat our model reduces the overall serving cost by 86%.'}, 'authors': [{'name': 'Tianwen Wei'}, {'name': 'Jianwei Qi'}, {'name': 'Shenghuan He'}], 'author_detail': {'name': 'Shenghuan He'}, 'author': 'Shenghuan He', 'arxiv_comment': 'To appear in ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2107.05377v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2107.05377v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2108.04750v2,2022-03-09 11:09:29+00:00,2021-08-10 15:27:47+00:00,Headed-Span-Based Projective Dependency Parsing,"[arxiv.Result.Author('Songlin Yang'), arxiv.Result.Author('Kewei Tu')]","We propose a new method for projective dependency parsing based on headed
spans. In a projective dependency tree, the largest subtree rooted at each word
covers a contiguous sequence (i.e., a span) in the surface order. We call such
a span marked by a root word \textit{headed span}.
  A projective dependency tree can be represented as a collection of headed
spans. We decompose the score of a dependency tree into the scores of the
headed spans and design a novel $O(n^3)$ dynamic programming algorithm to
enable global training and exact inference. Our model achieves state-of-the-art
or competitive results on PTB, CTB, and UD. Our code is publicly available at
\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.",ACL2022 camera ready,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2108.04750v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2108.04750v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2108.04750v2,"{'id': 'http://arxiv.org/abs/2108.04750v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2108.04750v2', 'updated': '2022-03-09T11:09:29Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=9, tm_hour=11, tm_min=9, tm_sec=29, tm_wday=2, tm_yday=68, tm_isdst=0), 'published': '2021-08-10T15:27:47Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=8, tm_mday=10, tm_hour=15, tm_min=27, tm_sec=47, tm_wday=1, tm_yday=222, tm_isdst=0), 'title': 'Headed-Span-Based Projective Dependency Parsing', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Headed-Span-Based Projective Dependency Parsing'}, 'summary': 'We propose a new method for projective dependency parsing based on headed\nspans. In a projective dependency tree, the largest subtree rooted at each word\ncovers a contiguous sequence (i.e., a span) in the surface order. We call such\na span marked by a root word \\textit{headed span}.\n  A projective dependency tree can be represented as a collection of headed\nspans. We decompose the score of a dependency tree into the scores of the\nheaded spans and design a novel $O(n^3)$ dynamic programming algorithm to\nenable global training and exact inference. Our model achieves state-of-the-art\nor competitive results on PTB, CTB, and UD. Our code is publicly available at\n\\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We propose a new method for projective dependency parsing based on headed\nspans. In a projective dependency tree, the largest subtree rooted at each word\ncovers a contiguous sequence (i.e., a span) in the surface order. We call such\na span marked by a root word \\textit{headed span}.\n  A projective dependency tree can be represented as a collection of headed\nspans. We decompose the score of a dependency tree into the scores of the\nheaded spans and design a novel $O(n^3)$ dynamic programming algorithm to\nenable global training and exact inference. Our model achieves state-of-the-art\nor competitive results on PTB, CTB, and UD. Our code is publicly available at\n\\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.'}, 'authors': [{'name': 'Songlin Yang'}, {'name': 'Kewei Tu'}], 'author_detail': {'name': 'Kewei Tu'}, 'author': 'Kewei Tu', 'arxiv_comment': 'ACL2022 camera ready', 'links': [{'href': 'http://arxiv.org/abs/2108.04750v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2108.04750v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2109.14739v2,2022-03-01 10:58:42+00:00,2021-09-29 22:02:18+00:00,Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System,"[arxiv.Result.Author('Yixuan Su'), arxiv.Result.Author('Lei Shu'), arxiv.Result.Author('Elman Mansimov'), arxiv.Result.Author('Arshit Gupta'), arxiv.Result.Author('Deng Cai'), arxiv.Result.Author('Yi-An Lai'), arxiv.Result.Author('Yi Zhang')]","Pre-trained language models have been recently shown to benefit task-oriented
dialogue (TOD) systems. Despite their success, existing methods often formulate
this task as a cascaded generation problem which can lead to error accumulation
across different sub-tasks and greater data annotation overhead. In this study,
we present PPTOD, a unified plug-and-play model for task-oriented dialogue. In
addition, we introduce a new dialogue multi-task pre-training strategy that
allows the model to learn the primary TOD task completion skills from
heterogeneous dialog corpora. We extensively test our model on three benchmark
TOD tasks, including end-to-end dialogue modelling, dialogue state tracking,
and intent classification. Experimental results show that PPTOD achieves new
state of the art on all evaluated tasks in both high-resource and low-resource
scenarios. Furthermore, comparisons against previous SOTA methods show that the
responses generated by PPTOD are more factually correct and semantically
coherent as judged by human annotators.",Camera-ready for ACL2022 main conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2109.14739v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2109.14739v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2109.14739v2,"{'id': 'http://arxiv.org/abs/2109.14739v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2109.14739v2', 'updated': '2022-03-01T10:58:42Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=1, tm_hour=10, tm_min=58, tm_sec=42, tm_wday=1, tm_yday=60, tm_isdst=0), 'published': '2021-09-29T22:02:18Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=9, tm_mday=29, tm_hour=22, tm_min=2, tm_sec=18, tm_wday=2, tm_yday=272, tm_isdst=0), 'title': 'Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Multi-Task Pre-Training for Plug-and-Play Task-Oriented Dialogue System'}, 'summary': 'Pre-trained language models have been recently shown to benefit task-oriented\ndialogue (TOD) systems. Despite their success, existing methods often formulate\nthis task as a cascaded generation problem which can lead to error accumulation\nacross different sub-tasks and greater data annotation overhead. In this study,\nwe present PPTOD, a unified plug-and-play model for task-oriented dialogue. In\naddition, we introduce a new dialogue multi-task pre-training strategy that\nallows the model to learn the primary TOD task completion skills from\nheterogeneous dialog corpora. We extensively test our model on three benchmark\nTOD tasks, including end-to-end dialogue modelling, dialogue state tracking,\nand intent classification. Experimental results show that PPTOD achieves new\nstate of the art on all evaluated tasks in both high-resource and low-resource\nscenarios. Furthermore, comparisons against previous SOTA methods show that the\nresponses generated by PPTOD are more factually correct and semantically\ncoherent as judged by human annotators.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Pre-trained language models have been recently shown to benefit task-oriented\ndialogue (TOD) systems. Despite their success, existing methods often formulate\nthis task as a cascaded generation problem which can lead to error accumulation\nacross different sub-tasks and greater data annotation overhead. In this study,\nwe present PPTOD, a unified plug-and-play model for task-oriented dialogue. In\naddition, we introduce a new dialogue multi-task pre-training strategy that\nallows the model to learn the primary TOD task completion skills from\nheterogeneous dialog corpora. We extensively test our model on three benchmark\nTOD tasks, including end-to-end dialogue modelling, dialogue state tracking,\nand intent classification. Experimental results show that PPTOD achieves new\nstate of the art on all evaluated tasks in both high-resource and low-resource\nscenarios. Furthermore, comparisons against previous SOTA methods show that the\nresponses generated by PPTOD are more factually correct and semantically\ncoherent as judged by human annotators.'}, 'authors': [{'name': 'Yixuan Su'}, {'name': 'Lei Shu'}, {'name': 'Elman Mansimov'}, {'name': 'Arshit Gupta'}, {'name': 'Deng Cai'}, {'name': 'Yi-An Lai'}, {'name': 'Yi Zhang'}], 'author_detail': {'name': 'Yi Zhang'}, 'author': 'Yi Zhang', 'arxiv_comment': 'Camera-ready for ACL2022 main conference', 'links': [{'href': 'http://arxiv.org/abs/2109.14739v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.14739v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2110.07855v5,2022-04-26 08:31:33+00:00,2021-10-15 04:45:15+00:00,Hierarchical Curriculum Learning for AMR Parsing,"[arxiv.Result.Author('Peiyi Wang'), arxiv.Result.Author('Liang Chen'), arxiv.Result.Author('Tianyu Liu'), arxiv.Result.Author('Damai Dai'), arxiv.Result.Author('Yunbo Cao'), arxiv.Result.Author('Baobao Chang'), arxiv.Result.Author('Zhifang Sui')]","Abstract Meaning Representation (AMR) parsing aims to translate sentences to
semantic representation with a hierarchical structure, and is recently
empowered by pretrained sequence-to-sequence models. However, there exists a
gap between their flat training objective (i.e., equally treats all output
tokens) and the hierarchical AMR structure, which limits the model
generalization. To bridge this gap, we propose a Hierarchical Curriculum
Learning (HCL) framework with Structure-level (SC) and Instance-level Curricula
(IC). SC switches progressively from core to detail AMR semantic elements while
IC transits from structure-simple to -complex AMR instances during training.
Through these two warming-up processes, HCL reduces the difficulty of learning
complex structures, thus the flat model can better adapt to the AMR hierarchy.
Extensive experiments on AMR2.0, AMR3.0, structure-complex and
out-of-distribution situations verify the effectiveness of HCL.","ACL2022 short paper; Code and model are available a
  https://github.com/Wangpeiyi9979/HCL-Text2AMR",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2110.07855v5', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2110.07855v5', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2110.07855v5,"{'id': 'http://arxiv.org/abs/2110.07855v5', 'guidislink': True, 'link': 'http://arxiv.org/abs/2110.07855v5', 'updated': '2022-04-26T08:31:33Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=26, tm_hour=8, tm_min=31, tm_sec=33, tm_wday=1, tm_yday=116, tm_isdst=0), 'published': '2021-10-15T04:45:15Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=10, tm_mday=15, tm_hour=4, tm_min=45, tm_sec=15, tm_wday=4, tm_yday=288, tm_isdst=0), 'title': 'Hierarchical Curriculum Learning for AMR Parsing', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Hierarchical Curriculum Learning for AMR Parsing'}, 'summary': 'Abstract Meaning Representation (AMR) parsing aims to translate sentences to\nsemantic representation with a hierarchical structure, and is recently\nempowered by pretrained sequence-to-sequence models. However, there exists a\ngap between their flat training objective (i.e., equally treats all output\ntokens) and the hierarchical AMR structure, which limits the model\ngeneralization. To bridge this gap, we propose a Hierarchical Curriculum\nLearning (HCL) framework with Structure-level (SC) and Instance-level Curricula\n(IC). SC switches progressively from core to detail AMR semantic elements while\nIC transits from structure-simple to -complex AMR instances during training.\nThrough these two warming-up processes, HCL reduces the difficulty of learning\ncomplex structures, thus the flat model can better adapt to the AMR hierarchy.\nExtensive experiments on AMR2.0, AMR3.0, structure-complex and\nout-of-distribution situations verify the effectiveness of HCL.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Abstract Meaning Representation (AMR) parsing aims to translate sentences to\nsemantic representation with a hierarchical structure, and is recently\nempowered by pretrained sequence-to-sequence models. However, there exists a\ngap between their flat training objective (i.e., equally treats all output\ntokens) and the hierarchical AMR structure, which limits the model\ngeneralization. To bridge this gap, we propose a Hierarchical Curriculum\nLearning (HCL) framework with Structure-level (SC) and Instance-level Curricula\n(IC). SC switches progressively from core to detail AMR semantic elements while\nIC transits from structure-simple to -complex AMR instances during training.\nThrough these two warming-up processes, HCL reduces the difficulty of learning\ncomplex structures, thus the flat model can better adapt to the AMR hierarchy.\nExtensive experiments on AMR2.0, AMR3.0, structure-complex and\nout-of-distribution situations verify the effectiveness of HCL.'}, 'authors': [{'name': 'Peiyi Wang'}, {'name': 'Liang Chen'}, {'name': 'Tianyu Liu'}, {'name': 'Damai Dai'}, {'name': 'Yunbo Cao'}, {'name': 'Baobao Chang'}, {'name': 'Zhifang Sui'}], 'author_detail': {'name': 'Zhifang Sui'}, 'author': 'Zhifang Sui', 'arxiv_comment': 'ACL2022 short paper; Code and model are available a\n  https://github.com/Wangpeiyi9979/HCL-Text2AMR', 'links': [{'href': 'http://arxiv.org/abs/2110.07855v5', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2110.07855v5', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2110.08018v2,2022-03-15 07:55:14+00:00,2021-10-15 11:28:43+00:00,Structural Characterization for Dialogue Disentanglement,"[arxiv.Result.Author('Xinbei Ma'), arxiv.Result.Author('Zhuosheng Zhang'), arxiv.Result.Author('Hai Zhao')]","Tangled multi-party dialogue contexts lead to challenges for dialogue reading
comprehension, where multiple dialogue threads flow simultaneously within a
common dialogue record, increasing difficulties in understanding the dialogue
history for both human and machine. Previous studies mainly focus on utterance
encoding methods with carefully designed features but pay inadequate attention
to characteristic features of the structure of dialogues. We specially take
structure factors into account and design a novel model for dialogue
disentangling. Based on the fact that dialogues are constructed on successive
participation and interactions between speakers, we model structural
information of dialogues in two aspects: 1)speaker property that indicates whom
a message is from, and 2) reference dependency that shows whom a message may
refer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC
benchmark dataset and contributes to dialogue-related comprehension.",Accepted by ACL2022,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2110.08018v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2110.08018v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2110.08018v2,"{'id': 'http://arxiv.org/abs/2110.08018v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2110.08018v2', 'updated': '2022-03-15T07:55:14Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=15, tm_hour=7, tm_min=55, tm_sec=14, tm_wday=1, tm_yday=74, tm_isdst=0), 'published': '2021-10-15T11:28:43Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=10, tm_mday=15, tm_hour=11, tm_min=28, tm_sec=43, tm_wday=4, tm_yday=288, tm_isdst=0), 'title': 'Structural Characterization for Dialogue Disentanglement', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Structural Characterization for Dialogue Disentanglement'}, 'summary': 'Tangled multi-party dialogue contexts lead to challenges for dialogue reading\ncomprehension, where multiple dialogue threads flow simultaneously within a\ncommon dialogue record, increasing difficulties in understanding the dialogue\nhistory for both human and machine. Previous studies mainly focus on utterance\nencoding methods with carefully designed features but pay inadequate attention\nto characteristic features of the structure of dialogues. We specially take\nstructure factors into account and design a novel model for dialogue\ndisentangling. Based on the fact that dialogues are constructed on successive\nparticipation and interactions between speakers, we model structural\ninformation of dialogues in two aspects: 1)speaker property that indicates whom\na message is from, and 2) reference dependency that shows whom a message may\nrefer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC\nbenchmark dataset and contributes to dialogue-related comprehension.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Tangled multi-party dialogue contexts lead to challenges for dialogue reading\ncomprehension, where multiple dialogue threads flow simultaneously within a\ncommon dialogue record, increasing difficulties in understanding the dialogue\nhistory for both human and machine. Previous studies mainly focus on utterance\nencoding methods with carefully designed features but pay inadequate attention\nto characteristic features of the structure of dialogues. We specially take\nstructure factors into account and design a novel model for dialogue\ndisentangling. Based on the fact that dialogues are constructed on successive\nparticipation and interactions between speakers, we model structural\ninformation of dialogues in two aspects: 1)speaker property that indicates whom\na message is from, and 2) reference dependency that shows whom a message may\nrefer to. The proposed method achieves new state-of-the-art on the Ubuntu IRC\nbenchmark dataset and contributes to dialogue-related comprehension.'}, 'authors': [{'name': 'Xinbei Ma'}, {'name': 'Zhuosheng Zhang'}, {'name': 'Hai Zhao'}], 'author_detail': {'name': 'Hai Zhao'}, 'author': 'Hai Zhao', 'arxiv_comment': 'Accepted by ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2110.08018v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2110.08018v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2202.13587v3,2022-04-03 23:32:50+00:00,2022-02-28 07:36:30+00:00,Rethinking and Refining the Distinct Metric,"[arxiv.Result.Author('Siyang Liu'), arxiv.Result.Author('Sahand Sabour'), arxiv.Result.Author('Yinhe Zheng'), arxiv.Result.Author('Pei Ke'), arxiv.Result.Author('Xiaoyan Zhu'), arxiv.Result.Author('Minlie Huang')]","Distinct-$n$ score\cite{Li2016} is a widely used automatic metric for
evaluating diversity in language generation tasks. However, we observed that
the original approach for calculating distinct scores has evident biases that
tend to assign higher penalties to longer sequences. We refine the calculation
of distinct scores by scaling the number of distinct tokens based on their
expectations. We provide both empirical and theoretical evidence to show that
our method effectively removes the biases existing in the original distinct
score. Our experiments show that our proposed metric,
\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human
judgment in evaluating response diversity. To foster future research, we
provide an example implementation at
\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.","4 pages, to be published at ACL2022",,,cs.CL,"['cs.CL', 'cs.AI', 'I.2.7']","[arxiv.Result.Link('http://arxiv.org/abs/2202.13587v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2202.13587v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2202.13587v3,"{'id': 'http://arxiv.org/abs/2202.13587v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2202.13587v3', 'updated': '2022-04-03T23:32:50Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=3, tm_hour=23, tm_min=32, tm_sec=50, tm_wday=6, tm_yday=93, tm_isdst=0), 'published': '2022-02-28T07:36:30Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=2, tm_mday=28, tm_hour=7, tm_min=36, tm_sec=30, tm_wday=0, tm_yday=59, tm_isdst=0), 'title': 'Rethinking and Refining the Distinct Metric', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Rethinking and Refining the Distinct Metric'}, 'summary': 'Distinct-$n$ score\\cite{Li2016} is a widely used automatic metric for\nevaluating diversity in language generation tasks. However, we observed that\nthe original approach for calculating distinct scores has evident biases that\ntend to assign higher penalties to longer sequences. We refine the calculation\nof distinct scores by scaling the number of distinct tokens based on their\nexpectations. We provide both empirical and theoretical evidence to show that\nour method effectively removes the biases existing in the original distinct\nscore. Our experiments show that our proposed metric,\n\\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human\njudgment in evaluating response diversity. To foster future research, we\nprovide an example implementation at\n\\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Distinct-$n$ score\\cite{Li2016} is a widely used automatic metric for\nevaluating diversity in language generation tasks. However, we observed that\nthe original approach for calculating distinct scores has evident biases that\ntend to assign higher penalties to longer sequences. We refine the calculation\nof distinct scores by scaling the number of distinct tokens based on their\nexpectations. We provide both empirical and theoretical evidence to show that\nour method effectively removes the biases existing in the original distinct\nscore. Our experiments show that our proposed metric,\n\\textit{Expectation-Adjusted Distinct (EAD)}, correlates better with human\njudgment in evaluating response diversity. To foster future research, we\nprovide an example implementation at\n\\url{https://github.com/lsy641/Expectation-Adjusted-Distinct}.'}, 'authors': [{'name': 'Siyang Liu'}, {'name': 'Sahand Sabour'}, {'name': 'Yinhe Zheng'}, {'name': 'Pei Ke'}, {'name': 'Xiaoyan Zhu'}, {'name': 'Minlie Huang'}], 'author_detail': {'name': 'Minlie Huang'}, 'author': 'Minlie Huang', 'arxiv_comment': '4 pages, to be published at ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2202.13587v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2202.13587v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.7', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.00056v1,2022-02-28 19:50:23+00:00,2022-02-28 19:50:23+00:00,An Empirical Study on Explanations in Out-of-Domain Settings,"[arxiv.Result.Author('George Chrysostomou'), arxiv.Result.Author('Nikolaos Aletras')]","Recent work in Natural Language Processing has focused on developing
approaches that extract faithful explanations, either via identifying the most
important tokens in the input (i.e. post-hoc explanations) or by designing
inherently faithful models that first select the most important tokens and then
use them to predict the correct label (i.e. select-then-predict models).
Currently, these approaches are largely evaluated on in-domain settings. Yet,
little is known about how post-hoc explanations and inherently faithful models
perform in out-of-domain settings. In this paper, we conduct an extensive
empirical study that examines: (1) the out-of-domain faithfulness of post-hoc
explanations, generated by five feature attribution methods; and (2) the
out-of-domain performance of two inherently faithful models over six datasets.
Contrary to our expectations, results show that in many cases out-of-domain
post-hoc explanation faithfulness measured by sufficiency and comprehensiveness
is higher compared to in-domain. We find this misleading and suggest using a
random baseline as a yardstick for evaluating post-hoc explanation
faithfulness. Our findings also show that select-then predict models
demonstrate comparable predictive performance in out-of-domain settings to
full-text trained models.",ACL2022 Pre-print,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.00056v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.00056v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.00056v1,"{'id': 'http://arxiv.org/abs/2203.00056v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.00056v1', 'updated': '2022-02-28T19:50:23Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=2, tm_mday=28, tm_hour=19, tm_min=50, tm_sec=23, tm_wday=0, tm_yday=59, tm_isdst=0), 'published': '2022-02-28T19:50:23Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=2, tm_mday=28, tm_hour=19, tm_min=50, tm_sec=23, tm_wday=0, tm_yday=59, tm_isdst=0), 'title': 'An Empirical Study on Explanations in Out-of-Domain Settings', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'An Empirical Study on Explanations in Out-of-Domain Settings'}, 'summary': 'Recent work in Natural Language Processing has focused on developing\napproaches that extract faithful explanations, either via identifying the most\nimportant tokens in the input (i.e. post-hoc explanations) or by designing\ninherently faithful models that first select the most important tokens and then\nuse them to predict the correct label (i.e. select-then-predict models).\nCurrently, these approaches are largely evaluated on in-domain settings. Yet,\nlittle is known about how post-hoc explanations and inherently faithful models\nperform in out-of-domain settings. In this paper, we conduct an extensive\nempirical study that examines: (1) the out-of-domain faithfulness of post-hoc\nexplanations, generated by five feature attribution methods; and (2) the\nout-of-domain performance of two inherently faithful models over six datasets.\nContrary to our expectations, results show that in many cases out-of-domain\npost-hoc explanation faithfulness measured by sufficiency and comprehensiveness\nis higher compared to in-domain. We find this misleading and suggest using a\nrandom baseline as a yardstick for evaluating post-hoc explanation\nfaithfulness. Our findings also show that select-then predict models\ndemonstrate comparable predictive performance in out-of-domain settings to\nfull-text trained models.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Recent work in Natural Language Processing has focused on developing\napproaches that extract faithful explanations, either via identifying the most\nimportant tokens in the input (i.e. post-hoc explanations) or by designing\ninherently faithful models that first select the most important tokens and then\nuse them to predict the correct label (i.e. select-then-predict models).\nCurrently, these approaches are largely evaluated on in-domain settings. Yet,\nlittle is known about how post-hoc explanations and inherently faithful models\nperform in out-of-domain settings. In this paper, we conduct an extensive\nempirical study that examines: (1) the out-of-domain faithfulness of post-hoc\nexplanations, generated by five feature attribution methods; and (2) the\nout-of-domain performance of two inherently faithful models over six datasets.\nContrary to our expectations, results show that in many cases out-of-domain\npost-hoc explanation faithfulness measured by sufficiency and comprehensiveness\nis higher compared to in-domain. We find this misleading and suggest using a\nrandom baseline as a yardstick for evaluating post-hoc explanation\nfaithfulness. Our findings also show that select-then predict models\ndemonstrate comparable predictive performance in out-of-domain settings to\nfull-text trained models.'}, 'authors': [{'name': 'George Chrysostomou'}, {'name': 'Nikolaos Aletras'}], 'author_detail': {'name': 'Nikolaos Aletras'}, 'author': 'Nikolaos Aletras', 'arxiv_comment': 'ACL2022 Pre-print', 'links': [{'href': 'http://arxiv.org/abs/2203.00056v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.00056v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.03802v2,2022-03-16 02:19:10+00:00,2022-03-08 01:47:42+00:00,Understanding Iterative Revision from Human-Written Text,"[arxiv.Result.Author('Wanyu Du'), arxiv.Result.Author('Vipul Raheja'), arxiv.Result.Author('Dhruv Kumar'), arxiv.Result.Author('Zae Myung Kim'), arxiv.Result.Author('Melissa Lopez'), arxiv.Result.Author('Dongyeop Kang')]","Writing is, by nature, a strategic, adaptive, and more importantly, an
iterative process. A crucial part of writing is editing and revising the text.
Previous works on text revision have focused on defining edit intention
taxonomies within a single domain or developing computational models with a
single level of edit granularity, such as sentence-level edits, which differ
from human's revision cycles. This work describes IteraTeR: the first
large-scale, multi-domain, edit-intention annotated corpus of iteratively
revised text. In particular, IteraTeR is collected based on a new framework to
comprehensively model the iterative text revisions that generalize to various
domains of formal writing, edit intentions, revision depths, and granularities.
When we incorporate our annotated edit intentions, both generative and
edit-based text revision models significantly improve automatic evaluations.
Through our work, we better understand the text revision process, making vital
connections between edit intentions and writing quality, enabling the creation
of diverse corpora to support computational modeling of iterative text
revisions.",To appear in ACL2022,,10.18653/v1/2022.acl-long.250,cs.CL,"['cs.CL', 'cs.HC']","[arxiv.Result.Link('http://dx.doi.org/10.18653/v1/2022.acl-long.250', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2203.03802v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.03802v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.03802v2,"{'id': 'http://arxiv.org/abs/2203.03802v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.03802v2', 'updated': '2022-03-16T02:19:10Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=16, tm_hour=2, tm_min=19, tm_sec=10, tm_wday=2, tm_yday=75, tm_isdst=0), 'published': '2022-03-08T01:47:42Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=8, tm_hour=1, tm_min=47, tm_sec=42, tm_wday=1, tm_yday=67, tm_isdst=0), 'title': 'Understanding Iterative Revision from Human-Written Text', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Understanding Iterative Revision from Human-Written Text'}, 'summary': ""Writing is, by nature, a strategic, adaptive, and more importantly, an\niterative process. A crucial part of writing is editing and revising the text.\nPrevious works on text revision have focused on defining edit intention\ntaxonomies within a single domain or developing computational models with a\nsingle level of edit granularity, such as sentence-level edits, which differ\nfrom human's revision cycles. This work describes IteraTeR: the first\nlarge-scale, multi-domain, edit-intention annotated corpus of iteratively\nrevised text. In particular, IteraTeR is collected based on a new framework to\ncomprehensively model the iterative text revisions that generalize to various\ndomains of formal writing, edit intentions, revision depths, and granularities.\nWhen we incorporate our annotated edit intentions, both generative and\nedit-based text revision models significantly improve automatic evaluations.\nThrough our work, we better understand the text revision process, making vital\nconnections between edit intentions and writing quality, enabling the creation\nof diverse corpora to support computational modeling of iterative text\nrevisions."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Writing is, by nature, a strategic, adaptive, and more importantly, an\niterative process. A crucial part of writing is editing and revising the text.\nPrevious works on text revision have focused on defining edit intention\ntaxonomies within a single domain or developing computational models with a\nsingle level of edit granularity, such as sentence-level edits, which differ\nfrom human's revision cycles. This work describes IteraTeR: the first\nlarge-scale, multi-domain, edit-intention annotated corpus of iteratively\nrevised text. In particular, IteraTeR is collected based on a new framework to\ncomprehensively model the iterative text revisions that generalize to various\ndomains of formal writing, edit intentions, revision depths, and granularities.\nWhen we incorporate our annotated edit intentions, both generative and\nedit-based text revision models significantly improve automatic evaluations.\nThrough our work, we better understand the text revision process, making vital\nconnections between edit intentions and writing quality, enabling the creation\nof diverse corpora to support computational modeling of iterative text\nrevisions.""}, 'authors': [{'name': 'Wanyu Du'}, {'name': 'Vipul Raheja'}, {'name': 'Dhruv Kumar'}, {'name': 'Zae Myung Kim'}, {'name': 'Melissa Lopez'}, {'name': 'Dongyeop Kang'}], 'author_detail': {'name': 'Dongyeop Kang'}, 'author': 'Dongyeop Kang', 'arxiv_doi': '10.18653/v1/2022.acl-long.250', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.18653/v1/2022.acl-long.250', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2203.03802v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.03802v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': 'To appear in ACL2022', 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.HC', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.07216v2,2022-10-27 21:29:36+00:00,2022-03-14 15:55:21+00:00,A Novel Perspective to Look At Attention: Bi-level Attention-based Explainable Topic Modeling for News Classification,"[arxiv.Result.Author('Dairui Liu'), arxiv.Result.Author('Derek Greene'), arxiv.Result.Author('Ruihai Dong')]","Many recent deep learning-based solutions have widely adopted the
attention-based mechanism in various tasks of the NLP discipline. However, the
inherent characteristics of deep learning models and the flexibility of the
attention mechanism increase the models' complexity, thus leading to challenges
in model explainability. In this paper, to address this challenge, we propose a
novel practical framework by utilizing a two-tier attention architecture to
decouple the complexity of explanation and the decision-making process. We
apply it in the context of a news article classification task. The experiments
on two large-scaled news corpora demonstrate that the proposed model can
achieve competitive performance with many state-of-the-art alternatives and
illustrate its appropriateness from an explainability perspective.",Findings of ACL2022,,10.18653/v1/2022.findings-acl.178,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://dx.doi.org/10.18653/v1/2022.findings-acl.178', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2203.07216v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.07216v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.07216v2,"{'id': 'http://arxiv.org/abs/2203.07216v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.07216v2', 'updated': '2022-10-27T21:29:36Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=10, tm_mday=27, tm_hour=21, tm_min=29, tm_sec=36, tm_wday=3, tm_yday=300, tm_isdst=0), 'published': '2022-03-14T15:55:21Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=14, tm_hour=15, tm_min=55, tm_sec=21, tm_wday=0, tm_yday=73, tm_isdst=0), 'title': 'A Novel Perspective to Look At Attention: Bi-level Attention-based\n  Explainable Topic Modeling for News Classification', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A Novel Perspective to Look At Attention: Bi-level Attention-based\n  Explainable Topic Modeling for News Classification'}, 'summary': ""Many recent deep learning-based solutions have widely adopted the\nattention-based mechanism in various tasks of the NLP discipline. However, the\ninherent characteristics of deep learning models and the flexibility of the\nattention mechanism increase the models' complexity, thus leading to challenges\nin model explainability. In this paper, to address this challenge, we propose a\nnovel practical framework by utilizing a two-tier attention architecture to\ndecouple the complexity of explanation and the decision-making process. We\napply it in the context of a news article classification task. The experiments\non two large-scaled news corpora demonstrate that the proposed model can\nachieve competitive performance with many state-of-the-art alternatives and\nillustrate its appropriateness from an explainability perspective."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Many recent deep learning-based solutions have widely adopted the\nattention-based mechanism in various tasks of the NLP discipline. However, the\ninherent characteristics of deep learning models and the flexibility of the\nattention mechanism increase the models' complexity, thus leading to challenges\nin model explainability. In this paper, to address this challenge, we propose a\nnovel practical framework by utilizing a two-tier attention architecture to\ndecouple the complexity of explanation and the decision-making process. We\napply it in the context of a news article classification task. The experiments\non two large-scaled news corpora demonstrate that the proposed model can\nachieve competitive performance with many state-of-the-art alternatives and\nillustrate its appropriateness from an explainability perspective.""}, 'authors': [{'name': 'Dairui Liu'}, {'name': 'Derek Greene'}, {'name': 'Ruihai Dong'}], 'author_detail': {'name': 'Ruihai Dong'}, 'author': 'Ruihai Dong', 'arxiv_doi': '10.18653/v1/2022.findings-acl.178', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.18653/v1/2022.findings-acl.178', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2203.07216v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.07216v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': 'Findings of ACL2022', 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.07836v4,2022-05-04 17:02:23+00:00,2022-03-15 12:47:00+00:00,Graph Pre-training for AMR Parsing and Generation,"[arxiv.Result.Author('Xuefeng Bai'), arxiv.Result.Author('Yulong Chen'), arxiv.Result.Author('Yue Zhang')]","Abstract meaning representation (AMR) highlights the core semantic
information of text in a graph structure. Recently, pre-trained language models
(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,
respectively. However, PLMs are typically pre-trained on textual data, thus are
sub-optimal for modeling structural knowledge. To this end, we investigate
graph self-supervised training to improve the structure awareness of PLMs over
AMR graphs. In particular, we introduce two graph auto-encoding strategies for
graph-to-graph pre-training and four tasks to integrate text and graph
information during pre-training. We further design a unified framework to
bridge the gap between pre-training and fine-tuning tasks. Experiments on both
AMR parsing and AMR-to-text generation show the superiority of our model. To
our knowledge, we are the first to consider pre-training on semantic graphs.",ACL2022 camera-ready final version,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.07836v4', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.07836v4', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.07836v4,"{'id': 'http://arxiv.org/abs/2203.07836v4', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.07836v4', 'updated': '2022-05-04T17:02:23Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=4, tm_hour=17, tm_min=2, tm_sec=23, tm_wday=2, tm_yday=124, tm_isdst=0), 'published': '2022-03-15T12:47:00Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=15, tm_hour=12, tm_min=47, tm_sec=0, tm_wday=1, tm_yday=74, tm_isdst=0), 'title': 'Graph Pre-training for AMR Parsing and Generation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Graph Pre-training for AMR Parsing and Generation'}, 'summary': 'Abstract meaning representation (AMR) highlights the core semantic\ninformation of text in a graph structure. Recently, pre-trained language models\n(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,\nrespectively. However, PLMs are typically pre-trained on textual data, thus are\nsub-optimal for modeling structural knowledge. To this end, we investigate\ngraph self-supervised training to improve the structure awareness of PLMs over\nAMR graphs. In particular, we introduce two graph auto-encoding strategies for\ngraph-to-graph pre-training and four tasks to integrate text and graph\ninformation during pre-training. We further design a unified framework to\nbridge the gap between pre-training and fine-tuning tasks. Experiments on both\nAMR parsing and AMR-to-text generation show the superiority of our model. To\nour knowledge, we are the first to consider pre-training on semantic graphs.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Abstract meaning representation (AMR) highlights the core semantic\ninformation of text in a graph structure. Recently, pre-trained language models\n(PLMs) have advanced tasks of AMR parsing and AMR-to-text generation,\nrespectively. However, PLMs are typically pre-trained on textual data, thus are\nsub-optimal for modeling structural knowledge. To this end, we investigate\ngraph self-supervised training to improve the structure awareness of PLMs over\nAMR graphs. In particular, we introduce two graph auto-encoding strategies for\ngraph-to-graph pre-training and four tasks to integrate text and graph\ninformation during pre-training. We further design a unified framework to\nbridge the gap between pre-training and fine-tuning tasks. Experiments on both\nAMR parsing and AMR-to-text generation show the superiority of our model. To\nour knowledge, we are the first to consider pre-training on semantic graphs.'}, 'authors': [{'name': 'Xuefeng Bai'}, {'name': 'Yulong Chen'}, {'name': 'Yue Zhang'}], 'author_detail': {'name': 'Yue Zhang'}, 'author': 'Yue Zhang', 'arxiv_comment': 'ACL2022 camera-ready final version', 'links': [{'href': 'http://arxiv.org/abs/2203.07836v4', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.07836v4', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.08259v1,2022-03-15 21:23:22+00:00,2022-03-15 21:23:22+00:00,Better Quality Estimation for Low Resource Corpus Mining,"[arxiv.Result.Author('Muhammed Yusuf Kocyigit'), arxiv.Result.Author('Jiho Lee'), arxiv.Result.Author('Derry Wijaya')]","Quality Estimation (QE) models have the potential to change how we evaluate
and maybe even train machine translation models. However, these models still
lack the robustness to achieve general adoption. We show that State-of-the-art
QE models, when tested in a Parallel Corpus Mining (PCM) setting, perform
unexpectedly bad due to a lack of robustness to out-of-domain examples. We
propose a combination of multitask training, data augmentation and contrastive
learning to achieve better and more robust QE performance. We show that our
method improves QE performance significantly in the MLQE challenge and the
robustness of QE models when tested in the Parallel Corpus Mining setup. We
increase the accuracy in PCM by more than 0.80, making it on par with
state-of-the-art PCM methods that use millions of sentence pairs to train their
models. In comparison, we use a thousand times less data, 7K parallel sentences
in total, and propose a novel low resource PCM method.",To be published in: Findigs of ACL2022. 9 Pages + Appendix,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2203.08259v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.08259v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.08259v1,"{'id': 'http://arxiv.org/abs/2203.08259v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.08259v1', 'updated': '2022-03-15T21:23:22Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=15, tm_hour=21, tm_min=23, tm_sec=22, tm_wday=1, tm_yday=74, tm_isdst=0), 'published': '2022-03-15T21:23:22Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=15, tm_hour=21, tm_min=23, tm_sec=22, tm_wday=1, tm_yday=74, tm_isdst=0), 'title': 'Better Quality Estimation for Low Resource Corpus Mining', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Better Quality Estimation for Low Resource Corpus Mining'}, 'summary': 'Quality Estimation (QE) models have the potential to change how we evaluate\nand maybe even train machine translation models. However, these models still\nlack the robustness to achieve general adoption. We show that State-of-the-art\nQE models, when tested in a Parallel Corpus Mining (PCM) setting, perform\nunexpectedly bad due to a lack of robustness to out-of-domain examples. We\npropose a combination of multitask training, data augmentation and contrastive\nlearning to achieve better and more robust QE performance. We show that our\nmethod improves QE performance significantly in the MLQE challenge and the\nrobustness of QE models when tested in the Parallel Corpus Mining setup. We\nincrease the accuracy in PCM by more than 0.80, making it on par with\nstate-of-the-art PCM methods that use millions of sentence pairs to train their\nmodels. In comparison, we use a thousand times less data, 7K parallel sentences\nin total, and propose a novel low resource PCM method.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Quality Estimation (QE) models have the potential to change how we evaluate\nand maybe even train machine translation models. However, these models still\nlack the robustness to achieve general adoption. We show that State-of-the-art\nQE models, when tested in a Parallel Corpus Mining (PCM) setting, perform\nunexpectedly bad due to a lack of robustness to out-of-domain examples. We\npropose a combination of multitask training, data augmentation and contrastive\nlearning to achieve better and more robust QE performance. We show that our\nmethod improves QE performance significantly in the MLQE challenge and the\nrobustness of QE models when tested in the Parallel Corpus Mining setup. We\nincrease the accuracy in PCM by more than 0.80, making it on par with\nstate-of-the-art PCM methods that use millions of sentence pairs to train their\nmodels. In comparison, we use a thousand times less data, 7K parallel sentences\nin total, and propose a novel low resource PCM method.'}, 'authors': [{'name': 'Muhammed Yusuf Kocyigit'}, {'name': 'Jiho Lee'}, {'name': 'Derry Wijaya'}], 'author_detail': {'name': 'Derry Wijaya'}, 'author': 'Derry Wijaya', 'arxiv_comment': 'To be published in: Findigs of ACL2022. 9 Pages + Appendix', 'links': [{'href': 'http://arxiv.org/abs/2203.08259v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.08259v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.09136v1,2022-03-17 07:30:05+00:00,2022-03-17 07:30:05+00:00,Type-Driven Multi-Turn Corrections for Grammatical Error Correction,"[arxiv.Result.Author('Shaopeng Lai'), arxiv.Result.Author('Qingyu Zhou'), arxiv.Result.Author('Jiali Zeng'), arxiv.Result.Author('Zhongli Li'), arxiv.Result.Author('Chao Li'), arxiv.Result.Author('Yunbo Cao'), arxiv.Result.Author('Jinsong Su')]","Grammatical Error Correction (GEC) aims to automatically detect and correct
grammatical errors. In this aspect, dominant models are trained by
one-iteration learning while performing multiple iterations of corrections
during inference. Previous studies mainly focus on the data augmentation
approach to combat the exposure bias, which suffers from two drawbacks. First,
they simply mix additionally-constructed training instances and original ones
to train models, which fails to help models be explicitly aware of the
procedure of gradual corrections. Second, they ignore the interdependence
between different types of corrections. In this paper, we propose a Type-Driven
Multi-Turn Corrections approach for GEC. Using this approach, from each
training instance, we additionally construct multiple training instances, each
of which involves the correction of a specific type of errors. Then, we use
these additionally-constructed training instances and the original one to train
the model in turn. Experimental results and in-depth analysis show that our
approach significantly benefits the model training. Particularly, our enhanced
model achieves state-of-the-art single-model performance on English GEC
benchmarks. We release our code at Github.",Findings of ACL2022,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.09136v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.09136v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.09136v1,"{'id': 'http://arxiv.org/abs/2203.09136v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.09136v1', 'updated': '2022-03-17T07:30:05Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=17, tm_hour=7, tm_min=30, tm_sec=5, tm_wday=3, tm_yday=76, tm_isdst=0), 'published': '2022-03-17T07:30:05Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=17, tm_hour=7, tm_min=30, tm_sec=5, tm_wday=3, tm_yday=76, tm_isdst=0), 'title': 'Type-Driven Multi-Turn Corrections for Grammatical Error Correction', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Type-Driven Multi-Turn Corrections for Grammatical Error Correction'}, 'summary': 'Grammatical Error Correction (GEC) aims to automatically detect and correct\ngrammatical errors. In this aspect, dominant models are trained by\none-iteration learning while performing multiple iterations of corrections\nduring inference. Previous studies mainly focus on the data augmentation\napproach to combat the exposure bias, which suffers from two drawbacks. First,\nthey simply mix additionally-constructed training instances and original ones\nto train models, which fails to help models be explicitly aware of the\nprocedure of gradual corrections. Second, they ignore the interdependence\nbetween different types of corrections. In this paper, we propose a Type-Driven\nMulti-Turn Corrections approach for GEC. Using this approach, from each\ntraining instance, we additionally construct multiple training instances, each\nof which involves the correction of a specific type of errors. Then, we use\nthese additionally-constructed training instances and the original one to train\nthe model in turn. Experimental results and in-depth analysis show that our\napproach significantly benefits the model training. Particularly, our enhanced\nmodel achieves state-of-the-art single-model performance on English GEC\nbenchmarks. We release our code at Github.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Grammatical Error Correction (GEC) aims to automatically detect and correct\ngrammatical errors. In this aspect, dominant models are trained by\none-iteration learning while performing multiple iterations of corrections\nduring inference. Previous studies mainly focus on the data augmentation\napproach to combat the exposure bias, which suffers from two drawbacks. First,\nthey simply mix additionally-constructed training instances and original ones\nto train models, which fails to help models be explicitly aware of the\nprocedure of gradual corrections. Second, they ignore the interdependence\nbetween different types of corrections. In this paper, we propose a Type-Driven\nMulti-Turn Corrections approach for GEC. Using this approach, from each\ntraining instance, we additionally construct multiple training instances, each\nof which involves the correction of a specific type of errors. Then, we use\nthese additionally-constructed training instances and the original one to train\nthe model in turn. Experimental results and in-depth analysis show that our\napproach significantly benefits the model training. Particularly, our enhanced\nmodel achieves state-of-the-art single-model performance on English GEC\nbenchmarks. We release our code at Github.'}, 'authors': [{'name': 'Shaopeng Lai'}, {'name': 'Qingyu Zhou'}, {'name': 'Jiali Zeng'}, {'name': 'Zhongli Li'}, {'name': 'Chao Li'}, {'name': 'Yunbo Cao'}, {'name': 'Jinsong Su'}], 'author_detail': {'name': 'Jinsong Su'}, 'author': 'Jinsong Su', 'arxiv_comment': 'Findings of ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2203.09136v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.09136v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.09173v1,2022-03-17 08:51:09+00:00,2022-03-17 08:51:09+00:00,On Vision Features in Multimodal Machine Translation,"[arxiv.Result.Author('Bei Li'), arxiv.Result.Author('Chuanhao Lv'), arxiv.Result.Author('Zefan Zhou'), arxiv.Result.Author('Tao Zhou'), arxiv.Result.Author('Tong Xiao'), arxiv.Result.Author('Anxiang Ma'), arxiv.Result.Author('JingBo Zhu')]","Previous work on multimodal machine translation (MMT) has focused on the way
of incorporating vision features into translation but little attention is on
the quality of vision models. In this work, we investigate the impact of vision
models on MMT. Given the fact that Transformer is becoming popular in computer
vision, we experiment with various strong models (such as Vision Transformer)
and enhanced features (such as object-detection and image captioning). We
develop a selective attention model to study the patch-level contribution of an
image in MMT. On detailed probing tasks, we find that stronger vision models
are helpful for learning translation from the visual modality. Our results also
suggest the need of carefully examining MMT models, especially when current
benchmarks are small-scale and biased. Our code could be found at
\url{https://github.com/libeineu/fairseq_mmt}.",Long paper accepted by ACL2022 main conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.09173v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.09173v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.09173v1,"{'id': 'http://arxiv.org/abs/2203.09173v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.09173v1', 'updated': '2022-03-17T08:51:09Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=17, tm_hour=8, tm_min=51, tm_sec=9, tm_wday=3, tm_yday=76, tm_isdst=0), 'published': '2022-03-17T08:51:09Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=17, tm_hour=8, tm_min=51, tm_sec=9, tm_wday=3, tm_yday=76, tm_isdst=0), 'title': 'On Vision Features in Multimodal Machine Translation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'On Vision Features in Multimodal Machine Translation'}, 'summary': 'Previous work on multimodal machine translation (MMT) has focused on the way\nof incorporating vision features into translation but little attention is on\nthe quality of vision models. In this work, we investigate the impact of vision\nmodels on MMT. Given the fact that Transformer is becoming popular in computer\nvision, we experiment with various strong models (such as Vision Transformer)\nand enhanced features (such as object-detection and image captioning). We\ndevelop a selective attention model to study the patch-level contribution of an\nimage in MMT. On detailed probing tasks, we find that stronger vision models\nare helpful for learning translation from the visual modality. Our results also\nsuggest the need of carefully examining MMT models, especially when current\nbenchmarks are small-scale and biased. Our code could be found at\n\\url{https://github.com/libeineu/fairseq_mmt}.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Previous work on multimodal machine translation (MMT) has focused on the way\nof incorporating vision features into translation but little attention is on\nthe quality of vision models. In this work, we investigate the impact of vision\nmodels on MMT. Given the fact that Transformer is becoming popular in computer\nvision, we experiment with various strong models (such as Vision Transformer)\nand enhanced features (such as object-detection and image captioning). We\ndevelop a selective attention model to study the patch-level contribution of an\nimage in MMT. On detailed probing tasks, we find that stronger vision models\nare helpful for learning translation from the visual modality. Our results also\nsuggest the need of carefully examining MMT models, especially when current\nbenchmarks are small-scale and biased. Our code could be found at\n\\url{https://github.com/libeineu/fairseq_mmt}.'}, 'authors': [{'name': 'Bei Li'}, {'name': 'Chuanhao Lv'}, {'name': 'Zefan Zhou'}, {'name': 'Tao Zhou'}, {'name': 'Tong Xiao'}, {'name': 'Anxiang Ma'}, {'name': 'JingBo Zhu'}], 'author_detail': {'name': 'JingBo Zhu'}, 'author': 'JingBo Zhu', 'arxiv_comment': 'Long paper accepted by ACL2022 main conference', 'links': [{'href': 'http://arxiv.org/abs/2203.09173v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.09173v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.09176v1,2022-03-17 08:54:31+00:00,2022-03-17 08:54:31+00:00,ODE Transformer: An Ordinary Differential Equation-Inspired Model for Sequence Generation,"[arxiv.Result.Author('Bei Li'), arxiv.Result.Author('Quan Du'), arxiv.Result.Author('Tao Zhou'), arxiv.Result.Author('Yi Jing'), arxiv.Result.Author('Shuhan Zhou'), arxiv.Result.Author('Xin Zeng'), arxiv.Result.Author('Tong Xiao'), arxiv.Result.Author('JingBo Zhu'), arxiv.Result.Author('Xuebo Liu'), arxiv.Result.Author('Min Zhang')]","Residual networks are an Euler discretization of solutions to Ordinary
Differential Equations (ODE). This paper explores a deeper relationship between
Transformer and numerical ODE methods. We first show that a residual block of
layers in Transformer can be described as a higher-order solution to ODE.
Inspired by this, we design a new architecture, {\it ODE Transformer}, which is
analogous to the Runge-Kutta method that is well motivated in ODE. As a natural
extension to Transformer, ODE Transformer is easy to implement and efficient to
use. Experimental results on the large-scale machine translation, abstractive
summarization, and grammar error correction tasks demonstrate the high
genericity of ODE Transformer. It can gain large improvements in model
performance over strong baselines (e.g., 30.77 and 44.11 BLEU scores on the
WMT'14 English-German and English-French benchmarks) at a slight cost in
inference efficiency.","Long paper accepted by ACL2022 main conference. arXiv admin note:
  substantial text overlap with arXiv:2104.02308",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.09176v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.09176v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.09176v1,"{'id': 'http://arxiv.org/abs/2203.09176v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.09176v1', 'updated': '2022-03-17T08:54:31Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=17, tm_hour=8, tm_min=54, tm_sec=31, tm_wday=3, tm_yday=76, tm_isdst=0), 'published': '2022-03-17T08:54:31Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=17, tm_hour=8, tm_min=54, tm_sec=31, tm_wday=3, tm_yday=76, tm_isdst=0), 'title': 'ODE Transformer: An Ordinary Differential Equation-Inspired Model for\n  Sequence Generation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'ODE Transformer: An Ordinary Differential Equation-Inspired Model for\n  Sequence Generation'}, 'summary': ""Residual networks are an Euler discretization of solutions to Ordinary\nDifferential Equations (ODE). This paper explores a deeper relationship between\nTransformer and numerical ODE methods. We first show that a residual block of\nlayers in Transformer can be described as a higher-order solution to ODE.\nInspired by this, we design a new architecture, {\\it ODE Transformer}, which is\nanalogous to the Runge-Kutta method that is well motivated in ODE. As a natural\nextension to Transformer, ODE Transformer is easy to implement and efficient to\nuse. Experimental results on the large-scale machine translation, abstractive\nsummarization, and grammar error correction tasks demonstrate the high\ngenericity of ODE Transformer. It can gain large improvements in model\nperformance over strong baselines (e.g., 30.77 and 44.11 BLEU scores on the\nWMT'14 English-German and English-French benchmarks) at a slight cost in\ninference efficiency."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Residual networks are an Euler discretization of solutions to Ordinary\nDifferential Equations (ODE). This paper explores a deeper relationship between\nTransformer and numerical ODE methods. We first show that a residual block of\nlayers in Transformer can be described as a higher-order solution to ODE.\nInspired by this, we design a new architecture, {\\it ODE Transformer}, which is\nanalogous to the Runge-Kutta method that is well motivated in ODE. As a natural\nextension to Transformer, ODE Transformer is easy to implement and efficient to\nuse. Experimental results on the large-scale machine translation, abstractive\nsummarization, and grammar error correction tasks demonstrate the high\ngenericity of ODE Transformer. It can gain large improvements in model\nperformance over strong baselines (e.g., 30.77 and 44.11 BLEU scores on the\nWMT'14 English-German and English-French benchmarks) at a slight cost in\ninference efficiency.""}, 'authors': [{'name': 'Bei Li'}, {'name': 'Quan Du'}, {'name': 'Tao Zhou'}, {'name': 'Yi Jing'}, {'name': 'Shuhan Zhou'}, {'name': 'Xin Zeng'}, {'name': 'Tong Xiao'}, {'name': 'JingBo Zhu'}, {'name': 'Xuebo Liu'}, {'name': 'Min Zhang'}], 'author_detail': {'name': 'Min Zhang'}, 'author': 'Min Zhang', 'arxiv_comment': 'Long paper accepted by ACL2022 main conference. arXiv admin note:\n  substantial text overlap with arXiv:2104.02308', 'links': [{'href': 'http://arxiv.org/abs/2203.09176v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.09176v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.12252v1,2022-03-23 07:56:27+00:00,2022-03-23 07:56:27+00:00,Few-shot Named Entity Recognition with Self-describing Networks,"[arxiv.Result.Author('Jiawei Chen'), arxiv.Result.Author('Qing Liu'), arxiv.Result.Author('Hongyu Lin'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Le Sun')]","Few-shot NER needs to effectively capture information from limited instances
and transfer useful knowledge from external resources. In this paper, we
propose a self-describing mechanism for few-shot NER, which can effectively
leverage illustrative instances and precisely transfer knowledge from external
resources by describing both entity types and mentions using a universal
concept set. Specifically, we design Self-describing Networks (SDNet), a
Seq2Seq generation model which can universally describe mentions using
concepts, automatically map novel entity types to concepts, and adaptively
recognize entities on-demand. We pre-train SDNet with large-scale corpus, and
conduct experiments on 8 benchmarks from different domains. Experiments show
that SDNet achieves competitive performances on all benchmarks and achieves the
new state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and
robustness.",Accepted to the main conference of ACL2022,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.12252v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.12252v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.12252v1,"{'id': 'http://arxiv.org/abs/2203.12252v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.12252v1', 'updated': '2022-03-23T07:56:27Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=7, tm_min=56, tm_sec=27, tm_wday=2, tm_yday=82, tm_isdst=0), 'published': '2022-03-23T07:56:27Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=7, tm_min=56, tm_sec=27, tm_wday=2, tm_yday=82, tm_isdst=0), 'title': 'Few-shot Named Entity Recognition with Self-describing Networks', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Few-shot Named Entity Recognition with Self-describing Networks'}, 'summary': 'Few-shot NER needs to effectively capture information from limited instances\nand transfer useful knowledge from external resources. In this paper, we\npropose a self-describing mechanism for few-shot NER, which can effectively\nleverage illustrative instances and precisely transfer knowledge from external\nresources by describing both entity types and mentions using a universal\nconcept set. Specifically, we design Self-describing Networks (SDNet), a\nSeq2Seq generation model which can universally describe mentions using\nconcepts, automatically map novel entity types to concepts, and adaptively\nrecognize entities on-demand. We pre-train SDNet with large-scale corpus, and\nconduct experiments on 8 benchmarks from different domains. Experiments show\nthat SDNet achieves competitive performances on all benchmarks and achieves the\nnew state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and\nrobustness.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Few-shot NER needs to effectively capture information from limited instances\nand transfer useful knowledge from external resources. In this paper, we\npropose a self-describing mechanism for few-shot NER, which can effectively\nleverage illustrative instances and precisely transfer knowledge from external\nresources by describing both entity types and mentions using a universal\nconcept set. Specifically, we design Self-describing Networks (SDNet), a\nSeq2Seq generation model which can universally describe mentions using\nconcepts, automatically map novel entity types to concepts, and adaptively\nrecognize entities on-demand. We pre-train SDNet with large-scale corpus, and\nconduct experiments on 8 benchmarks from different domains. Experiments show\nthat SDNet achieves competitive performances on all benchmarks and achieves the\nnew state-of-the-art on 6 benchmarks, which demonstrates its effectiveness and\nrobustness.'}, 'authors': [{'name': 'Jiawei Chen'}, {'name': 'Qing Liu'}, {'name': 'Hongyu Lin'}, {'name': 'Xianpei Han'}, {'name': 'Le Sun'}], 'author_detail': {'name': 'Le Sun'}, 'author': 'Le Sun', 'arxiv_comment': 'Accepted to the main conference of ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2203.12252v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.12252v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.12258v1,2022-03-23 08:10:07+00:00,2022-03-23 08:10:07+00:00,Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View,"[arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Hongyu Lin'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Fangchao Liu'), arxiv.Result.Author('Le Sun')]","Prompt-based probing has been widely used in evaluating the abilities of
pretrained language models (PLMs). Unfortunately, recent studies have
discovered such an evaluation may be inaccurate, inconsistent and unreliable.
Furthermore, the lack of understanding its inner workings, combined with its
wide applicability, has the potential to lead to unforeseen risks for
evaluating and applying PLMs in real-world applications. To discover,
understand and quantify the risks, this paper investigates the prompt-based
probing from a causal view, highlights three critical biases which could induce
biased results and conclusions, and proposes to conduct debiasing via causal
intervention. This paper provides valuable insights for the design of unbiased
datasets, better probing frameworks and more reliable evaluations of pretrained
language models. Furthermore, our conclusions also echo that we need to rethink
the criteria for identifying better pretrained language models. We openly
released the source code and data at https://github.com/c-box/causalEval.",Accepted to the main conference of ACL2022,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.12258v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.12258v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.12258v1,"{'id': 'http://arxiv.org/abs/2203.12258v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.12258v1', 'updated': '2022-03-23T08:10:07Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=8, tm_min=10, tm_sec=7, tm_wday=2, tm_yday=82, tm_isdst=0), 'published': '2022-03-23T08:10:07Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=8, tm_min=10, tm_sec=7, tm_wday=2, tm_yday=82, tm_isdst=0), 'title': 'Can Prompt Probe Pretrained Language Models? Understanding the Invisible\n  Risks from a Causal View', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Can Prompt Probe Pretrained Language Models? Understanding the Invisible\n  Risks from a Causal View'}, 'summary': 'Prompt-based probing has been widely used in evaluating the abilities of\npretrained language models (PLMs). Unfortunately, recent studies have\ndiscovered such an evaluation may be inaccurate, inconsistent and unreliable.\nFurthermore, the lack of understanding its inner workings, combined with its\nwide applicability, has the potential to lead to unforeseen risks for\nevaluating and applying PLMs in real-world applications. To discover,\nunderstand and quantify the risks, this paper investigates the prompt-based\nprobing from a causal view, highlights three critical biases which could induce\nbiased results and conclusions, and proposes to conduct debiasing via causal\nintervention. This paper provides valuable insights for the design of unbiased\ndatasets, better probing frameworks and more reliable evaluations of pretrained\nlanguage models. Furthermore, our conclusions also echo that we need to rethink\nthe criteria for identifying better pretrained language models. We openly\nreleased the source code and data at https://github.com/c-box/causalEval.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Prompt-based probing has been widely used in evaluating the abilities of\npretrained language models (PLMs). Unfortunately, recent studies have\ndiscovered such an evaluation may be inaccurate, inconsistent and unreliable.\nFurthermore, the lack of understanding its inner workings, combined with its\nwide applicability, has the potential to lead to unforeseen risks for\nevaluating and applying PLMs in real-world applications. To discover,\nunderstand and quantify the risks, this paper investigates the prompt-based\nprobing from a causal view, highlights three critical biases which could induce\nbiased results and conclusions, and proposes to conduct debiasing via causal\nintervention. This paper provides valuable insights for the design of unbiased\ndatasets, better probing frameworks and more reliable evaluations of pretrained\nlanguage models. Furthermore, our conclusions also echo that we need to rethink\nthe criteria for identifying better pretrained language models. We openly\nreleased the source code and data at https://github.com/c-box/causalEval.'}, 'authors': [{'name': 'Boxi Cao'}, {'name': 'Hongyu Lin'}, {'name': 'Xianpei Han'}, {'name': 'Fangchao Liu'}, {'name': 'Le Sun'}], 'author_detail': {'name': 'Le Sun'}, 'author': 'Le Sun', 'arxiv_comment': 'Accepted to the main conference of ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2203.12258v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.12258v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.12264v1,2022-03-23 08:20:45+00:00,2022-03-23 08:20:45+00:00,ECO v1: Towards Event-Centric Opinion Mining,"[arxiv.Result.Author('Ruoxi Xu'), arxiv.Result.Author('Hongyu Lin'), arxiv.Result.Author('Meng Liao'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Jin Xu'), arxiv.Result.Author('Wei Tan'), arxiv.Result.Author('Yingfei Sun'), arxiv.Result.Author('Le Sun')]","Events are considered as the fundamental building blocks of the world. Mining
event-centric opinions can benefit decision making, people communication, and
social good. Unfortunately, there is little literature addressing event-centric
opinion mining, although which significantly diverges from the well-studied
entity-centric opinion mining in connotation, structure, and expression. In
this paper, we propose and formulate the task of event-centric opinion mining
based on event-argument structure and expression categorizing theory. We also
benchmark this task by constructing a pioneer corpus and designing a two-step
benchmark framework. Experiment results show that event-centric opinion mining
is feasible and challenging, and the proposed task, dataset, and baselines are
beneficial for future studies.",Accepted to Findings of ACL2022,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.12264v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.12264v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.12264v1,"{'id': 'http://arxiv.org/abs/2203.12264v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.12264v1', 'updated': '2022-03-23T08:20:45Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=8, tm_min=20, tm_sec=45, tm_wday=2, tm_yday=82, tm_isdst=0), 'published': '2022-03-23T08:20:45Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=8, tm_min=20, tm_sec=45, tm_wday=2, tm_yday=82, tm_isdst=0), 'title': 'ECO v1: Towards Event-Centric Opinion Mining', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'ECO v1: Towards Event-Centric Opinion Mining'}, 'summary': 'Events are considered as the fundamental building blocks of the world. Mining\nevent-centric opinions can benefit decision making, people communication, and\nsocial good. Unfortunately, there is little literature addressing event-centric\nopinion mining, although which significantly diverges from the well-studied\nentity-centric opinion mining in connotation, structure, and expression. In\nthis paper, we propose and formulate the task of event-centric opinion mining\nbased on event-argument structure and expression categorizing theory. We also\nbenchmark this task by constructing a pioneer corpus and designing a two-step\nbenchmark framework. Experiment results show that event-centric opinion mining\nis feasible and challenging, and the proposed task, dataset, and baselines are\nbeneficial for future studies.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Events are considered as the fundamental building blocks of the world. Mining\nevent-centric opinions can benefit decision making, people communication, and\nsocial good. Unfortunately, there is little literature addressing event-centric\nopinion mining, although which significantly diverges from the well-studied\nentity-centric opinion mining in connotation, structure, and expression. In\nthis paper, we propose and formulate the task of event-centric opinion mining\nbased on event-argument structure and expression categorizing theory. We also\nbenchmark this task by constructing a pioneer corpus and designing a two-step\nbenchmark framework. Experiment results show that event-centric opinion mining\nis feasible and challenging, and the proposed task, dataset, and baselines are\nbeneficial for future studies.'}, 'authors': [{'name': 'Ruoxi Xu'}, {'name': 'Hongyu Lin'}, {'name': 'Meng Liao'}, {'name': 'Xianpei Han'}, {'name': 'Jin Xu'}, {'name': 'Wei Tan'}, {'name': 'Yingfei Sun'}, {'name': 'Le Sun'}], 'author_detail': {'name': 'Le Sun'}, 'author': 'Le Sun', 'arxiv_comment': 'Accepted to Findings of ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2203.12264v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.12264v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.12274v1,2022-03-23 08:43:52+00:00,2022-03-23 08:43:52+00:00,Pre-training to Match for Unified Low-shot Relation Extraction,"[arxiv.Result.Author('Fangchao Liu'), arxiv.Result.Author('Hongyu Lin'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Boxi Cao'), arxiv.Result.Author('Le Sun')]","Low-shot relation extraction~(RE) aims to recognize novel relations with very
few or even no samples, which is critical in real scenario application.
Few-shot and zero-shot RE are two representative low-shot RE tasks, which seem
to be with similar target but require totally different underlying abilities.
In this paper, we propose Multi-Choice Matching Networks to unify low-shot
relation extraction. To fill in the gap between zero-shot and few-shot RE, we
propose the triplet-paraphrase meta-training, which leverages triplet
paraphrase to pre-train zero-shot label matching ability and uses meta-learning
paradigm to learn few-shot instance summarizing ability. Experimental results
on three different low-shot RE tasks show that the proposed method outperforms
strong baselines by a large margin, and achieve the best performance on
few-shot RE leaderboard.",Accepted to the main conference of ACL2022,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.12274v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.12274v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.12274v1,"{'id': 'http://arxiv.org/abs/2203.12274v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.12274v1', 'updated': '2022-03-23T08:43:52Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=8, tm_min=43, tm_sec=52, tm_wday=2, tm_yday=82, tm_isdst=0), 'published': '2022-03-23T08:43:52Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=8, tm_min=43, tm_sec=52, tm_wday=2, tm_yday=82, tm_isdst=0), 'title': 'Pre-training to Match for Unified Low-shot Relation Extraction', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Pre-training to Match for Unified Low-shot Relation Extraction'}, 'summary': 'Low-shot relation extraction~(RE) aims to recognize novel relations with very\nfew or even no samples, which is critical in real scenario application.\nFew-shot and zero-shot RE are two representative low-shot RE tasks, which seem\nto be with similar target but require totally different underlying abilities.\nIn this paper, we propose Multi-Choice Matching Networks to unify low-shot\nrelation extraction. To fill in the gap between zero-shot and few-shot RE, we\npropose the triplet-paraphrase meta-training, which leverages triplet\nparaphrase to pre-train zero-shot label matching ability and uses meta-learning\nparadigm to learn few-shot instance summarizing ability. Experimental results\non three different low-shot RE tasks show that the proposed method outperforms\nstrong baselines by a large margin, and achieve the best performance on\nfew-shot RE leaderboard.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Low-shot relation extraction~(RE) aims to recognize novel relations with very\nfew or even no samples, which is critical in real scenario application.\nFew-shot and zero-shot RE are two representative low-shot RE tasks, which seem\nto be with similar target but require totally different underlying abilities.\nIn this paper, we propose Multi-Choice Matching Networks to unify low-shot\nrelation extraction. To fill in the gap between zero-shot and few-shot RE, we\npropose the triplet-paraphrase meta-training, which leverages triplet\nparaphrase to pre-train zero-shot label matching ability and uses meta-learning\nparadigm to learn few-shot instance summarizing ability. Experimental results\non three different low-shot RE tasks show that the proposed method outperforms\nstrong baselines by a large margin, and achieve the best performance on\nfew-shot RE leaderboard.'}, 'authors': [{'name': 'Fangchao Liu'}, {'name': 'Hongyu Lin'}, {'name': 'Xianpei Han'}, {'name': 'Boxi Cao'}, {'name': 'Le Sun'}], 'author_detail': {'name': 'Le Sun'}, 'author': 'Le Sun', 'arxiv_comment': 'Accepted to the main conference of ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2203.12274v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.12274v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.12277v1,2022-03-23 08:49:29+00:00,2022-03-23 08:49:29+00:00,Unified Structure Generation for Universal Information Extraction,"[arxiv.Result.Author('Yaojie Lu'), arxiv.Result.Author('Qing Liu'), arxiv.Result.Author('Dai Dai'), arxiv.Result.Author('Xinyan Xiao'), arxiv.Result.Author('Hongyu Lin'), arxiv.Result.Author('Xianpei Han'), arxiv.Result.Author('Le Sun'), arxiv.Result.Author('Hua Wu')]","Information extraction suffers from its varying targets, heterogeneous
structures, and demand-specific schemas. In this paper, we propose a unified
text-to-structure generation framework, namely UIE, which can universally model
different IE tasks, adaptively generate targeted structures, and
collaboratively learn general IE abilities from different knowledge sources.
Specifically, UIE uniformly encodes different extraction structures via a
structured extraction language, adaptively generates target extractions via a
schema-based prompt mechanism - structural schema instructor, and captures the
common IE abilities via a large-scale pre-trained text-to-structure model.
Experiments show that UIE achieved the state-of-the-art performance on 4 IE
tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings
for a wide range of entity, relation, event and sentiment extraction tasks and
their unification. These results verified the effectiveness, universality, and
transferability of UIE.",Accepted to the main conference of ACL2022,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2203.12277v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.12277v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.12277v1,"{'id': 'http://arxiv.org/abs/2203.12277v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.12277v1', 'updated': '2022-03-23T08:49:29Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=8, tm_min=49, tm_sec=29, tm_wday=2, tm_yday=82, tm_isdst=0), 'published': '2022-03-23T08:49:29Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=23, tm_hour=8, tm_min=49, tm_sec=29, tm_wday=2, tm_yday=82, tm_isdst=0), 'title': 'Unified Structure Generation for Universal Information Extraction', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Unified Structure Generation for Universal Information Extraction'}, 'summary': 'Information extraction suffers from its varying targets, heterogeneous\nstructures, and demand-specific schemas. In this paper, we propose a unified\ntext-to-structure generation framework, namely UIE, which can universally model\ndifferent IE tasks, adaptively generate targeted structures, and\ncollaboratively learn general IE abilities from different knowledge sources.\nSpecifically, UIE uniformly encodes different extraction structures via a\nstructured extraction language, adaptively generates target extractions via a\nschema-based prompt mechanism - structural schema instructor, and captures the\ncommon IE abilities via a large-scale pre-trained text-to-structure model.\nExperiments show that UIE achieved the state-of-the-art performance on 4 IE\ntasks, 13 datasets, and on all supervised, low-resource, and few-shot settings\nfor a wide range of entity, relation, event and sentiment extraction tasks and\ntheir unification. These results verified the effectiveness, universality, and\ntransferability of UIE.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Information extraction suffers from its varying targets, heterogeneous\nstructures, and demand-specific schemas. In this paper, we propose a unified\ntext-to-structure generation framework, namely UIE, which can universally model\ndifferent IE tasks, adaptively generate targeted structures, and\ncollaboratively learn general IE abilities from different knowledge sources.\nSpecifically, UIE uniformly encodes different extraction structures via a\nstructured extraction language, adaptively generates target extractions via a\nschema-based prompt mechanism - structural schema instructor, and captures the\ncommon IE abilities via a large-scale pre-trained text-to-structure model.\nExperiments show that UIE achieved the state-of-the-art performance on 4 IE\ntasks, 13 datasets, and on all supervised, low-resource, and few-shot settings\nfor a wide range of entity, relation, event and sentiment extraction tasks and\ntheir unification. These results verified the effectiveness, universality, and\ntransferability of UIE.'}, 'authors': [{'name': 'Yaojie Lu'}, {'name': 'Qing Liu'}, {'name': 'Dai Dai'}, {'name': 'Xinyan Xiao'}, {'name': 'Hongyu Lin'}, {'name': 'Xianpei Han'}, {'name': 'Le Sun'}, {'name': 'Hua Wu'}], 'author_detail': {'name': 'Hua Wu'}, 'author': 'Hua Wu', 'arxiv_comment': 'Accepted to the main conference of ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2203.12277v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.12277v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2204.01227v1,2022-04-04 04:09:15+00:00,2022-04-04 04:09:15+00:00,Diverse Text Generation via Variational Encoder-Decoder Models with Gaussian Process Priors,"[arxiv.Result.Author('Wanyu Du'), arxiv.Result.Author('Jianqiao Zhao'), arxiv.Result.Author('Liwei Wang'), arxiv.Result.Author('Yangfeng Ji')]","Generating high quality texts with high diversity is important for many NLG
applications, but current methods mostly focus on building deterministic models
to generate higher quality texts and do not provide many options for promoting
diversity. In this work, we present a novel latent structured variable model to
generate high quality texts by enriching contextual representation learning of
encoder-decoder models. Specifically, we introduce a stochastic function to map
deterministic encoder hidden states into random context variables. The proposed
stochastic function is sampled from a Gaussian process prior to (1) provide
infinite number of joint Gaussian distributions of random context variables
(diversity-promoting) and (2) explicitly model dependency between context
variables (accurate-encoding). To address the learning challenge of Gaussian
processes, we propose an efficient variational inference approach to
approximate the posterior distribution of random context variables. We evaluate
our method in two typical text generation tasks: paraphrase generation and text
style transfer. Experimental results on benchmark datasets demonstrate that our
method improves the generation quality and diversity compared with other
baselines.",Accepted by 6th Workshop on Structured Prediction for NLP at ACL2022,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2204.01227v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.01227v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2204.01227v1,"{'id': 'http://arxiv.org/abs/2204.01227v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2204.01227v1', 'updated': '2022-04-04T04:09:15Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=4, tm_hour=4, tm_min=9, tm_sec=15, tm_wday=0, tm_yday=94, tm_isdst=0), 'published': '2022-04-04T04:09:15Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=4, tm_hour=4, tm_min=9, tm_sec=15, tm_wday=0, tm_yday=94, tm_isdst=0), 'title': 'Diverse Text Generation via Variational Encoder-Decoder Models with\n  Gaussian Process Priors', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Diverse Text Generation via Variational Encoder-Decoder Models with\n  Gaussian Process Priors'}, 'summary': 'Generating high quality texts with high diversity is important for many NLG\napplications, but current methods mostly focus on building deterministic models\nto generate higher quality texts and do not provide many options for promoting\ndiversity. In this work, we present a novel latent structured variable model to\ngenerate high quality texts by enriching contextual representation learning of\nencoder-decoder models. Specifically, we introduce a stochastic function to map\ndeterministic encoder hidden states into random context variables. The proposed\nstochastic function is sampled from a Gaussian process prior to (1) provide\ninfinite number of joint Gaussian distributions of random context variables\n(diversity-promoting) and (2) explicitly model dependency between context\nvariables (accurate-encoding). To address the learning challenge of Gaussian\nprocesses, we propose an efficient variational inference approach to\napproximate the posterior distribution of random context variables. We evaluate\nour method in two typical text generation tasks: paraphrase generation and text\nstyle transfer. Experimental results on benchmark datasets demonstrate that our\nmethod improves the generation quality and diversity compared with other\nbaselines.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Generating high quality texts with high diversity is important for many NLG\napplications, but current methods mostly focus on building deterministic models\nto generate higher quality texts and do not provide many options for promoting\ndiversity. In this work, we present a novel latent structured variable model to\ngenerate high quality texts by enriching contextual representation learning of\nencoder-decoder models. Specifically, we introduce a stochastic function to map\ndeterministic encoder hidden states into random context variables. The proposed\nstochastic function is sampled from a Gaussian process prior to (1) provide\ninfinite number of joint Gaussian distributions of random context variables\n(diversity-promoting) and (2) explicitly model dependency between context\nvariables (accurate-encoding). To address the learning challenge of Gaussian\nprocesses, we propose an efficient variational inference approach to\napproximate the posterior distribution of random context variables. We evaluate\nour method in two typical text generation tasks: paraphrase generation and text\nstyle transfer. Experimental results on benchmark datasets demonstrate that our\nmethod improves the generation quality and diversity compared with other\nbaselines.'}, 'authors': [{'name': 'Wanyu Du'}, {'name': 'Jianqiao Zhao'}, {'name': 'Liwei Wang'}, {'name': 'Yangfeng Ji'}], 'author_detail': {'name': 'Yangfeng Ji'}, 'author': 'Yangfeng Ji', 'arxiv_comment': 'Accepted by 6th Workshop on Structured Prediction for NLP at ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2204.01227v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2204.01227v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2204.02261v1,2022-04-05 14:52:18+00:00,2022-04-05 14:52:18+00:00,Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors,"[arxiv.Result.Author('Isar Nejadgholi'), arxiv.Result.Author('Kathleen C. Fraser'), arxiv.Result.Author('Svetlana Kiritchenko')]","Robustness of machine learning models on ever-changing real-world data is
critical, especially for applications affecting human well-being such as
content moderation. New kinds of abusive language continually emerge in online
discussions in response to current events (e.g., COVID-19), and the deployed
abuse detection systems should be updated regularly to remain accurate. In this
paper, we show that general abusive language classifiers tend to be fairly
reliable in detecting out-of-domain explicitly abusive utterances but fail to
detect new types of more subtle, implicit abuse. Next, we propose an
interpretability technique, based on the Testing Concept Activation Vector
(TCAV) method from computer vision, to quantify the sensitivity of a trained
model to the human-defined concepts of explicit and implicit abusive language,
and use that to explain the generalizability of the model on new data, in this
case, COVID-related anti-Asian hate speech. Extending this technique, we
introduce a novel metric, Degree of Explicitness, for a single instance and
show that the new metric is beneficial in suggesting out-of-domain unlabeled
examples to effectively enrich the training data with informative, implicitly
abusive texts.",accepted to be published at ACL2022,,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2204.02261v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.02261v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2204.02261v1,"{'id': 'http://arxiv.org/abs/2204.02261v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2204.02261v1', 'updated': '2022-04-05T14:52:18Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=5, tm_hour=14, tm_min=52, tm_sec=18, tm_wday=1, tm_yday=95, tm_isdst=0), 'published': '2022-04-05T14:52:18Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=5, tm_hour=14, tm_min=52, tm_sec=18, tm_wday=1, tm_yday=95, tm_isdst=0), 'title': 'Improving Generalizability in Implicitly Abusive Language Detection with\n  Concept Activation Vectors', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Improving Generalizability in Implicitly Abusive Language Detection with\n  Concept Activation Vectors'}, 'summary': 'Robustness of machine learning models on ever-changing real-world data is\ncritical, especially for applications affecting human well-being such as\ncontent moderation. New kinds of abusive language continually emerge in online\ndiscussions in response to current events (e.g., COVID-19), and the deployed\nabuse detection systems should be updated regularly to remain accurate. In this\npaper, we show that general abusive language classifiers tend to be fairly\nreliable in detecting out-of-domain explicitly abusive utterances but fail to\ndetect new types of more subtle, implicit abuse. Next, we propose an\ninterpretability technique, based on the Testing Concept Activation Vector\n(TCAV) method from computer vision, to quantify the sensitivity of a trained\nmodel to the human-defined concepts of explicit and implicit abusive language,\nand use that to explain the generalizability of the model on new data, in this\ncase, COVID-related anti-Asian hate speech. Extending this technique, we\nintroduce a novel metric, Degree of Explicitness, for a single instance and\nshow that the new metric is beneficial in suggesting out-of-domain unlabeled\nexamples to effectively enrich the training data with informative, implicitly\nabusive texts.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Robustness of machine learning models on ever-changing real-world data is\ncritical, especially for applications affecting human well-being such as\ncontent moderation. New kinds of abusive language continually emerge in online\ndiscussions in response to current events (e.g., COVID-19), and the deployed\nabuse detection systems should be updated regularly to remain accurate. In this\npaper, we show that general abusive language classifiers tend to be fairly\nreliable in detecting out-of-domain explicitly abusive utterances but fail to\ndetect new types of more subtle, implicit abuse. Next, we propose an\ninterpretability technique, based on the Testing Concept Activation Vector\n(TCAV) method from computer vision, to quantify the sensitivity of a trained\nmodel to the human-defined concepts of explicit and implicit abusive language,\nand use that to explain the generalizability of the model on new data, in this\ncase, COVID-related anti-Asian hate speech. Extending this technique, we\nintroduce a novel metric, Degree of Explicitness, for a single instance and\nshow that the new metric is beneficial in suggesting out-of-domain unlabeled\nexamples to effectively enrich the training data with informative, implicitly\nabusive texts.'}, 'authors': [{'name': 'Isar Nejadgholi'}, {'name': 'Kathleen C. Fraser'}, {'name': 'Svetlana Kiritchenko'}], 'author_detail': {'name': 'Svetlana Kiritchenko'}, 'author': 'Svetlana Kiritchenko', 'arxiv_comment': 'accepted to be published at ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2204.02261v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2204.02261v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2204.07299v1,2022-04-15 02:30:45+00:00,2022-04-15 02:30:45+00:00,Where to Go for the Holidays: Towards Mixed-Type Dialogs for Clarification of User Goals,"[arxiv.Result.Author('Zeming Liu'), arxiv.Result.Author('Jun Xu'), arxiv.Result.Author('Zeyang Lei'), arxiv.Result.Author('Haifeng Wang'), arxiv.Result.Author('Zheng-Yu Niu'), arxiv.Result.Author('Hua Wu')]","Most dialog systems posit that users have figured out clear and specific
goals before starting an interaction. For example, users have determined the
departure, the destination, and the travel time for booking a flight. However,
in many scenarios, limited by experience and knowledge, users may know what
they need, but still struggle to figure out clear and specific goals by
determining all the necessary slots.
  In this paper, we identify this challenge and make a step forward by
collecting a new human-to-human mixed-type dialog corpus. It contains 5k dialog
sessions and 168k utterances for 4 dialog types and 5 domains. Within each
session, an agent first provides user-goal-related knowledge to help figure out
clear and specific goals, and then help achieve them.
  Furthermore, we propose a mixed-type dialog model with a novel Prompt-based
continual learning mechanism. Specifically, the mechanism enables the model to
continually strengthen its ability on any specific type by utilizing existing
dialog corpora effectively.","ACL2022 Main conference. First two authors contributed equally to
  this work",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2204.07299v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.07299v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2204.07299v1,"{'id': 'http://arxiv.org/abs/2204.07299v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2204.07299v1', 'updated': '2022-04-15T02:30:45Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=15, tm_hour=2, tm_min=30, tm_sec=45, tm_wday=4, tm_yday=105, tm_isdst=0), 'published': '2022-04-15T02:30:45Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=15, tm_hour=2, tm_min=30, tm_sec=45, tm_wday=4, tm_yday=105, tm_isdst=0), 'title': 'Where to Go for the Holidays: Towards Mixed-Type Dialogs for\n  Clarification of User Goals', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Where to Go for the Holidays: Towards Mixed-Type Dialogs for\n  Clarification of User Goals'}, 'summary': 'Most dialog systems posit that users have figured out clear and specific\ngoals before starting an interaction. For example, users have determined the\ndeparture, the destination, and the travel time for booking a flight. However,\nin many scenarios, limited by experience and knowledge, users may know what\nthey need, but still struggle to figure out clear and specific goals by\ndetermining all the necessary slots.\n  In this paper, we identify this challenge and make a step forward by\ncollecting a new human-to-human mixed-type dialog corpus. It contains 5k dialog\nsessions and 168k utterances for 4 dialog types and 5 domains. Within each\nsession, an agent first provides user-goal-related knowledge to help figure out\nclear and specific goals, and then help achieve them.\n  Furthermore, we propose a mixed-type dialog model with a novel Prompt-based\ncontinual learning mechanism. Specifically, the mechanism enables the model to\ncontinually strengthen its ability on any specific type by utilizing existing\ndialog corpora effectively.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Most dialog systems posit that users have figured out clear and specific\ngoals before starting an interaction. For example, users have determined the\ndeparture, the destination, and the travel time for booking a flight. However,\nin many scenarios, limited by experience and knowledge, users may know what\nthey need, but still struggle to figure out clear and specific goals by\ndetermining all the necessary slots.\n  In this paper, we identify this challenge and make a step forward by\ncollecting a new human-to-human mixed-type dialog corpus. It contains 5k dialog\nsessions and 168k utterances for 4 dialog types and 5 domains. Within each\nsession, an agent first provides user-goal-related knowledge to help figure out\nclear and specific goals, and then help achieve them.\n  Furthermore, we propose a mixed-type dialog model with a novel Prompt-based\ncontinual learning mechanism. Specifically, the mechanism enables the model to\ncontinually strengthen its ability on any specific type by utilizing existing\ndialog corpora effectively.'}, 'authors': [{'name': 'Zeming Liu'}, {'name': 'Jun Xu'}, {'name': 'Zeyang Lei'}, {'name': 'Haifeng Wang'}, {'name': 'Zheng-Yu Niu'}, {'name': 'Hua Wu'}], 'author_detail': {'name': 'Hua Wu'}, 'author': 'Hua Wu', 'arxiv_comment': 'ACL2022 Main conference. First two authors contributed equally to\n  this work', 'links': [{'href': 'http://arxiv.org/abs/2204.07299v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2204.07299v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2204.08325v1,2022-04-18 13:56:58+00:00,2022-04-18 13:56:58+00:00,GL-CLeF: A Global-Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding,"[arxiv.Result.Author('Libo Qin'), arxiv.Result.Author('Qiguang Chen'), arxiv.Result.Author('Tianbao Xie'), arxiv.Result.Author('Qixin Li'), arxiv.Result.Author('Jian-Guang Lou'), arxiv.Result.Author('Wanxiang Che'), arxiv.Result.Author('Min-Yen Kan')]","Due to high data demands of current methods, attention to zero-shot
cross-lingual spoken language understanding (SLU) has grown, as such approaches
greatly reduce human annotation effort. However, existing models solely rely on
shared parameters, which can only perform implicit alignment across languages.
We present Global--Local Contrastive Learning Framework (GL-CLeF) to address
this shortcoming. Specifically, we employ contrastive learning, leveraging
bilingual dictionaries to construct multilingual views of the same utterance,
then encourage their representations to be more similar than negative example
pairs, which achieves to explicitly aligned representations of similar
sentences across languages. In addition, a key step in GL-CLeF is a proposed
Local and Global component, which achieves a fine-grained cross-lingual
transfer (i.e., sentence-level Local intent transfer, token-level Local slot
transfer, and semantic-level Global transfer across intent and slot).
Experiments on MultiATIS++ show that GL-CLeF achieves the best performance and
successfully pulls representations of similar sentences across languages
closer.",Accepted at ACL2022 Main Conference,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2204.08325v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.08325v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2204.08325v1,"{'id': 'http://arxiv.org/abs/2204.08325v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2204.08325v1', 'updated': '2022-04-18T13:56:58Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=18, tm_hour=13, tm_min=56, tm_sec=58, tm_wday=0, tm_yday=108, tm_isdst=0), 'published': '2022-04-18T13:56:58Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=18, tm_hour=13, tm_min=56, tm_sec=58, tm_wday=0, tm_yday=108, tm_isdst=0), 'title': 'GL-CLeF: A Global-Local Contrastive Learning Framework for Cross-lingual\n  Spoken Language Understanding', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'GL-CLeF: A Global-Local Contrastive Learning Framework for Cross-lingual\n  Spoken Language Understanding'}, 'summary': 'Due to high data demands of current methods, attention to zero-shot\ncross-lingual spoken language understanding (SLU) has grown, as such approaches\ngreatly reduce human annotation effort. However, existing models solely rely on\nshared parameters, which can only perform implicit alignment across languages.\nWe present Global--Local Contrastive Learning Framework (GL-CLeF) to address\nthis shortcoming. Specifically, we employ contrastive learning, leveraging\nbilingual dictionaries to construct multilingual views of the same utterance,\nthen encourage their representations to be more similar than negative example\npairs, which achieves to explicitly aligned representations of similar\nsentences across languages. In addition, a key step in GL-CLeF is a proposed\nLocal and Global component, which achieves a fine-grained cross-lingual\ntransfer (i.e., sentence-level Local intent transfer, token-level Local slot\ntransfer, and semantic-level Global transfer across intent and slot).\nExperiments on MultiATIS++ show that GL-CLeF achieves the best performance and\nsuccessfully pulls representations of similar sentences across languages\ncloser.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Due to high data demands of current methods, attention to zero-shot\ncross-lingual spoken language understanding (SLU) has grown, as such approaches\ngreatly reduce human annotation effort. However, existing models solely rely on\nshared parameters, which can only perform implicit alignment across languages.\nWe present Global--Local Contrastive Learning Framework (GL-CLeF) to address\nthis shortcoming. Specifically, we employ contrastive learning, leveraging\nbilingual dictionaries to construct multilingual views of the same utterance,\nthen encourage their representations to be more similar than negative example\npairs, which achieves to explicitly aligned representations of similar\nsentences across languages. In addition, a key step in GL-CLeF is a proposed\nLocal and Global component, which achieves a fine-grained cross-lingual\ntransfer (i.e., sentence-level Local intent transfer, token-level Local slot\ntransfer, and semantic-level Global transfer across intent and slot).\nExperiments on MultiATIS++ show that GL-CLeF achieves the best performance and\nsuccessfully pulls representations of similar sentences across languages\ncloser.'}, 'authors': [{'name': 'Libo Qin'}, {'name': 'Qiguang Chen'}, {'name': 'Tianbao Xie'}, {'name': 'Qixin Li'}, {'name': 'Jian-Guang Lou'}, {'name': 'Wanxiang Che'}, {'name': 'Min-Yen Kan'}], 'author_detail': {'name': 'Min-Yen Kan'}, 'author': 'Min-Yen Kan', 'arxiv_comment': 'Accepted at ACL2022 Main Conference', 'links': [{'href': 'http://arxiv.org/abs/2204.08325v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2204.08325v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2204.09597v2,2022-04-21 06:18:34+00:00,2022-03-20 04:23:57+00:00,Perceiving the World: Question-guided Reinforcement Learning for Text-based Games,"[arxiv.Result.Author('Yunqiu Xu'), arxiv.Result.Author('Meng Fang'), arxiv.Result.Author('Ling Chen'), arxiv.Result.Author('Yali Du'), arxiv.Result.Author('Joey Tianyi Zhou'), arxiv.Result.Author('Chengqi Zhang')]","Text-based games provide an interactive way to study natural language
processing. While deep reinforcement learning has shown effectiveness in
developing the game playing agent, the low sample efficiency and the large
action space remain to be the two major challenges that hinder the DRL from
being applied in the real world. In this paper, we address the challenges by
introducing world-perceiving modules, which automatically decompose tasks and
prune actions by answering questions about the environment. We then propose a
two-phase training framework to decouple language learning from reinforcement
learning, which further improves the sample efficiency. The experimental
results show that the proposed method significantly improves the performance
and sample efficiency. Besides, it shows robustness against compound error and
limited pre-training data.","ACL2022, fix some typos",,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2204.09597v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.09597v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2204.09597v2,"{'id': 'http://arxiv.org/abs/2204.09597v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2204.09597v2', 'updated': '2022-04-21T06:18:34Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=21, tm_hour=6, tm_min=18, tm_sec=34, tm_wday=3, tm_yday=111, tm_isdst=0), 'published': '2022-03-20T04:23:57Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=20, tm_hour=4, tm_min=23, tm_sec=57, tm_wday=6, tm_yday=79, tm_isdst=0), 'title': 'Perceiving the World: Question-guided Reinforcement Learning for\n  Text-based Games', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Perceiving the World: Question-guided Reinforcement Learning for\n  Text-based Games'}, 'summary': 'Text-based games provide an interactive way to study natural language\nprocessing. While deep reinforcement learning has shown effectiveness in\ndeveloping the game playing agent, the low sample efficiency and the large\naction space remain to be the two major challenges that hinder the DRL from\nbeing applied in the real world. In this paper, we address the challenges by\nintroducing world-perceiving modules, which automatically decompose tasks and\nprune actions by answering questions about the environment. We then propose a\ntwo-phase training framework to decouple language learning from reinforcement\nlearning, which further improves the sample efficiency. The experimental\nresults show that the proposed method significantly improves the performance\nand sample efficiency. Besides, it shows robustness against compound error and\nlimited pre-training data.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Text-based games provide an interactive way to study natural language\nprocessing. While deep reinforcement learning has shown effectiveness in\ndeveloping the game playing agent, the low sample efficiency and the large\naction space remain to be the two major challenges that hinder the DRL from\nbeing applied in the real world. In this paper, we address the challenges by\nintroducing world-perceiving modules, which automatically decompose tasks and\nprune actions by answering questions about the environment. We then propose a\ntwo-phase training framework to decouple language learning from reinforcement\nlearning, which further improves the sample efficiency. The experimental\nresults show that the proposed method significantly improves the performance\nand sample efficiency. Besides, it shows robustness against compound error and\nlimited pre-training data.'}, 'authors': [{'name': 'Yunqiu Xu'}, {'name': 'Meng Fang'}, {'name': 'Ling Chen'}, {'name': 'Yali Du'}, {'name': 'Joey Tianyi Zhou'}, {'name': 'Chengqi Zhang'}], 'author_detail': {'name': 'Chengqi Zhang'}, 'author': 'Chengqi Zhang', 'arxiv_comment': 'ACL2022, fix some typos', 'links': [{'href': 'http://arxiv.org/abs/2204.09597v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2204.09597v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2204.13346v1,2022-04-28 08:35:26+00:00,2022-04-28 08:35:26+00:00,UniTE: Unified Translation Evaluation,"[arxiv.Result.Author('Yu Wan'), arxiv.Result.Author('Dayiheng Liu'), arxiv.Result.Author('Baosong Yang'), arxiv.Result.Author('Haibo Zhang'), arxiv.Result.Author('Boxing Chen'), arxiv.Result.Author('Derek F. Wong'), arxiv.Result.Author('Lidia S. Chao')]","Translation quality evaluation plays a crucial role in machine translation.
According to the input format, it is mainly separated into three tasks, i.e.,
reference-only, source-only and source-reference-combined. Recent methods,
despite their promising results, are specifically designed and optimized on one
of them. This limits the convenience of these methods, and overlooks the
commonalities among tasks. In this paper, we propose UniTE, which is the first
unified framework engaged with abilities to handle all three evaluation tasks.
Concretely, we propose monotonic regional attention to control the interaction
among input segments, and unified pretraining to better adapt multi-task
learning. We testify our framework on WMT 2019 Metrics and WMT 2020 Quality
Estimation benchmarks. Extensive analyses show that our \textit{single model}
can universally surpass various state-of-the-art or winner methods across
tasks. Both source code and associated models are available at
https://github.com/NLP2CT/UniTE.",ACL2022,,10.18653/v1/2022.acl-long.558,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://dx.doi.org/10.18653/v1/2022.acl-long.558', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2204.13346v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.13346v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2204.13346v1,"{'id': 'http://arxiv.org/abs/2204.13346v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2204.13346v1', 'updated': '2022-04-28T08:35:26Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=28, tm_hour=8, tm_min=35, tm_sec=26, tm_wday=3, tm_yday=118, tm_isdst=0), 'published': '2022-04-28T08:35:26Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=28, tm_hour=8, tm_min=35, tm_sec=26, tm_wday=3, tm_yday=118, tm_isdst=0), 'title': 'UniTE: Unified Translation Evaluation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'UniTE: Unified Translation Evaluation'}, 'summary': 'Translation quality evaluation plays a crucial role in machine translation.\nAccording to the input format, it is mainly separated into three tasks, i.e.,\nreference-only, source-only and source-reference-combined. Recent methods,\ndespite their promising results, are specifically designed and optimized on one\nof them. This limits the convenience of these methods, and overlooks the\ncommonalities among tasks. In this paper, we propose UniTE, which is the first\nunified framework engaged with abilities to handle all three evaluation tasks.\nConcretely, we propose monotonic regional attention to control the interaction\namong input segments, and unified pretraining to better adapt multi-task\nlearning. We testify our framework on WMT 2019 Metrics and WMT 2020 Quality\nEstimation benchmarks. Extensive analyses show that our \\textit{single model}\ncan universally surpass various state-of-the-art or winner methods across\ntasks. Both source code and associated models are available at\nhttps://github.com/NLP2CT/UniTE.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Translation quality evaluation plays a crucial role in machine translation.\nAccording to the input format, it is mainly separated into three tasks, i.e.,\nreference-only, source-only and source-reference-combined. Recent methods,\ndespite their promising results, are specifically designed and optimized on one\nof them. This limits the convenience of these methods, and overlooks the\ncommonalities among tasks. In this paper, we propose UniTE, which is the first\nunified framework engaged with abilities to handle all three evaluation tasks.\nConcretely, we propose monotonic regional attention to control the interaction\namong input segments, and unified pretraining to better adapt multi-task\nlearning. We testify our framework on WMT 2019 Metrics and WMT 2020 Quality\nEstimation benchmarks. Extensive analyses show that our \\textit{single model}\ncan universally surpass various state-of-the-art or winner methods across\ntasks. Both source code and associated models are available at\nhttps://github.com/NLP2CT/UniTE.'}, 'authors': [{'name': 'Yu Wan'}, {'name': 'Dayiheng Liu'}, {'name': 'Baosong Yang'}, {'name': 'Haibo Zhang'}, {'name': 'Boxing Chen'}, {'name': 'Derek F. Wong'}, {'name': 'Lidia S. Chao'}], 'author_detail': {'name': 'Lidia S. Chao'}, 'author': 'Lidia S. Chao', 'arxiv_doi': '10.18653/v1/2022.acl-long.558', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.18653/v1/2022.acl-long.558', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2204.13346v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2204.13346v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': 'ACL2022', 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2205.04404v1,2022-05-09 16:19:28+00:00,2022-05-09 16:19:28+00:00,TeamX@DravidianLangTech-ACL2022: A Comparative Analysis for Troll-Based Meme Classification,"[arxiv.Result.Author('Rabindra Nath Nandi'), arxiv.Result.Author('Firoj Alam'), arxiv.Result.Author('Preslav Nakov')]","The spread of fake news, propaganda, misinformation, disinformation, and
harmful content online raised concerns among social media platforms, government
agencies, policymakers, and society as a whole. This is because such harmful or
abusive content leads to several consequences to people such as physical,
emotional, relational, and financial. Among different harmful content
\textit{trolling-based} online content is one of them, where the idea is to
post a message that is provocative, offensive, or menacing with an intent to
mislead the audience. The content can be textual, visual, a combination of
both, or a meme. In this study, we provide a comparative analysis of
troll-based memes classification using the textual, visual, and multimodal
content. We report several interesting findings in terms of code-mixed text,
multimodal setting, and combining an additional dataset, which shows
improvements over the majority baseline.","Accepted at DravidianLangTech-ACL2022 (Colocated with ACL-2022).
  disinformation, misinformation, factuality, harmfulness, fake news,
  propaganda, multimodality, text, images, videos, network structure,
  temporality",,,cs.CL,"['cs.CL', 'cs.AI', 'cs.CV', 'cs.MM', 'cs.SI', '68T50', 'I.2.7']","[arxiv.Result.Link('http://arxiv.org/abs/2205.04404v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.04404v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2205.04404v1,"{'id': 'http://arxiv.org/abs/2205.04404v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2205.04404v1', 'updated': '2022-05-09T16:19:28Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=9, tm_hour=16, tm_min=19, tm_sec=28, tm_wday=0, tm_yday=129, tm_isdst=0), 'published': '2022-05-09T16:19:28Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=9, tm_hour=16, tm_min=19, tm_sec=28, tm_wday=0, tm_yday=129, tm_isdst=0), 'title': 'TeamX@DravidianLangTech-ACL2022: A Comparative Analysis for Troll-Based\n  Meme Classification', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'TeamX@DravidianLangTech-ACL2022: A Comparative Analysis for Troll-Based\n  Meme Classification'}, 'summary': 'The spread of fake news, propaganda, misinformation, disinformation, and\nharmful content online raised concerns among social media platforms, government\nagencies, policymakers, and society as a whole. This is because such harmful or\nabusive content leads to several consequences to people such as physical,\nemotional, relational, and financial. Among different harmful content\n\\textit{trolling-based} online content is one of them, where the idea is to\npost a message that is provocative, offensive, or menacing with an intent to\nmislead the audience. The content can be textual, visual, a combination of\nboth, or a meme. In this study, we provide a comparative analysis of\ntroll-based memes classification using the textual, visual, and multimodal\ncontent. We report several interesting findings in terms of code-mixed text,\nmultimodal setting, and combining an additional dataset, which shows\nimprovements over the majority baseline.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The spread of fake news, propaganda, misinformation, disinformation, and\nharmful content online raised concerns among social media platforms, government\nagencies, policymakers, and society as a whole. This is because such harmful or\nabusive content leads to several consequences to people such as physical,\nemotional, relational, and financial. Among different harmful content\n\\textit{trolling-based} online content is one of them, where the idea is to\npost a message that is provocative, offensive, or menacing with an intent to\nmislead the audience. The content can be textual, visual, a combination of\nboth, or a meme. In this study, we provide a comparative analysis of\ntroll-based memes classification using the textual, visual, and multimodal\ncontent. We report several interesting findings in terms of code-mixed text,\nmultimodal setting, and combining an additional dataset, which shows\nimprovements over the majority baseline.'}, 'authors': [{'name': 'Rabindra Nath Nandi'}, {'name': 'Firoj Alam'}, {'name': 'Preslav Nakov'}], 'author_detail': {'name': 'Preslav Nakov'}, 'author': 'Preslav Nakov', 'arxiv_comment': 'Accepted at DravidianLangTech-ACL2022 (Colocated with ACL-2022).\n  disinformation, misinformation, factuality, harmfulness, fake news,\n  propaganda, multimodality, text, images, videos, network structure,\n  temporality', 'links': [{'href': 'http://arxiv.org/abs/2205.04404v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.04404v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.MM', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.SI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': '68T50', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'I.2.7', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2205.05124v1,2022-05-10 19:04:37+00:00,2022-05-10 19:04:37+00:00,Extracting Latent Steering Vectors from Pretrained Language Models,"[arxiv.Result.Author('Nishant Subramani'), arxiv.Result.Author('Nivedita Suresh'), arxiv.Result.Author('Matthew E. Peters')]","Prior work on controllable text generation has focused on learning how to
control language models through trainable decoding, smart-prompt design, or
fine-tuning based on a desired objective. We hypothesize that the information
needed to steer the model to generate a target sentence is already encoded
within the model. Accordingly, we explore a different approach altogether:
extracting latent vectors directly from pretrained language model decoders
without fine-tuning. Experiments show that there exist steering vectors, which,
when added to the hidden states of the language model, generate a target
sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of
domains. We show that vector arithmetic can be used for unsupervised sentiment
transfer on the Yelp sentiment benchmark, with performance comparable to models
tailored to this task. We find that distances between steering vectors reflect
sentence similarity when evaluated on a textual similarity benchmark (STS-B),
outperforming pooled hidden states of models. Finally, we present an analysis
of the intrinsic properties of the steering vectors. Taken together, our
results suggest that frozen LMs can be effectively controlled through their
latent steering space.","Accepted to ACL2022 Findings; 16 pages (9 pages plus references and
  appendices); Code: https://github.com/nishantsubramani/steering_vectors; Some
  text overlap with arXiv:2008.09049",,,cs.CL,"['cs.CL', 'cs.AI', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2205.05124v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.05124v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2205.05124v1,"{'id': 'http://arxiv.org/abs/2205.05124v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2205.05124v1', 'updated': '2022-05-10T19:04:37Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=10, tm_hour=19, tm_min=4, tm_sec=37, tm_wday=1, tm_yday=130, tm_isdst=0), 'published': '2022-05-10T19:04:37Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=10, tm_hour=19, tm_min=4, tm_sec=37, tm_wday=1, tm_yday=130, tm_isdst=0), 'title': 'Extracting Latent Steering Vectors from Pretrained Language Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Extracting Latent Steering Vectors from Pretrained Language Models'}, 'summary': 'Prior work on controllable text generation has focused on learning how to\ncontrol language models through trainable decoding, smart-prompt design, or\nfine-tuning based on a desired objective. We hypothesize that the information\nneeded to steer the model to generate a target sentence is already encoded\nwithin the model. Accordingly, we explore a different approach altogether:\nextracting latent vectors directly from pretrained language model decoders\nwithout fine-tuning. Experiments show that there exist steering vectors, which,\nwhen added to the hidden states of the language model, generate a target\nsentence nearly perfectly (> 99 BLEU) for English sentences from a variety of\ndomains. We show that vector arithmetic can be used for unsupervised sentiment\ntransfer on the Yelp sentiment benchmark, with performance comparable to models\ntailored to this task. We find that distances between steering vectors reflect\nsentence similarity when evaluated on a textual similarity benchmark (STS-B),\noutperforming pooled hidden states of models. Finally, we present an analysis\nof the intrinsic properties of the steering vectors. Taken together, our\nresults suggest that frozen LMs can be effectively controlled through their\nlatent steering space.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Prior work on controllable text generation has focused on learning how to\ncontrol language models through trainable decoding, smart-prompt design, or\nfine-tuning based on a desired objective. We hypothesize that the information\nneeded to steer the model to generate a target sentence is already encoded\nwithin the model. Accordingly, we explore a different approach altogether:\nextracting latent vectors directly from pretrained language model decoders\nwithout fine-tuning. Experiments show that there exist steering vectors, which,\nwhen added to the hidden states of the language model, generate a target\nsentence nearly perfectly (> 99 BLEU) for English sentences from a variety of\ndomains. We show that vector arithmetic can be used for unsupervised sentiment\ntransfer on the Yelp sentiment benchmark, with performance comparable to models\ntailored to this task. We find that distances between steering vectors reflect\nsentence similarity when evaluated on a textual similarity benchmark (STS-B),\noutperforming pooled hidden states of models. Finally, we present an analysis\nof the intrinsic properties of the steering vectors. Taken together, our\nresults suggest that frozen LMs can be effectively controlled through their\nlatent steering space.'}, 'authors': [{'name': 'Nishant Subramani'}, {'name': 'Nivedita Suresh'}, {'name': 'Matthew E. Peters'}], 'author_detail': {'name': 'Matthew E. Peters'}, 'author': 'Matthew E. Peters', 'arxiv_comment': 'Accepted to ACL2022 Findings; 16 pages (9 pages plus references and\n  appendices); Code: https://github.com/nishantsubramani/steering_vectors; Some\n  text overlap with arXiv:2008.09049', 'links': [{'href': 'http://arxiv.org/abs/2205.05124v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.05124v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2205.09393v1,2022-05-19 08:46:14+00:00,2022-05-19 08:46:14+00:00,Two-Step Question Retrieval for Open-Domain QA,"[arxiv.Result.Author('Yeon Seonwoo'), arxiv.Result.Author('Juhee Son'), arxiv.Result.Author('Jiho Jin'), arxiv.Result.Author('Sang-Woo Lee'), arxiv.Result.Author('Ji-Hoon Kim'), arxiv.Result.Author('Jung-Woo Ha'), arxiv.Result.Author('Alice Oh')]","The retriever-reader pipeline has shown promising performance in open-domain
QA but suffers from a very slow inference speed. Recently proposed question
retrieval models tackle this problem by indexing question-answer pairs and
searching for similar questions. These models have shown a significant increase
in inference speed, but at the cost of lower QA performance compared to the
retriever-reader models. This paper proposes a two-step question retrieval
model, SQuID (Sequential Question-Indexed Dense retrieval) and distant
supervision for training. SQuID uses two bi-encoders for question retrieval.
The first-step retriever selects top-k similar questions, and the second-step
retriever finds the most similar question from the top-k questions. We evaluate
the performance and the computational efficiency of SQuID. The results show
that SQuID significantly increases the performance of existing question
retrieval models with a negligible loss on inference speed.",ACL2022-Findings,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2205.09393v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.09393v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2205.09393v1,"{'id': 'http://arxiv.org/abs/2205.09393v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2205.09393v1', 'updated': '2022-05-19T08:46:14Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=19, tm_hour=8, tm_min=46, tm_sec=14, tm_wday=3, tm_yday=139, tm_isdst=0), 'published': '2022-05-19T08:46:14Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=19, tm_hour=8, tm_min=46, tm_sec=14, tm_wday=3, tm_yday=139, tm_isdst=0), 'title': 'Two-Step Question Retrieval for Open-Domain QA', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Two-Step Question Retrieval for Open-Domain QA'}, 'summary': 'The retriever-reader pipeline has shown promising performance in open-domain\nQA but suffers from a very slow inference speed. Recently proposed question\nretrieval models tackle this problem by indexing question-answer pairs and\nsearching for similar questions. These models have shown a significant increase\nin inference speed, but at the cost of lower QA performance compared to the\nretriever-reader models. This paper proposes a two-step question retrieval\nmodel, SQuID (Sequential Question-Indexed Dense retrieval) and distant\nsupervision for training. SQuID uses two bi-encoders for question retrieval.\nThe first-step retriever selects top-k similar questions, and the second-step\nretriever finds the most similar question from the top-k questions. We evaluate\nthe performance and the computational efficiency of SQuID. The results show\nthat SQuID significantly increases the performance of existing question\nretrieval models with a negligible loss on inference speed.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The retriever-reader pipeline has shown promising performance in open-domain\nQA but suffers from a very slow inference speed. Recently proposed question\nretrieval models tackle this problem by indexing question-answer pairs and\nsearching for similar questions. These models have shown a significant increase\nin inference speed, but at the cost of lower QA performance compared to the\nretriever-reader models. This paper proposes a two-step question retrieval\nmodel, SQuID (Sequential Question-Indexed Dense retrieval) and distant\nsupervision for training. SQuID uses two bi-encoders for question retrieval.\nThe first-step retriever selects top-k similar questions, and the second-step\nretriever finds the most similar question from the top-k questions. We evaluate\nthe performance and the computational efficiency of SQuID. The results show\nthat SQuID significantly increases the performance of existing question\nretrieval models with a negligible loss on inference speed.'}, 'authors': [{'name': 'Yeon Seonwoo'}, {'name': 'Juhee Son'}, {'name': 'Jiho Jin'}, {'name': 'Sang-Woo Lee'}, {'name': 'Ji-Hoon Kim'}, {'name': 'Jung-Woo Ha'}, {'name': 'Alice Oh'}], 'author_detail': {'name': 'Alice Oh'}, 'author': 'Alice Oh', 'arxiv_comment': 'ACL2022-Findings', 'links': [{'href': 'http://arxiv.org/abs/2205.09393v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.09393v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2205.13346v1,2022-05-26 13:26:03+00:00,2022-05-26 13:26:03+00:00,Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation,"[arxiv.Result.Author('Mingzhe Li'), arxiv.Result.Author('XieXiong Lin'), arxiv.Result.Author('Xiuying Chen'), arxiv.Result.Author('Jinxiong Chang'), arxiv.Result.Author('Qishen Zhang'), arxiv.Result.Author('Feng Wang'), arxiv.Result.Author('Taifeng Wang'), arxiv.Result.Author('Zhongyi Liu'), arxiv.Result.Author('Wei Chu'), arxiv.Result.Author('Dongyan Zhao'), arxiv.Result.Author('Rui Yan')]","Contrastive learning has achieved impressive success in generation tasks to
militate the ""exposure bias"" problem and discriminatively exploit the different
quality of references. Existing works mostly focus on contrastive learning on
the instance-level without discriminating the contribution of each word, while
keywords are the gist of the text and dominant the constrained mapping
relationships. Hence, in this work, we propose a hierarchical contrastive
learning mechanism, which can unify hybrid granularities semantic meaning in
the input text. Concretely, we first propose a keyword graph via contrastive
correlations of positive-negative pairs to iteratively polish the keyword
representations. Then, we construct intra-contrasts within instance-level and
keyword-level, where we assume words are sampled nodes from a sentence
distribution. Finally, to bridge the gap between independent contrast levels
and tackle the common contrast vanishing problem, we propose an inter-contrast
mechanism that measures the discrepancy between contrastive keyword nodes
respectively to the instance distribution. Experiments demonstrate that our
model outperforms competitive baselines on paraphrasing, dialogue generation,
and storytelling tasks.",Accepted by ACL2022,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2205.13346v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.13346v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2205.13346v1,"{'id': 'http://arxiv.org/abs/2205.13346v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2205.13346v1', 'updated': '2022-05-26T13:26:03Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=26, tm_hour=13, tm_min=26, tm_sec=3, tm_wday=3, tm_yday=146, tm_isdst=0), 'published': '2022-05-26T13:26:03Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=26, tm_hour=13, tm_min=26, tm_sec=3, tm_wday=3, tm_yday=146, tm_isdst=0), 'title': 'Keywords and Instances: A Hierarchical Contrastive Learning Framework\n  Unifying Hybrid Granularities for Text Generation', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Keywords and Instances: A Hierarchical Contrastive Learning Framework\n  Unifying Hybrid Granularities for Text Generation'}, 'summary': 'Contrastive learning has achieved impressive success in generation tasks to\nmilitate the ""exposure bias"" problem and discriminatively exploit the different\nquality of references. Existing works mostly focus on contrastive learning on\nthe instance-level without discriminating the contribution of each word, while\nkeywords are the gist of the text and dominant the constrained mapping\nrelationships. Hence, in this work, we propose a hierarchical contrastive\nlearning mechanism, which can unify hybrid granularities semantic meaning in\nthe input text. Concretely, we first propose a keyword graph via contrastive\ncorrelations of positive-negative pairs to iteratively polish the keyword\nrepresentations. Then, we construct intra-contrasts within instance-level and\nkeyword-level, where we assume words are sampled nodes from a sentence\ndistribution. Finally, to bridge the gap between independent contrast levels\nand tackle the common contrast vanishing problem, we propose an inter-contrast\nmechanism that measures the discrepancy between contrastive keyword nodes\nrespectively to the instance distribution. Experiments demonstrate that our\nmodel outperforms competitive baselines on paraphrasing, dialogue generation,\nand storytelling tasks.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Contrastive learning has achieved impressive success in generation tasks to\nmilitate the ""exposure bias"" problem and discriminatively exploit the different\nquality of references. Existing works mostly focus on contrastive learning on\nthe instance-level without discriminating the contribution of each word, while\nkeywords are the gist of the text and dominant the constrained mapping\nrelationships. Hence, in this work, we propose a hierarchical contrastive\nlearning mechanism, which can unify hybrid granularities semantic meaning in\nthe input text. Concretely, we first propose a keyword graph via contrastive\ncorrelations of positive-negative pairs to iteratively polish the keyword\nrepresentations. Then, we construct intra-contrasts within instance-level and\nkeyword-level, where we assume words are sampled nodes from a sentence\ndistribution. Finally, to bridge the gap between independent contrast levels\nand tackle the common contrast vanishing problem, we propose an inter-contrast\nmechanism that measures the discrepancy between contrastive keyword nodes\nrespectively to the instance distribution. Experiments demonstrate that our\nmodel outperforms competitive baselines on paraphrasing, dialogue generation,\nand storytelling tasks.'}, 'authors': [{'name': 'Mingzhe Li'}, {'name': 'XieXiong Lin'}, {'name': 'Xiuying Chen'}, {'name': 'Jinxiong Chang'}, {'name': 'Qishen Zhang'}, {'name': 'Feng Wang'}, {'name': 'Taifeng Wang'}, {'name': 'Zhongyi Liu'}, {'name': 'Wei Chu'}, {'name': 'Dongyan Zhao'}, {'name': 'Rui Yan'}], 'author_detail': {'name': 'Rui Yan'}, 'author': 'Rui Yan', 'arxiv_comment': 'Accepted by ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2205.13346v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.13346v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2209.02967v1,2022-09-07 07:21:23+00:00,2022-09-07 07:21:23+00:00,That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation with Switch-memory,"[arxiv.Result.Author('Xuemei Tang'), arxiv.Result.Author('Qi Su'), arxiv.Result.Author('Jun Wang')]","The evolution of language follows the rule of gradual change. Grammar,
vocabulary, and lexical semantic shifts take place over time, resulting in a
diachronic linguistic gap. As such, a considerable amount of texts are written
in languages of different eras, which creates obstacles for natural language
processing tasks, such as word segmentation and machine translation. Although
the Chinese language has a long history, previous Chinese natural language
processing research has primarily focused on tasks within a specific era.
Therefore, we propose a cross-era learning framework for Chinese word
segmentation (CWS), CROSSWISE, which uses the Switch-memory (SM) module to
incorporate era-specific linguistic knowledge. Experiments on four corpora from
different eras show that the performance of each corpus significantly improves.
Further analyses also demonstrate that the SM can effectively integrate the
knowledge of the eras into the neural network.","11 pages, 3 figures, accepted by ACL2022",,10.18653/v1/2022.acl-long.540,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://dx.doi.org/10.18653/v1/2022.acl-long.540', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2209.02967v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2209.02967v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2209.02967v1,"{'id': 'http://arxiv.org/abs/2209.02967v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2209.02967v1', 'updated': '2022-09-07T07:21:23Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=9, tm_mday=7, tm_hour=7, tm_min=21, tm_sec=23, tm_wday=2, tm_yday=250, tm_isdst=0), 'published': '2022-09-07T07:21:23Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=9, tm_mday=7, tm_hour=7, tm_min=21, tm_sec=23, tm_wday=2, tm_yday=250, tm_isdst=0), 'title': 'That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation\n  with Switch-memory', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'That Slepen Al the Nyght with Open Ye! Cross-era Sequence Segmentation\n  with Switch-memory'}, 'summary': 'The evolution of language follows the rule of gradual change. Grammar,\nvocabulary, and lexical semantic shifts take place over time, resulting in a\ndiachronic linguistic gap. As such, a considerable amount of texts are written\nin languages of different eras, which creates obstacles for natural language\nprocessing tasks, such as word segmentation and machine translation. Although\nthe Chinese language has a long history, previous Chinese natural language\nprocessing research has primarily focused on tasks within a specific era.\nTherefore, we propose a cross-era learning framework for Chinese word\nsegmentation (CWS), CROSSWISE, which uses the Switch-memory (SM) module to\nincorporate era-specific linguistic knowledge. Experiments on four corpora from\ndifferent eras show that the performance of each corpus significantly improves.\nFurther analyses also demonstrate that the SM can effectively integrate the\nknowledge of the eras into the neural network.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The evolution of language follows the rule of gradual change. Grammar,\nvocabulary, and lexical semantic shifts take place over time, resulting in a\ndiachronic linguistic gap. As such, a considerable amount of texts are written\nin languages of different eras, which creates obstacles for natural language\nprocessing tasks, such as word segmentation and machine translation. Although\nthe Chinese language has a long history, previous Chinese natural language\nprocessing research has primarily focused on tasks within a specific era.\nTherefore, we propose a cross-era learning framework for Chinese word\nsegmentation (CWS), CROSSWISE, which uses the Switch-memory (SM) module to\nincorporate era-specific linguistic knowledge. Experiments on four corpora from\ndifferent eras show that the performance of each corpus significantly improves.\nFurther analyses also demonstrate that the SM can effectively integrate the\nknowledge of the eras into the neural network.'}, 'authors': [{'name': 'Xuemei Tang'}, {'name': 'Qi Su'}, {'name': 'Jun Wang'}], 'author_detail': {'name': 'Jun Wang'}, 'author': 'Jun Wang', 'arxiv_doi': '10.18653/v1/2022.acl-long.540', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.18653/v1/2022.acl-long.540', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2209.02967v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2209.02967v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': '11 pages, 3 figures, accepted by ACL2022', 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2108.05838v2,2022-03-09 10:38:17+00:00,2021-08-12 16:42:00+00:00,Combining (second-order) graph-based and headed-span-based projective dependency parsing,"[arxiv.Result.Author('Songlin Yang'), arxiv.Result.Author('Kewei Tu')]","Graph-based methods, which decompose the score of a dependency tree into
scores of dependency arcs, are popular in dependency parsing for decades.
Recently, \citet{Yang2022Span} propose a headed-span-based method that
decomposes the score of a dependency tree into scores of headed spans. They
show improvement over first-order graph-based methods. However, their method
does not score dependency arcs at all, and dependency arcs are implicitly
induced by their cubic-time algorithm, which is possibly sub-optimal since
modeling dependency arcs is intuitively useful. In this work, we aim to combine
graph-based and headed-span-based methods, incorporating both arc scores and
headed span scores into our model. First, we show a direct way to combine with
$O(n^4)$ parsing complexity. To decrease complexity, inspired by the classical
head-splitting trick, we show two $O(n^3)$ dynamic programming algorithms to
combine first- and second-order graph-based and headed-span-based methods. Our
experiments on PTB, CTB, and UD show that combining first-order graph-based and
headed-span-based methods is effective. We also confirm the effectiveness of
second-order graph-based parsing in the deep learning age, however, we observe
marginal or no improvement when combining second-order graph-based and
headed-span-based methods. Our code is publicly available at
\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.",Findings of ACL2022,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2108.05838v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2108.05838v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2108.05838v2,"{'id': 'http://arxiv.org/abs/2108.05838v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2108.05838v2', 'updated': '2022-03-09T10:38:17Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=9, tm_hour=10, tm_min=38, tm_sec=17, tm_wday=2, tm_yday=68, tm_isdst=0), 'published': '2021-08-12T16:42:00Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=8, tm_mday=12, tm_hour=16, tm_min=42, tm_sec=0, tm_wday=3, tm_yday=224, tm_isdst=0), 'title': 'Combining (second-order) graph-based and headed-span-based projective\n  dependency parsing', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Combining (second-order) graph-based and headed-span-based projective\n  dependency parsing'}, 'summary': 'Graph-based methods, which decompose the score of a dependency tree into\nscores of dependency arcs, are popular in dependency parsing for decades.\nRecently, \\citet{Yang2022Span} propose a headed-span-based method that\ndecomposes the score of a dependency tree into scores of headed spans. They\nshow improvement over first-order graph-based methods. However, their method\ndoes not score dependency arcs at all, and dependency arcs are implicitly\ninduced by their cubic-time algorithm, which is possibly sub-optimal since\nmodeling dependency arcs is intuitively useful. In this work, we aim to combine\ngraph-based and headed-span-based methods, incorporating both arc scores and\nheaded span scores into our model. First, we show a direct way to combine with\n$O(n^4)$ parsing complexity. To decrease complexity, inspired by the classical\nhead-splitting trick, we show two $O(n^3)$ dynamic programming algorithms to\ncombine first- and second-order graph-based and headed-span-based methods. Our\nexperiments on PTB, CTB, and UD show that combining first-order graph-based and\nheaded-span-based methods is effective. We also confirm the effectiveness of\nsecond-order graph-based parsing in the deep learning age, however, we observe\nmarginal or no improvement when combining second-order graph-based and\nheaded-span-based methods. Our code is publicly available at\n\\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Graph-based methods, which decompose the score of a dependency tree into\nscores of dependency arcs, are popular in dependency parsing for decades.\nRecently, \\citet{Yang2022Span} propose a headed-span-based method that\ndecomposes the score of a dependency tree into scores of headed spans. They\nshow improvement over first-order graph-based methods. However, their method\ndoes not score dependency arcs at all, and dependency arcs are implicitly\ninduced by their cubic-time algorithm, which is possibly sub-optimal since\nmodeling dependency arcs is intuitively useful. In this work, we aim to combine\ngraph-based and headed-span-based methods, incorporating both arc scores and\nheaded span scores into our model. First, we show a direct way to combine with\n$O(n^4)$ parsing complexity. To decrease complexity, inspired by the classical\nhead-splitting trick, we show two $O(n^3)$ dynamic programming algorithms to\ncombine first- and second-order graph-based and headed-span-based methods. Our\nexperiments on PTB, CTB, and UD show that combining first-order graph-based and\nheaded-span-based methods is effective. We also confirm the effectiveness of\nsecond-order graph-based parsing in the deep learning age, however, we observe\nmarginal or no improvement when combining second-order graph-based and\nheaded-span-based methods. Our code is publicly available at\n\\url{https://github.com/sustcsonglin/span-based-dependency-parsing}.'}, 'authors': [{'name': 'Songlin Yang'}, {'name': 'Kewei Tu'}], 'author_detail': {'name': 'Kewei Tu'}, 'author': 'Kewei Tu', 'arxiv_comment': 'Findings of ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2108.05838v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2108.05838v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2109.04332v3,2022-03-14 02:05:06+00:00,2021-09-09 15:11:04+00:00,PPT: Pre-trained Prompt Tuning for Few-shot Learning,"[arxiv.Result.Author('Yuxian Gu'), arxiv.Result.Author('Xu Han'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Minlie Huang')]","Prompts for pre-trained language models (PLMs) have shown remarkable
performance by bridging the gap between pre-training tasks and various
downstream tasks. Among these methods, prompt tuning, which freezes PLMs and
only tunes soft prompts, provides an efficient and effective solution for
adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to
be fully explored. In our pilot experiments, we find that prompt tuning
performs comparably with conventional full-model fine-tuning when downstream
data are sufficient, whereas it performs much worse under few-shot learning
settings, which may hinder the application of prompt tuning in practice. We
attribute this low performance to the manner of initializing soft prompts.
Therefore, in this work, we propose to pre-train prompts by adding soft prompts
into the pre-training stage to obtain a better initialization. We name this
Pre-trained Prompt Tuning framework ""PPT"". To ensure the generalization of PPT,
we formulate similar classification tasks into a unified task form and
pre-train soft prompts for this unified task. Extensive experiments show that
tuning pre-trained prompts for downstream tasks can reach or even outperform
full-model fine-tuning under both full-data and few-shot settings. Our approach
is effective and efficient for using large-scale PLMs in practice.",Accepted by ACL2022 (main conference),,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2109.04332v3', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2109.04332v3', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2109.04332v3,"{'id': 'http://arxiv.org/abs/2109.04332v3', 'guidislink': True, 'link': 'http://arxiv.org/abs/2109.04332v3', 'updated': '2022-03-14T02:05:06Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=14, tm_hour=2, tm_min=5, tm_sec=6, tm_wday=0, tm_yday=73, tm_isdst=0), 'published': '2021-09-09T15:11:04Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=9, tm_mday=9, tm_hour=15, tm_min=11, tm_sec=4, tm_wday=3, tm_yday=252, tm_isdst=0), 'title': 'PPT: Pre-trained Prompt Tuning for Few-shot Learning', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'PPT: Pre-trained Prompt Tuning for Few-shot Learning'}, 'summary': 'Prompts for pre-trained language models (PLMs) have shown remarkable\nperformance by bridging the gap between pre-training tasks and various\ndownstream tasks. Among these methods, prompt tuning, which freezes PLMs and\nonly tunes soft prompts, provides an efficient and effective solution for\nadapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to\nbe fully explored. In our pilot experiments, we find that prompt tuning\nperforms comparably with conventional full-model fine-tuning when downstream\ndata are sufficient, whereas it performs much worse under few-shot learning\nsettings, which may hinder the application of prompt tuning in practice. We\nattribute this low performance to the manner of initializing soft prompts.\nTherefore, in this work, we propose to pre-train prompts by adding soft prompts\ninto the pre-training stage to obtain a better initialization. We name this\nPre-trained Prompt Tuning framework ""PPT"". To ensure the generalization of PPT,\nwe formulate similar classification tasks into a unified task form and\npre-train soft prompts for this unified task. Extensive experiments show that\ntuning pre-trained prompts for downstream tasks can reach or even outperform\nfull-model fine-tuning under both full-data and few-shot settings. Our approach\nis effective and efficient for using large-scale PLMs in practice.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Prompts for pre-trained language models (PLMs) have shown remarkable\nperformance by bridging the gap between pre-training tasks and various\ndownstream tasks. Among these methods, prompt tuning, which freezes PLMs and\nonly tunes soft prompts, provides an efficient and effective solution for\nadapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to\nbe fully explored. In our pilot experiments, we find that prompt tuning\nperforms comparably with conventional full-model fine-tuning when downstream\ndata are sufficient, whereas it performs much worse under few-shot learning\nsettings, which may hinder the application of prompt tuning in practice. We\nattribute this low performance to the manner of initializing soft prompts.\nTherefore, in this work, we propose to pre-train prompts by adding soft prompts\ninto the pre-training stage to obtain a better initialization. We name this\nPre-trained Prompt Tuning framework ""PPT"". To ensure the generalization of PPT,\nwe formulate similar classification tasks into a unified task form and\npre-train soft prompts for this unified task. Extensive experiments show that\ntuning pre-trained prompts for downstream tasks can reach or even outperform\nfull-model fine-tuning under both full-data and few-shot settings. Our approach\nis effective and efficient for using large-scale PLMs in practice.'}, 'authors': [{'name': 'Yuxian Gu'}, {'name': 'Xu Han'}, {'name': 'Zhiyuan Liu'}, {'name': 'Minlie Huang'}], 'author_detail': {'name': 'Minlie Huang'}, 'author': 'Minlie Huang', 'arxiv_comment': 'Accepted by ACL2022 (main conference)', 'links': [{'href': 'http://arxiv.org/abs/2109.04332v3', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.04332v3', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2109.10341v2,2022-05-17 17:39:22+00:00,2021-09-21 17:49:34+00:00,Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents,"[arxiv.Result.Author('Biao Zhang'), arxiv.Result.Author('Ankur Bapna'), arxiv.Result.Author('Melvin Johnson'), arxiv.Result.Author('Ali Dabirmoghaddam'), arxiv.Result.Author('Naveen Arivazhagan'), arxiv.Result.Author('Orhan Firat')]","Document-level neural machine translation (DocNMT) achieves coherent
translations by incorporating cross-sentence context. However, for most
language pairs there's a shortage of parallel documents, although parallel
sentences are readily available. In this paper, we study whether and how
contextual modeling in DocNMT is transferable via multilingual modeling. We
focus on the scenario of zero-shot transfer from teacher languages with
document level data to student languages with no documents but sentence level
data, and for the first time treat document-level translation as a transfer
learning problem. Using simple concatenation-based DocNMT, we explore the
effect of 3 factors on the transfer: the number of teacher languages with
document level data, the balance between document and sentence level data at
training, and the data condition of parallel documents (genuine vs.
backtranslated). Our experiments on Europarl-7 and IWSLT-10 show the
feasibility of multilingual transfer for DocNMT, particularly on
document-specific metrics. We observe that more teacher languages and adequate
data balance both contribute to better transfer quality. Surprisingly, the
transfer is less sensitive to the data condition, where multilingual DocNMT
delivers decent performance with either backtranslated or genuine document
pairs.",ACL2022,,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2109.10341v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2109.10341v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2109.10341v2,"{'id': 'http://arxiv.org/abs/2109.10341v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2109.10341v2', 'updated': '2022-05-17T17:39:22Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=17, tm_hour=17, tm_min=39, tm_sec=22, tm_wday=1, tm_yday=137, tm_isdst=0), 'published': '2021-09-21T17:49:34Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=9, tm_mday=21, tm_hour=17, tm_min=49, tm_sec=34, tm_wday=1, tm_yday=264, tm_isdst=0), 'title': 'Multilingual Document-Level Translation Enables Zero-Shot Transfer From\n  Sentences to Documents', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Multilingual Document-Level Translation Enables Zero-Shot Transfer From\n  Sentences to Documents'}, 'summary': ""Document-level neural machine translation (DocNMT) achieves coherent\ntranslations by incorporating cross-sentence context. However, for most\nlanguage pairs there's a shortage of parallel documents, although parallel\nsentences are readily available. In this paper, we study whether and how\ncontextual modeling in DocNMT is transferable via multilingual modeling. We\nfocus on the scenario of zero-shot transfer from teacher languages with\ndocument level data to student languages with no documents but sentence level\ndata, and for the first time treat document-level translation as a transfer\nlearning problem. Using simple concatenation-based DocNMT, we explore the\neffect of 3 factors on the transfer: the number of teacher languages with\ndocument level data, the balance between document and sentence level data at\ntraining, and the data condition of parallel documents (genuine vs.\nbacktranslated). Our experiments on Europarl-7 and IWSLT-10 show the\nfeasibility of multilingual transfer for DocNMT, particularly on\ndocument-specific metrics. We observe that more teacher languages and adequate\ndata balance both contribute to better transfer quality. Surprisingly, the\ntransfer is less sensitive to the data condition, where multilingual DocNMT\ndelivers decent performance with either backtranslated or genuine document\npairs."", 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': ""Document-level neural machine translation (DocNMT) achieves coherent\ntranslations by incorporating cross-sentence context. However, for most\nlanguage pairs there's a shortage of parallel documents, although parallel\nsentences are readily available. In this paper, we study whether and how\ncontextual modeling in DocNMT is transferable via multilingual modeling. We\nfocus on the scenario of zero-shot transfer from teacher languages with\ndocument level data to student languages with no documents but sentence level\ndata, and for the first time treat document-level translation as a transfer\nlearning problem. Using simple concatenation-based DocNMT, we explore the\neffect of 3 factors on the transfer: the number of teacher languages with\ndocument level data, the balance between document and sentence level data at\ntraining, and the data condition of parallel documents (genuine vs.\nbacktranslated). Our experiments on Europarl-7 and IWSLT-10 show the\nfeasibility of multilingual transfer for DocNMT, particularly on\ndocument-specific metrics. We observe that more teacher languages and adequate\ndata balance both contribute to better transfer quality. Surprisingly, the\ntransfer is less sensitive to the data condition, where multilingual DocNMT\ndelivers decent performance with either backtranslated or genuine document\npairs.""}, 'authors': [{'name': 'Biao Zhang'}, {'name': 'Ankur Bapna'}, {'name': 'Melvin Johnson'}, {'name': 'Ali Dabirmoghaddam'}, {'name': 'Naveen Arivazhagan'}, {'name': 'Orhan Firat'}], 'author_detail': {'name': 'Orhan Firat'}, 'author': 'Orhan Firat', 'arxiv_comment': 'ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2109.10341v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2109.10341v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.07996v2,2022-03-26 04:11:10+00:00,2022-02-24 15:12:17+00:00,Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition,"[arxiv.Result.Author('Xichen Pan'), arxiv.Result.Author('Peiyu Chen'), arxiv.Result.Author('Yichen Gong'), arxiv.Result.Author('Helong Zhou'), arxiv.Result.Author('Xinbing Wang'), arxiv.Result.Author('Zhouhan Lin')]","Training Transformer-based models demands a large amount of data, while
obtaining aligned and labelled data in multimodality is rather cost-demanding,
especially for audio-visual speech recognition (AVSR). Thus it makes a lot of
sense to make use of unlabelled unimodal data. On the other side, although the
effectiveness of large-scale self-supervised learning is well established in
both audio and visual modalities, how to integrate those pre-trained models
into a multimodal scenario remains underexplored. In this work, we successfully
leverage unimodal self-supervised learning to promote the multimodal AVSR. In
particular, audio and visual front-ends are trained on large-scale unimodal
datasets, then we integrate components of both front-ends into a larger
multimodal framework which learns to recognize parallel audio-visual data into
characters through a combination of CTC and seq2seq decoding. We show that both
components inherited from unimodal self-supervised learning cooperate well,
resulting in that the multimodal framework yields competitive results through
fine-tuning. Our model is experimentally validated on both word-level and
sentence-level tasks. Especially, even without an external language model, our
proposed model raises the state-of-the-art performances on the widely accepted
Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative
improvement of 30%.",ACL2022 Main Conference,,,cs.SD,"['cs.SD', 'cs.AI', 'cs.CL', 'cs.CV', 'eess.AS']","[arxiv.Result.Link('http://arxiv.org/abs/2203.07996v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.07996v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.07996v2,"{'id': 'http://arxiv.org/abs/2203.07996v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.07996v2', 'updated': '2022-03-26T04:11:10Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=26, tm_hour=4, tm_min=11, tm_sec=10, tm_wday=5, tm_yday=85, tm_isdst=0), 'published': '2022-02-24T15:12:17Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=2, tm_mday=24, tm_hour=15, tm_min=12, tm_sec=17, tm_wday=3, tm_yday=55, tm_isdst=0), 'title': 'Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual\n  Speech Recognition', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual\n  Speech Recognition'}, 'summary': 'Training Transformer-based models demands a large amount of data, while\nobtaining aligned and labelled data in multimodality is rather cost-demanding,\nespecially for audio-visual speech recognition (AVSR). Thus it makes a lot of\nsense to make use of unlabelled unimodal data. On the other side, although the\neffectiveness of large-scale self-supervised learning is well established in\nboth audio and visual modalities, how to integrate those pre-trained models\ninto a multimodal scenario remains underexplored. In this work, we successfully\nleverage unimodal self-supervised learning to promote the multimodal AVSR. In\nparticular, audio and visual front-ends are trained on large-scale unimodal\ndatasets, then we integrate components of both front-ends into a larger\nmultimodal framework which learns to recognize parallel audio-visual data into\ncharacters through a combination of CTC and seq2seq decoding. We show that both\ncomponents inherited from unimodal self-supervised learning cooperate well,\nresulting in that the multimodal framework yields competitive results through\nfine-tuning. Our model is experimentally validated on both word-level and\nsentence-level tasks. Especially, even without an external language model, our\nproposed model raises the state-of-the-art performances on the widely accepted\nLip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative\nimprovement of 30%.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Training Transformer-based models demands a large amount of data, while\nobtaining aligned and labelled data in multimodality is rather cost-demanding,\nespecially for audio-visual speech recognition (AVSR). Thus it makes a lot of\nsense to make use of unlabelled unimodal data. On the other side, although the\neffectiveness of large-scale self-supervised learning is well established in\nboth audio and visual modalities, how to integrate those pre-trained models\ninto a multimodal scenario remains underexplored. In this work, we successfully\nleverage unimodal self-supervised learning to promote the multimodal AVSR. In\nparticular, audio and visual front-ends are trained on large-scale unimodal\ndatasets, then we integrate components of both front-ends into a larger\nmultimodal framework which learns to recognize parallel audio-visual data into\ncharacters through a combination of CTC and seq2seq decoding. We show that both\ncomponents inherited from unimodal self-supervised learning cooperate well,\nresulting in that the multimodal framework yields competitive results through\nfine-tuning. Our model is experimentally validated on both word-level and\nsentence-level tasks. Especially, even without an external language model, our\nproposed model raises the state-of-the-art performances on the widely accepted\nLip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative\nimprovement of 30%.'}, 'authors': [{'name': 'Xichen Pan'}, {'name': 'Peiyu Chen'}, {'name': 'Yichen Gong'}, {'name': 'Helong Zhou'}, {'name': 'Xinbing Wang'}, {'name': 'Zhouhan Lin'}], 'author_detail': {'name': 'Zhouhan Lin'}, 'author': 'Zhouhan Lin', 'arxiv_comment': 'ACL2022 Main Conference', 'links': [{'href': 'http://arxiv.org/abs/2203.07996v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.07996v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.SD', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'eess.AS', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.08556v1,2022-03-16 11:40:02+00:00,2022-03-16 11:40:02+00:00,LEVEN: A Large-Scale Chinese Legal Event Detection Dataset,"[arxiv.Result.Author('Feng Yao'), arxiv.Result.Author('Chaojun Xiao'), arxiv.Result.Author('Xiaozhi Wang'), arxiv.Result.Author('Zhiyuan Liu'), arxiv.Result.Author('Lei Hou'), arxiv.Result.Author('Cunchao Tu'), arxiv.Result.Author('Juanzi Li'), arxiv.Result.Author('Yun Liu'), arxiv.Result.Author('Weixing Shen'), arxiv.Result.Author('Maosong Sun')]","Recognizing facts is the most fundamental step in making judgments, hence
detecting events in the legal documents is important to legal case analysis
tasks. However, existing Legal Event Detection (LED) datasets only concern
incomprehensive event types and have limited annotated data, which restricts
the development of LED methods and their downstream applications. To alleviate
these issues, we present LEVEN a large-scale Chinese LEgal eVENt detection
dataset, with 8,116 legal documents and 150,977 human-annotated event mentions
in 108 event types. Not only charge-related events, LEVEN also covers general
events, which are critical for legal case understanding but neglected in
existing LED datasets. To our knowledge, LEVEN is the largest LED dataset and
has dozens of times the data scale of others, which shall significantly promote
the training and evaluation of LED methods. The results of extensive
experiments indicate that LED is challenging and needs further effort.
Moreover, we simply utilize legal events as side information to promote
downstream applications. The method achieves improvements of average 2.2 points
precision in low-resource judgment prediction, and 1.5 points mean average
precision in unsupervised case retrieval, which suggests the fundamentality of
LED. The source code and dataset can be obtained from
https://github.com/thunlp/LEVEN.",Accepted to ACL2022 Findings,,,cs.CL,"['cs.CL', 'cs.AI']","[arxiv.Result.Link('http://arxiv.org/abs/2203.08556v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.08556v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.08556v1,"{'id': 'http://arxiv.org/abs/2203.08556v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.08556v1', 'updated': '2022-03-16T11:40:02Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=16, tm_hour=11, tm_min=40, tm_sec=2, tm_wday=2, tm_yday=75, tm_isdst=0), 'published': '2022-03-16T11:40:02Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=16, tm_hour=11, tm_min=40, tm_sec=2, tm_wday=2, tm_yday=75, tm_isdst=0), 'title': 'LEVEN: A Large-Scale Chinese Legal Event Detection Dataset', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'LEVEN: A Large-Scale Chinese Legal Event Detection Dataset'}, 'summary': 'Recognizing facts is the most fundamental step in making judgments, hence\ndetecting events in the legal documents is important to legal case analysis\ntasks. However, existing Legal Event Detection (LED) datasets only concern\nincomprehensive event types and have limited annotated data, which restricts\nthe development of LED methods and their downstream applications. To alleviate\nthese issues, we present LEVEN a large-scale Chinese LEgal eVENt detection\ndataset, with 8,116 legal documents and 150,977 human-annotated event mentions\nin 108 event types. Not only charge-related events, LEVEN also covers general\nevents, which are critical for legal case understanding but neglected in\nexisting LED datasets. To our knowledge, LEVEN is the largest LED dataset and\nhas dozens of times the data scale of others, which shall significantly promote\nthe training and evaluation of LED methods. The results of extensive\nexperiments indicate that LED is challenging and needs further effort.\nMoreover, we simply utilize legal events as side information to promote\ndownstream applications. The method achieves improvements of average 2.2 points\nprecision in low-resource judgment prediction, and 1.5 points mean average\nprecision in unsupervised case retrieval, which suggests the fundamentality of\nLED. The source code and dataset can be obtained from\nhttps://github.com/thunlp/LEVEN.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Recognizing facts is the most fundamental step in making judgments, hence\ndetecting events in the legal documents is important to legal case analysis\ntasks. However, existing Legal Event Detection (LED) datasets only concern\nincomprehensive event types and have limited annotated data, which restricts\nthe development of LED methods and their downstream applications. To alleviate\nthese issues, we present LEVEN a large-scale Chinese LEgal eVENt detection\ndataset, with 8,116 legal documents and 150,977 human-annotated event mentions\nin 108 event types. Not only charge-related events, LEVEN also covers general\nevents, which are critical for legal case understanding but neglected in\nexisting LED datasets. To our knowledge, LEVEN is the largest LED dataset and\nhas dozens of times the data scale of others, which shall significantly promote\nthe training and evaluation of LED methods. The results of extensive\nexperiments indicate that LED is challenging and needs further effort.\nMoreover, we simply utilize legal events as side information to promote\ndownstream applications. The method achieves improvements of average 2.2 points\nprecision in low-resource judgment prediction, and 1.5 points mean average\nprecision in unsupervised case retrieval, which suggests the fundamentality of\nLED. The source code and dataset can be obtained from\nhttps://github.com/thunlp/LEVEN.'}, 'authors': [{'name': 'Feng Yao'}, {'name': 'Chaojun Xiao'}, {'name': 'Xiaozhi Wang'}, {'name': 'Zhiyuan Liu'}, {'name': 'Lei Hou'}, {'name': 'Cunchao Tu'}, {'name': 'Juanzi Li'}, {'name': 'Yun Liu'}, {'name': 'Weixing Shen'}, {'name': 'Maosong Sun'}], 'author_detail': {'name': 'Maosong Sun'}, 'author': 'Maosong Sun', 'arxiv_comment': 'Accepted to ACL2022 Findings', 'links': [{'href': 'http://arxiv.org/abs/2203.08556v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.08556v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.AI', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2203.09067v1,2022-03-17 03:53:11+00:00,2022-03-17 03:53:11+00:00,UNIMO-2: End-to-End Unified Vision-Language Grounded Learning,"[arxiv.Result.Author('Wei Li'), arxiv.Result.Author('Can Gao'), arxiv.Result.Author('Guocheng Niu'), arxiv.Result.Author('Xinyan Xiao'), arxiv.Result.Author('Hao Liu'), arxiv.Result.Author('Jiachen Liu'), arxiv.Result.Author('Hua Wu'), arxiv.Result.Author('Haifeng Wang')]","Vision-Language Pre-training (VLP) has achieved impressive performance on
various cross-modal downstream tasks. However, most existing methods can only
learn from aligned image-caption data and rely heavily on expensive regional
features, which greatly limits their scalability and performance. In this
paper, we propose an end-to-end unified-modal pre-training framework, namely
UNIMO-2, for joint learning on both aligned image-caption data and unaligned
image-only and text-only corpus. We build a unified Transformer model to
jointly learn visual representations, textual representations and semantic
alignment between images and texts. In particular, we propose to conduct
grounded learning on both images and texts via a sharing grounded space, which
helps bridge unaligned images and texts, and align the visual and textual
semantic spaces on different types of corpora. The experiments show that our
grounded learning method can improve textual and visual semantic alignment for
improving performance on various cross-modal tasks. Moreover, benefiting from
effective joint modeling of different types of corpora, our model also achieves
impressive performance on single-modal visual and textual tasks. Our code and
models are public at the UNIMO project page https://unimo-ptm.github.io/.",Accepted by ACL2022,,,cs.CV,"['cs.CV', 'cs.CL']","[arxiv.Result.Link('http://arxiv.org/abs/2203.09067v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2203.09067v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2203.09067v1,"{'id': 'http://arxiv.org/abs/2203.09067v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2203.09067v1', 'updated': '2022-03-17T03:53:11Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=17, tm_hour=3, tm_min=53, tm_sec=11, tm_wday=3, tm_yday=76, tm_isdst=0), 'published': '2022-03-17T03:53:11Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=3, tm_mday=17, tm_hour=3, tm_min=53, tm_sec=11, tm_wday=3, tm_yday=76, tm_isdst=0), 'title': 'UNIMO-2: End-to-End Unified Vision-Language Grounded Learning', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'UNIMO-2: End-to-End Unified Vision-Language Grounded Learning'}, 'summary': 'Vision-Language Pre-training (VLP) has achieved impressive performance on\nvarious cross-modal downstream tasks. However, most existing methods can only\nlearn from aligned image-caption data and rely heavily on expensive regional\nfeatures, which greatly limits their scalability and performance. In this\npaper, we propose an end-to-end unified-modal pre-training framework, namely\nUNIMO-2, for joint learning on both aligned image-caption data and unaligned\nimage-only and text-only corpus. We build a unified Transformer model to\njointly learn visual representations, textual representations and semantic\nalignment between images and texts. In particular, we propose to conduct\ngrounded learning on both images and texts via a sharing grounded space, which\nhelps bridge unaligned images and texts, and align the visual and textual\nsemantic spaces on different types of corpora. The experiments show that our\ngrounded learning method can improve textual and visual semantic alignment for\nimproving performance on various cross-modal tasks. Moreover, benefiting from\neffective joint modeling of different types of corpora, our model also achieves\nimpressive performance on single-modal visual and textual tasks. Our code and\nmodels are public at the UNIMO project page https://unimo-ptm.github.io/.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Vision-Language Pre-training (VLP) has achieved impressive performance on\nvarious cross-modal downstream tasks. However, most existing methods can only\nlearn from aligned image-caption data and rely heavily on expensive regional\nfeatures, which greatly limits their scalability and performance. In this\npaper, we propose an end-to-end unified-modal pre-training framework, namely\nUNIMO-2, for joint learning on both aligned image-caption data and unaligned\nimage-only and text-only corpus. We build a unified Transformer model to\njointly learn visual representations, textual representations and semantic\nalignment between images and texts. In particular, we propose to conduct\ngrounded learning on both images and texts via a sharing grounded space, which\nhelps bridge unaligned images and texts, and align the visual and textual\nsemantic spaces on different types of corpora. The experiments show that our\ngrounded learning method can improve textual and visual semantic alignment for\nimproving performance on various cross-modal tasks. Moreover, benefiting from\neffective joint modeling of different types of corpora, our model also achieves\nimpressive performance on single-modal visual and textual tasks. Our code and\nmodels are public at the UNIMO project page https://unimo-ptm.github.io/.'}, 'authors': [{'name': 'Wei Li'}, {'name': 'Can Gao'}, {'name': 'Guocheng Niu'}, {'name': 'Xinyan Xiao'}, {'name': 'Hao Liu'}, {'name': 'Jiachen Liu'}, {'name': 'Hua Wu'}, {'name': 'Haifeng Wang'}], 'author_detail': {'name': 'Haifeng Wang'}, 'author': 'Haifeng Wang', 'arxiv_comment': 'Accepted by ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2203.09067v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2203.09067v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CV', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2204.03685v2,2022-09-23 20:21:04+00:00,2022-04-07 18:33:10+00:00,"Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision","[arxiv.Result.Author('Wanyu Du'), arxiv.Result.Author('Zae Myung Kim'), arxiv.Result.Author('Vipul Raheja'), arxiv.Result.Author('Dhruv Kumar'), arxiv.Result.Author('Dongyeop Kang')]","Revision is an essential part of the human writing process. It tends to be
strategic, adaptive, and, more importantly, iterative in nature. Despite the
success of large language models on text revision tasks, they are limited to
non-iterative, one-shot revisions. Examining and evaluating the capability of
large language models for making continuous revisions and collaborating with
human writers is a critical step towards building effective writing assistants.
In this work, we present a human-in-the-loop iterative text revision system,
Read, Revise, Repeat (R3), which aims at achieving high quality text revisions
with minimal human efforts by reading model-generated revisions and user
feedbacks, revising documents, and repeating human-machine interactions. In R3,
a text revision model provides text editing suggestions for human writers, who
can accept or reject the suggested edits. The accepted edits are then
incorporated into the model for the next iteration of document revision.
Writers can therefore revise documents iteratively by interacting with the
system and simply accepting/rejecting its suggested edits until the text
revision model stops making further revisions or reaches a predefined maximum
number of revisions. Empirical experiments show that R3 can generate revisions
with comparable acceptance rate to human writers at early revision depths, and
the human-machine interaction can get higher quality revisions with fewer
iterations and edits. The collected human-model interaction dataset and system
code are available at \url{https://github.com/vipulraheja/IteraTeR}. Our system
demonstration is available at \url{https://youtu.be/lK08tIpEoaE}.","Accepted by The First Workshop on Intelligent and Interactive Writing
  Assistants at ACL2022",,10.18653/v1/2022.in2writing-1.14,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://dx.doi.org/10.18653/v1/2022.in2writing-1.14', title='doi', rel='related', content_type=None), arxiv.Result.Link('http://arxiv.org/abs/2204.03685v2', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.03685v2', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2204.03685v2,"{'id': 'http://arxiv.org/abs/2204.03685v2', 'guidislink': True, 'link': 'http://arxiv.org/abs/2204.03685v2', 'updated': '2022-09-23T20:21:04Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=9, tm_mday=23, tm_hour=20, tm_min=21, tm_sec=4, tm_wday=4, tm_yday=266, tm_isdst=0), 'published': '2022-04-07T18:33:10Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=7, tm_hour=18, tm_min=33, tm_sec=10, tm_wday=3, tm_yday=97, tm_isdst=0), 'title': 'Read, Revise, Repeat: A System Demonstration for Human-in-the-loop\n  Iterative Text Revision', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Read, Revise, Repeat: A System Demonstration for Human-in-the-loop\n  Iterative Text Revision'}, 'summary': 'Revision is an essential part of the human writing process. It tends to be\nstrategic, adaptive, and, more importantly, iterative in nature. Despite the\nsuccess of large language models on text revision tasks, they are limited to\nnon-iterative, one-shot revisions. Examining and evaluating the capability of\nlarge language models for making continuous revisions and collaborating with\nhuman writers is a critical step towards building effective writing assistants.\nIn this work, we present a human-in-the-loop iterative text revision system,\nRead, Revise, Repeat (R3), which aims at achieving high quality text revisions\nwith minimal human efforts by reading model-generated revisions and user\nfeedbacks, revising documents, and repeating human-machine interactions. In R3,\na text revision model provides text editing suggestions for human writers, who\ncan accept or reject the suggested edits. The accepted edits are then\nincorporated into the model for the next iteration of document revision.\nWriters can therefore revise documents iteratively by interacting with the\nsystem and simply accepting/rejecting its suggested edits until the text\nrevision model stops making further revisions or reaches a predefined maximum\nnumber of revisions. Empirical experiments show that R3 can generate revisions\nwith comparable acceptance rate to human writers at early revision depths, and\nthe human-machine interaction can get higher quality revisions with fewer\niterations and edits. The collected human-model interaction dataset and system\ncode are available at \\url{https://github.com/vipulraheja/IteraTeR}. Our system\ndemonstration is available at \\url{https://youtu.be/lK08tIpEoaE}.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Revision is an essential part of the human writing process. It tends to be\nstrategic, adaptive, and, more importantly, iterative in nature. Despite the\nsuccess of large language models on text revision tasks, they are limited to\nnon-iterative, one-shot revisions. Examining and evaluating the capability of\nlarge language models for making continuous revisions and collaborating with\nhuman writers is a critical step towards building effective writing assistants.\nIn this work, we present a human-in-the-loop iterative text revision system,\nRead, Revise, Repeat (R3), which aims at achieving high quality text revisions\nwith minimal human efforts by reading model-generated revisions and user\nfeedbacks, revising documents, and repeating human-machine interactions. In R3,\na text revision model provides text editing suggestions for human writers, who\ncan accept or reject the suggested edits. The accepted edits are then\nincorporated into the model for the next iteration of document revision.\nWriters can therefore revise documents iteratively by interacting with the\nsystem and simply accepting/rejecting its suggested edits until the text\nrevision model stops making further revisions or reaches a predefined maximum\nnumber of revisions. Empirical experiments show that R3 can generate revisions\nwith comparable acceptance rate to human writers at early revision depths, and\nthe human-machine interaction can get higher quality revisions with fewer\niterations and edits. The collected human-model interaction dataset and system\ncode are available at \\url{https://github.com/vipulraheja/IteraTeR}. Our system\ndemonstration is available at \\url{https://youtu.be/lK08tIpEoaE}.'}, 'authors': [{'name': 'Wanyu Du'}, {'name': 'Zae Myung Kim'}, {'name': 'Vipul Raheja'}, {'name': 'Dhruv Kumar'}, {'name': 'Dongyeop Kang'}], 'author_detail': {'name': 'Dongyeop Kang'}, 'author': 'Dongyeop Kang', 'arxiv_doi': '10.18653/v1/2022.in2writing-1.14', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.18653/v1/2022.in2writing-1.14', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2204.03685v2', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2204.03685v2', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': 'Accepted by The First Workshop on Intelligent and Interactive Writing\n  Assistants at ACL2022', 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2204.04521v1,2022-04-09 18:01:18+00:00,2022-04-09 18:01:18+00:00,Benchmarking for Public Health Surveillance tasks on Social Media with a Domain-Specific Pretrained Language Model,"[arxiv.Result.Author('Usman Naseem'), arxiv.Result.Author('Byoung Chan Lee'), arxiv.Result.Author('Matloob Khushi'), arxiv.Result.Author('Jinman Kim'), arxiv.Result.Author('Adam G. Dunn')]","A user-generated text on social media enables health workers to keep track of
information, identify possible outbreaks, forecast disease trends, monitor
emergency cases, and ascertain disease awareness and response to official
health correspondence. This exchange of health information on social media has
been regarded as an attempt to enhance public health surveillance (PHS).
Despite its potential, the technology is still in its early stages and is not
ready for widespread application. Advancements in pretrained language models
(PLMs) have facilitated the development of several domain-specific PLMs and a
variety of downstream applications. However, there are no PLMs for social media
tasks involving PHS. We present and release PHS-BERT, a transformer-based PLM,
to identify tasks related to public health surveillance on social media. We
compared and benchmarked the performance of PHS-BERT on 25 datasets from
different social medial platforms related to 7 different PHS tasks. Compared
with existing PLMs that are mainly evaluated on limited tasks, PHS-BERT
achieved state-of-the-art performance on all 25 tested datasets, showing that
our PLM is robust and generalizable in the common PHS tasks. By making PHS-BERT
available, we aim to facilitate the community to reduce the computational cost
and introduce new baselines for future works across various PHS-related tasks.","Accepted @ ACL2022 Workshop: The First Workshop on Efficient
  Benchmarking in NLP",,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2204.04521v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.04521v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2204.04521v1,"{'id': 'http://arxiv.org/abs/2204.04521v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2204.04521v1', 'updated': '2022-04-09T18:01:18Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=9, tm_hour=18, tm_min=1, tm_sec=18, tm_wday=5, tm_yday=99, tm_isdst=0), 'published': '2022-04-09T18:01:18Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=9, tm_hour=18, tm_min=1, tm_sec=18, tm_wday=5, tm_yday=99, tm_isdst=0), 'title': 'Benchmarking for Public Health Surveillance tasks on Social Media with a\n  Domain-Specific Pretrained Language Model', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Benchmarking for Public Health Surveillance tasks on Social Media with a\n  Domain-Specific Pretrained Language Model'}, 'summary': 'A user-generated text on social media enables health workers to keep track of\ninformation, identify possible outbreaks, forecast disease trends, monitor\nemergency cases, and ascertain disease awareness and response to official\nhealth correspondence. This exchange of health information on social media has\nbeen regarded as an attempt to enhance public health surveillance (PHS).\nDespite its potential, the technology is still in its early stages and is not\nready for widespread application. Advancements in pretrained language models\n(PLMs) have facilitated the development of several domain-specific PLMs and a\nvariety of downstream applications. However, there are no PLMs for social media\ntasks involving PHS. We present and release PHS-BERT, a transformer-based PLM,\nto identify tasks related to public health surveillance on social media. We\ncompared and benchmarked the performance of PHS-BERT on 25 datasets from\ndifferent social medial platforms related to 7 different PHS tasks. Compared\nwith existing PLMs that are mainly evaluated on limited tasks, PHS-BERT\nachieved state-of-the-art performance on all 25 tested datasets, showing that\nour PLM is robust and generalizable in the common PHS tasks. By making PHS-BERT\navailable, we aim to facilitate the community to reduce the computational cost\nand introduce new baselines for future works across various PHS-related tasks.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A user-generated text on social media enables health workers to keep track of\ninformation, identify possible outbreaks, forecast disease trends, monitor\nemergency cases, and ascertain disease awareness and response to official\nhealth correspondence. This exchange of health information on social media has\nbeen regarded as an attempt to enhance public health surveillance (PHS).\nDespite its potential, the technology is still in its early stages and is not\nready for widespread application. Advancements in pretrained language models\n(PLMs) have facilitated the development of several domain-specific PLMs and a\nvariety of downstream applications. However, there are no PLMs for social media\ntasks involving PHS. We present and release PHS-BERT, a transformer-based PLM,\nto identify tasks related to public health surveillance on social media. We\ncompared and benchmarked the performance of PHS-BERT on 25 datasets from\ndifferent social medial platforms related to 7 different PHS tasks. Compared\nwith existing PLMs that are mainly evaluated on limited tasks, PHS-BERT\nachieved state-of-the-art performance on all 25 tested datasets, showing that\nour PLM is robust and generalizable in the common PHS tasks. By making PHS-BERT\navailable, we aim to facilitate the community to reduce the computational cost\nand introduce new baselines for future works across various PHS-related tasks.'}, 'authors': [{'name': 'Usman Naseem'}, {'name': 'Byoung Chan Lee'}, {'name': 'Matloob Khushi'}, {'name': 'Jinman Kim'}, {'name': 'Adam G. Dunn'}], 'author_detail': {'name': 'Adam G. Dunn'}, 'author': 'Adam G. Dunn', 'arxiv_comment': 'Accepted @ ACL2022 Workshop: The First Workshop on Efficient\n  Benchmarking in NLP', 'links': [{'href': 'http://arxiv.org/abs/2204.04521v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2204.04521v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2204.07288v1,2022-04-15 01:52:45+00:00,2022-04-15 01:52:45+00:00,Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context NLP Models,"[arxiv.Result.Author('Phyllis Ang'), arxiv.Result.Author('Bhuwan Dhingra'), arxiv.Result.Author('Lisa Wu Wills')]","With many real-world applications of Natural Language Processing (NLP)
comprising of long texts, there has been a rise in NLP benchmarks that measure
the accuracy of models that can handle longer input sequences. However, these
benchmarks do not consider the trade-offs between accuracy, speed, and power
consumption as input sizes or model sizes are varied. In this work, we perform
a systematic study of this accuracy vs. efficiency trade-off on two widely used
long-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during
fine-tuning and inference on four datasets from the SCROLLS benchmark. To study
how this trade-off differs across hyperparameter settings, we compare the
models across four sequence lengths (1024, 2048, 3072, 4096) and two model
sizes (base and large) under a fixed resource budget. We find that LED
consistently achieves better accuracy at lower energy costs than Big Bird. For
summarization, we find that increasing model size is more energy efficient than
increasing sequence length for higher accuracy. However, this comes at the cost
of a large drop in inference speed. For question answering, we find that
smaller models are both more efficient and more accurate due to the larger
training batch sizes possible under a fixed resource budget.","Accepted at NLP Power! Workshop on Efficient Benchmarking in NLP at
  ACL2022",,,cs.CL,"['cs.CL', 'cs.LG']","[arxiv.Result.Link('http://arxiv.org/abs/2204.07288v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2204.07288v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2204.07288v1,"{'id': 'http://arxiv.org/abs/2204.07288v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2204.07288v1', 'updated': '2022-04-15T01:52:45Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=15, tm_hour=1, tm_min=52, tm_sec=45, tm_wday=4, tm_yday=105, tm_isdst=0), 'published': '2022-04-15T01:52:45Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=4, tm_mday=15, tm_hour=1, tm_min=52, tm_sec=45, tm_wday=4, tm_yday=105, tm_isdst=0), 'title': 'Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context\n  NLP Models', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Characterizing the Efficiency vs. Accuracy Trade-off for Long-Context\n  NLP Models'}, 'summary': 'With many real-world applications of Natural Language Processing (NLP)\ncomprising of long texts, there has been a rise in NLP benchmarks that measure\nthe accuracy of models that can handle longer input sequences. However, these\nbenchmarks do not consider the trade-offs between accuracy, speed, and power\nconsumption as input sizes or model sizes are varied. In this work, we perform\na systematic study of this accuracy vs. efficiency trade-off on two widely used\nlong-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during\nfine-tuning and inference on four datasets from the SCROLLS benchmark. To study\nhow this trade-off differs across hyperparameter settings, we compare the\nmodels across four sequence lengths (1024, 2048, 3072, 4096) and two model\nsizes (base and large) under a fixed resource budget. We find that LED\nconsistently achieves better accuracy at lower energy costs than Big Bird. For\nsummarization, we find that increasing model size is more energy efficient than\nincreasing sequence length for higher accuracy. However, this comes at the cost\nof a large drop in inference speed. For question answering, we find that\nsmaller models are both more efficient and more accurate due to the larger\ntraining batch sizes possible under a fixed resource budget.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'With many real-world applications of Natural Language Processing (NLP)\ncomprising of long texts, there has been a rise in NLP benchmarks that measure\nthe accuracy of models that can handle longer input sequences. However, these\nbenchmarks do not consider the trade-offs between accuracy, speed, and power\nconsumption as input sizes or model sizes are varied. In this work, we perform\na systematic study of this accuracy vs. efficiency trade-off on two widely used\nlong-sequence models - Longformer-Encoder-Decoder (LED) and Big Bird - during\nfine-tuning and inference on four datasets from the SCROLLS benchmark. To study\nhow this trade-off differs across hyperparameter settings, we compare the\nmodels across four sequence lengths (1024, 2048, 3072, 4096) and two model\nsizes (base and large) under a fixed resource budget. We find that LED\nconsistently achieves better accuracy at lower energy costs than Big Bird. For\nsummarization, we find that increasing model size is more energy efficient than\nincreasing sequence length for higher accuracy. However, this comes at the cost\nof a large drop in inference speed. For question answering, we find that\nsmaller models are both more efficient and more accurate due to the larger\ntraining batch sizes possible under a fixed resource budget.'}, 'authors': [{'name': 'Phyllis Ang'}, {'name': 'Bhuwan Dhingra'}, {'name': 'Lisa Wu Wills'}], 'author_detail': {'name': 'Lisa Wu Wills'}, 'author': 'Lisa Wu Wills', 'arxiv_comment': 'Accepted at NLP Power! Workshop on Efficient Benchmarking in NLP at\n  ACL2022', 'links': [{'href': 'http://arxiv.org/abs/2204.07288v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2204.07288v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cs.LG', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
http://arxiv.org/abs/2205.09536v1,2022-05-19 13:03:01+00:00,2022-05-19 13:03:01+00:00,A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction,"[arxiv.Result.Author('Yang Liu'), arxiv.Result.Author('Jinpeng Hu'), arxiv.Result.Author('Xiang Wan'), arxiv.Result.Author('Tsung-Hui Chang')]","Few-Shot Relation Extraction aims at predicting the relation for a pair of
entities in a sentence by training with a few labelled examples in each
relation. Some recent works have introduced relation information (i.e.,
relation labels or descriptions) to assist model learning based on Prototype
Network. However, most of them constrain the prototypes of each relation class
implicitly with relation information, generally through designing complex
network structures, like generating hybrid features, combining with contrastive
learning or attention networks. We argue that relation information can be
introduced more explicitly and effectively into the model. Thus, this paper
proposes a direct addition approach to introduce relation information.
Specifically, for each relation class, the relation representation is first
generated by concatenating two views of relations (i.e., [CLS] token embedding
and the mean value of embeddings of all tokens) and then directly added to the
original prototype for both train and prediction. Experimental results on the
benchmark dataset FewRel 1.0 show significant improvements and achieve
comparable results to the state-of-the-art, which demonstrates the
effectiveness of our proposed approach. Besides, further analyses verify that
the direct addition is a much more effective way to integrate the relation
representations and the original prototypes.",accepted to ACL2022 findings,,,cs.CL,['cs.CL'],"[arxiv.Result.Link('http://arxiv.org/abs/2205.09536v1', title=None, rel='alternate', content_type=None), arxiv.Result.Link('http://arxiv.org/pdf/2205.09536v1', title='pdf', rel='related', content_type=None)]",http://arxiv.org/pdf/2205.09536v1,"{'id': 'http://arxiv.org/abs/2205.09536v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2205.09536v1', 'updated': '2022-05-19T13:03:01Z', 'updated_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=19, tm_hour=13, tm_min=3, tm_sec=1, tm_wday=3, tm_yday=139, tm_isdst=0), 'published': '2022-05-19T13:03:01Z', 'published_parsed': time.struct_time(tm_year=2022, tm_mon=5, tm_mday=19, tm_hour=13, tm_min=3, tm_sec=1, tm_wday=3, tm_yday=139, tm_isdst=0), 'title': 'A Simple yet Effective Relation Information Guided Approach for Few-Shot\n  Relation Extraction', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'A Simple yet Effective Relation Information Guided Approach for Few-Shot\n  Relation Extraction'}, 'summary': 'Few-Shot Relation Extraction aims at predicting the relation for a pair of\nentities in a sentence by training with a few labelled examples in each\nrelation. Some recent works have introduced relation information (i.e.,\nrelation labels or descriptions) to assist model learning based on Prototype\nNetwork. However, most of them constrain the prototypes of each relation class\nimplicitly with relation information, generally through designing complex\nnetwork structures, like generating hybrid features, combining with contrastive\nlearning or attention networks. We argue that relation information can be\nintroduced more explicitly and effectively into the model. Thus, this paper\nproposes a direct addition approach to introduce relation information.\nSpecifically, for each relation class, the relation representation is first\ngenerated by concatenating two views of relations (i.e., [CLS] token embedding\nand the mean value of embeddings of all tokens) and then directly added to the\noriginal prototype for both train and prediction. Experimental results on the\nbenchmark dataset FewRel 1.0 show significant improvements and achieve\ncomparable results to the state-of-the-art, which demonstrates the\neffectiveness of our proposed approach. Besides, further analyses verify that\nthe direct addition is a much more effective way to integrate the relation\nrepresentations and the original prototypes.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Few-Shot Relation Extraction aims at predicting the relation for a pair of\nentities in a sentence by training with a few labelled examples in each\nrelation. Some recent works have introduced relation information (i.e.,\nrelation labels or descriptions) to assist model learning based on Prototype\nNetwork. However, most of them constrain the prototypes of each relation class\nimplicitly with relation information, generally through designing complex\nnetwork structures, like generating hybrid features, combining with contrastive\nlearning or attention networks. We argue that relation information can be\nintroduced more explicitly and effectively into the model. Thus, this paper\nproposes a direct addition approach to introduce relation information.\nSpecifically, for each relation class, the relation representation is first\ngenerated by concatenating two views of relations (i.e., [CLS] token embedding\nand the mean value of embeddings of all tokens) and then directly added to the\noriginal prototype for both train and prediction. Experimental results on the\nbenchmark dataset FewRel 1.0 show significant improvements and achieve\ncomparable results to the state-of-the-art, which demonstrates the\neffectiveness of our proposed approach. Besides, further analyses verify that\nthe direct addition is a much more effective way to integrate the relation\nrepresentations and the original prototypes.'}, 'authors': [{'name': 'Yang Liu'}, {'name': 'Jinpeng Hu'}, {'name': 'Xiang Wan'}, {'name': 'Tsung-Hui Chang'}], 'author_detail': {'name': 'Tsung-Hui Chang'}, 'author': 'Tsung-Hui Chang', 'arxiv_comment': 'accepted to ACL2022 findings', 'links': [{'href': 'http://arxiv.org/abs/2205.09536v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2205.09536v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.CL', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}"
