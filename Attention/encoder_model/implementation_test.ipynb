{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-01T04:53:00.099474Z",
     "start_time": "2023-08-01T04:52:57.041906Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'input_ids': [1, 273, 481, 379, 7092, 394, 260, 420, 273, 295, 298, 1672, 3731, 260, 273, 481, 277, 2609, 260, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pure Transformer Masking Experiment\n",
    "Huggingface AutoTokenizer는 attention mask를 이용해 패딩값에 마스킹을 적용\n",
    "\"\"\"\n",
    "\n",
    "text = 'I am very hungry now. But I can not eat anymore. I am on diet.'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "tokenizer.encode_plus(\n",
    "    text,\n",
    "    max_length=128,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors=None,\n",
    "    add_special_tokens=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T10:47:15.070625842Z",
     "start_time": "2023-07-31T10:47:14.403673144Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 2., 3.],\n        [4., 5., 6.],\n        [-inf, -inf, -inf]])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Encoder Padding Token Maksing Method\n",
    "1) 처음부터 row, col 함께 마스킹\n",
    "2) col만 마스킹 하고 나중에 loss 계산할 때 row 마스킹\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Pytorch nn.Tensor.masked_fill_() Test \"\"\"\n",
    "x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "mask = torch.Tensor([[1, 2, 3], [4, 5, 6], [0, 0, 0]])\n",
    "\n",
    "x.masked_fill(mask == 0, float('-inf'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T14:26:16.751495519Z",
     "start_time": "2023-07-31T14:26:16.732791780Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True,  True]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[False, False, False,  True,  True,  True],\n         [False, False, False,  True,  True,  True],\n         [False, False, False,  True,  True,  True],\n         [False, False, False,  True,  True,  True],\n         [False, False, False,  True,  True,  True],\n         [False, False, False,  True,  True,  True]],\n\n        [[False, False,  True,  True,  True,  True],\n         [False, False,  True,  True,  True,  True],\n         [False, False,  True,  True,  True,  True],\n         [False, False,  True,  True,  True,  True],\n         [False, False,  True,  True,  True,  True],\n         [False, False,  True,  True,  True,  True]]])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_index = 1\n",
    "src = torch.tensor([[4,6,5,1,1,1],[7,7,1,1,1,1]])  # 이게 문장 두개가 되는거구나\n",
    "src = (src == pad_index)  # [batch, sequence_length]\n",
    "print(src)\n",
    "src.repeat(1, 6).view(2, 6, 6)  # [batch, sequence_length, sequence_length]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T14:31:30.315886869Z",
     "start_time": "2023-07-31T14:31:30.268123787Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 1, 1, 0, 0, 0],\n        [1, 1, 0, 0, 0, 0]], dtype=torch.int32)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(src != pad_index).int()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T14:47:08.838101561Z",
     "start_time": "2023-07-31T14:47:08.797321910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 0, 1, 1, 1, 1],\n        [0, 0, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1]], dtype=torch.int32)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 행과 열에 존재 하는 마스킹 값을 동시에 처리해 하나의 seqxseq 마스킹 행렬로 만드는게 쉽지가 않다. \"\"\"\n",
    "\n",
    "src = src.int()\n",
    "row, col = src[1], src[1].view(-1, 1)\n",
    "row_matrix = row.repeat(1, 6).view(6, 6)\n",
    "col_matrix = col.repeat(1, 6).view(6, 6)\n",
    "row_matrix | col_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T14:29:24.353617305Z",
     "start_time": "2023-07-31T14:29:24.295964805Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 4, 6])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(src != pad_index).int().repeat(1, 4).view(2, 4, 6).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T16:27:56.997317Z",
     "start_time": "2023-07-31T16:27:56.962813Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 6])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_index = 1\n",
    "src = torch.tensor([[4,6,5,1,1,1],[7,7,1,1,1,1]])  # 이게 문장 두개가 되는거구나\n",
    "src.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T16:25:39.456751Z",
     "start_time": "2023-07-31T16:25:39.452268Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1, 1, 1, 0, 0, 0],\n         [1, 1, 1, 0, 0, 0],\n         [1, 1, 1, 0, 0, 0],\n         [1, 1, 1, 0, 0, 0],\n         [1, 1, 1, 0, 0, 0],\n         [1, 1, 1, 0, 0, 0]],\n\n        [[1, 1, 0, 0, 0, 0],\n         [1, 1, 0, 0, 0, 0],\n         [1, 1, 0, 0, 0, 0],\n         [1, 1, 0, 0, 0, 0],\n         [1, 1, 0, 0, 0, 0],\n         [1, 1, 0, 0, 0, 0]]], dtype=torch.int32)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_x = (src != pad_index).int().repeat(1, src.shape[-1]).view(src.shape[0], src.shape[-1], src.shape[-1])\n",
    "enc_x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T16:19:51.667712Z",
     "start_time": "2023-07-31T16:19:51.653075Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[  1.3864,   2.4091, -11.7833,     -inf,     -inf,     -inf],\n         [ -8.7445,   5.7077,   0.4128,     -inf,     -inf,     -inf],\n         [ 12.8448,  -4.3385,  14.7385,     -inf,     -inf,     -inf],\n         [  5.9451,  -4.9951,   9.4302,     -inf,     -inf,     -inf],\n         [ -7.9134,  -6.1186, -10.4371,     -inf,     -inf,     -inf],\n         [ -4.3155,  -6.2908,  -0.2527,     -inf,     -inf,     -inf]],\n\n        [[ 14.3429,   6.3632,     -inf,     -inf,     -inf,     -inf],\n         [-13.8716,  -4.7118,     -inf,     -inf,     -inf,     -inf],\n         [  2.7359,   9.3584,     -inf,     -inf,     -inf,     -inf],\n         [-15.9456,  -5.7949,     -inf,     -inf,     -inf,     -inf],\n         [  6.2284,  -6.7821,     -inf,     -inf,     -inf,     -inf],\n         [-13.9051,   0.3745,     -inf,     -inf,     -inf,     -inf]]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.randn(2, 6, 64)\n",
    "k = torch.randn(2, 6, 64)\n",
    "attention_matrix = torch.matmul(q, k.transpose(-1, -2))\n",
    "attention_matrix.masked_fill(enc_x == 0, float('-inf'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T16:19:53.388338Z",
     "start_time": "2023-07-31T16:19:53.357247Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[1, 1, 1, 0],\n          [1, 1, 1, 0],\n          [1, 1, 1, 0],\n          [1, 1, 1, 0]],\n \n         [[1, 1, 0, 0],\n          [1, 1, 0, 0],\n          [1, 1, 0, 0],\n          [1, 1, 0, 0]]], dtype=torch.int32),\n torch.Size([2, 4, 4]),\n tensor([[[1., 0., 0., 0.],\n          [1., 1., 0., 0.],\n          [1., 1., 1., 0.],\n          [1., 1., 1., 1.]],\n \n         [[1., 0., 0., 0.],\n          [1., 1., 0., 0.],\n          [1., 1., 1., 0.],\n          [1., 1., 1., 1.]]]),\n torch.Size([2, 4, 4]))"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Testing for Decoder LM Mask \"\"\"\n",
    "pad_index = 1\n",
    "trg = torch.tensor([[4,6,5,1],[7,7,1,1]])  # 이게 문장 두개가 되는거구나\n",
    "pad_mask = (trg != pad_index).int().repeat(1, trg.shape[-1]).view(trg.shape[0], trg.shape[-1], trg.shape[-1])\n",
    "lm_mask = torch.tril(torch.ones(trg.shape[0], trg.shape[-1], trg.shape[-1]))\n",
    "pad_mask, pad_mask.shape, lm_mask, lm_mask.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T16:19:55.135562Z",
     "start_time": "2023-07-31T16:19:55.126967Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1., 0., 0., 0.],\n         [1., 1., 0., 0.],\n         [1., 1., 1., 0.],\n         [1., 1., 1., 0.]],\n\n        [[1., 0., 0., 0.],\n         [1., 1., 0., 0.],\n         [1., 1., 0., 0.],\n         [1., 1., 0., 0.]]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Testing for Decoder Mask \"\"\"\n",
    "dec_mask = pad_mask * lm_mask\n",
    "dec_mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T16:19:57.442436Z",
     "start_time": "2023-07-31T16:19:57.430304Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 6, 6]), torch.Size([2, 4, 4]))"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Testing for Encoder-Decoder Mask \"\"\"\n",
    "enc_x.shape, dec_mask.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-31T16:20:07.297928Z",
     "start_time": "2023-07-31T16:20:07.294345Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing for Encoder-Decoder Mask\n",
    "Make Rectangle Masking Matrix\n",
    "\"\"\"\n",
    "enc_dec_mask =\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    \"\"\"\n",
    "    Making projection matrix(Q, K, V) for each attention head\n",
    "    When you call this class, it returns projection matrix of each attention head\n",
    "    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)\n",
    "    Args:\n",
    "        num_heads: number of heads in MHA, default 8\n",
    "        dim_head: dimension of each attention head, default 64\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads: int = 8, dim_head: int = 64) -> None:\n",
    "        super(Projector, self).__init__()\n",
    "        self.dim_model = num_heads * dim_head\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_head = dim_head\n",
    "\n",
    "    def __call__(self):\n",
    "        fc_q = nn.Linear(self.dim_model, self.dim_head)\n",
    "        fc_k = nn.Linear(self.dim_model, self.dim_head)\n",
    "        fc_v = nn.Linear(self.dim_model, self.dim_head)\n",
    "        return fc_q, fc_k, fc_v\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for multi-head attention (MHA) module in vanilla transformer\n",
    "    We apply linear transformation to input vector by each attention head's projection matrix (8, 512, 64)\n",
    "    Other approaches are possible, such as using one projection matrix for all attention heads (1, 512, 512)\n",
    "    and then split into each attention heads (8. 512, 64)\n",
    "    Args:\n",
    "        dim_model: dimension of model's latent vector space, default 512 from official paper\n",
    "        num_heads: number of heads in MHA, default 8 from official paper\n",
    "        dropout: dropout rate, default 0.1\n",
    "    Math:\n",
    "        MHA(Q, K, V) = Concat(Head1, Head2, ... Head8) * W_concat\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/1706.03762\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dim = dim_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dim_head = int(self.dim / self.num_heads)  # dimension of each attention head\n",
    "        self.dot_scale = torch.sqrt(torch.tensor(self.dim_head))  # scale factor for Q•K^T Result\n",
    "\n",
    "        # linear combination: projection matrix(Q_1, K_1, V_1, ... Q_n, K_n, V_n) for each attention head\n",
    "        self.projector = Projector(self.num_heads, self.dim_head)  # init instance\n",
    "        self.projector_list = [list(self.projector()) for _ in range(self.num_heads)]  # call instance\n",
    "        self.fc_concat = nn.Linear(self.dim, self.dim)  # for concatenation of each attention head\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: bool = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        1) make Q, K, V matrix for each attention head: [BS, HEAD, SEQ_LEN, DIM_HEAD], ex) [10, 8, 512, 64]\n",
    "        2) Do self-attention in each attention head\n",
    "            - Matmul (Q, K^T) with scale factor (sqrt(DIM_HEAD))\n",
    "            - Mask for padding token (Option for Decoder)\n",
    "            - Softmax\n",
    "            - Matmul (Softmax, V)\n",
    "        3) Concatenate each attention head & linear transformation (512, 512)\n",
    "        \"\"\"\n",
    "        # 1) make Q, K, V matrix for each attention head\n",
    "        Q, K, V = [], [], []\n",
    "\n",
    "        for i in range(self.num_heads):\n",
    "            Q.append(self.projector_list[i][0](x))\n",
    "            K.append(self.projector_list[i][1](x))\n",
    "            V.append(self.projector_list[i][2](x))\n",
    "\n",
    "        Q = torch.stack(Q, dim=1)\n",
    "        K = torch.stack(K, dim=1)\n",
    "        V = torch.stack(V, dim=1)\n",
    "        # 2) Do self-attention in each attention head\n",
    "        attention_score = torch.matmul(Q, K.transpose(-1, -2)) / self.dot_scale\n",
    "        if mask is not None:  # for padding token\n",
    "            attention_score[mask] = float('-inf')\n",
    "        attention_dist = F.softmax(attention_score, dim=-1)  # [BS, HEAD, SEQ_LEN, SEQ_LEN]\n",
    "        attention_matrix = torch.matmul(attention_dist, V).transpose(1, 2).reshape(x.shape[0], x.shape[1], self.dim)  # [BS, SEQ_LEN, DIM]\n",
    "\n",
    "        # 3) Concatenate each attention head & linear transformation (512, 512)\n",
    "        x = self.fc_concat(attention_matrix)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T12:22:07.207490343Z",
     "start_time": "2023-07-18T12:22:07.200225153Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Debug for MultiHeadAttention \"\"\"\n",
    "\n",
    "x = torch.randn(10, 512, 512)\n",
    "test_head = MultiHeadAttention()\n",
    "test_result = test_head(x)\n",
    "test_result, test_result.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1024, 768])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" torch.reshape test for making input shape in Vision Transformers \"\"\"\n",
    "patch_size, num_patches = 16, 32\n",
    "x = torch.randn(10, 3, 512, 512)\n",
    "x = x.reshape(x.shape[0], num_patches**2, patch_size**2 * x.shape[1])\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:10.284936109Z",
     "start_time": "2023-07-18T14:17:10.207383191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1024, 1024])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Check Input Embedding shape \"\"\"\n",
    "input_embedding = nn.Linear(768, 1024)\n",
    "x = input_embedding(x)\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:11.601900007Z",
     "start_time": "2023-07-18T14:17:11.545842018Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1, 1024])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" make classification token for Vision Transformers \"\"\"\n",
    "cls_token = torch.zeros(x.shape[0], 1, x.shape[2])  # can change init method\n",
    "cls_token.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:43.334781057Z",
     "start_time": "2023-07-18T14:17:43.332286758Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1025, 1024])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([cls_token, x], dim=1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:53.794588640Z",
     "start_time": "2023-07-18T14:17:53.775080340Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1024, 512])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Test for Hybrid Model \"\"\"\n",
    "x = torch.randn(10, 3, 512, 512)\n",
    "dim_model = 512\n",
    "patch_size = 16\n",
    "num_patches = 32\n",
    "conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=dim_model,\n",
    "            kernel_size=patch_size,\n",
    "            stride=16\n",
    ")\n",
    "x = conv(x).reshape(x.shape[0], dim_model, num_patches**2).transpose(-1, -2)\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T13:35:54.918886Z",
     "start_time": "2023-07-24T13:35:54.817215Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "1024"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T13:29:33.814138Z",
     "start_time": "2023-07-24T13:29:33.809814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-4.4441, -2.0285,  3.8037,  ..., -2.1064,  0.4153, -1.5477],\n         [ 0.5457, -2.0689,  2.4735,  ...,  2.0869, -0.8964, -1.1958],\n         [-1.5544, -1.4172,  0.4664,  ...,  1.3028,  0.0901,  1.3808],\n         ...,\n         [ 2.2167, -0.3013, -2.5602,  ..., -3.3431, -5.7226, -0.5048],\n         [-1.1391,  1.3073,  4.0575,  ...,  2.8553, -1.3381, -3.5350],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.8348,  8.0450,  1.2452]],\n        grad_fn=<StackBackward0>),\n torch.Size([1024, 1024]))"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Test for DeBERTa Disentangled Self-Attention \"\"\"\n",
    "batch, sequence, dim_model, dim_head, k = 10, 1024, 2048, 64, 512\n",
    "position_embedding = nn.Embedding(2*k, dim_model)\n",
    "x = torch.randn(sequence, dim_model)  # [Batch, Sequence, Dim]\n",
    "p_x = position_embedding(torch.arange(2*k))\n",
    "\n",
    "\n",
    "fc_q = nn.Linear(dim_model, dim_head)\n",
    "fc_k = nn.Linear(dim_model, dim_head)\n",
    "fc_v = nn.Linear(dim_model, dim_head)\n",
    "fc_qr = nn.Linear(dim_model, dim_head)  # projector for Relative Position Query matrix\n",
    "fc_kr = nn.Linear(dim_model, dim_head)  # projector for Relative Position Key matrix\n",
    "\n",
    "q = fc_q(x)\n",
    "kr = fc_kr(p_x)\n",
    "\n",
    "# c2p attention matrix\n",
    "tmp_c2p= torch.stack(\n",
    "    [torch.matmul(q[i, :], kr.transpose(-1, -2)) for i in range(x.shape[0])],\n",
    "    dim=0\n",
    ")\n",
    "tmp_c2p, tmp_c2p.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T06:36:42.413617Z",
     "start_time": "2023-08-01T06:36:42.330238Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "i번째 토큰의 latent vector space 1024 차원에서 max relative position 값인 k만 뽑아 내는게 목적\n",
    "그리고 빼는 것도 max sequence length 만큼 상대 위치 임베딩 토큰을 구하는거구나."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-4.4441, -2.0285,  3.8037,  ..., -2.1064,  0.4153, -1.5477],\n         [ 0.5457, -2.0689,  2.4735,  ...,  2.0869, -0.8964, -1.1958],\n         [-1.5544, -1.4172,  0.4664,  ...,  1.3028,  0.0901,  1.3808],\n         ...,\n         [ 2.2167, -0.3013, -2.5602,  ..., -3.3431, -5.7226, -0.5048],\n         [-1.1391,  1.3073,  4.0575,  ...,  2.8553, -1.3381, -3.5350],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.8348,  8.0450,  1.2452]],\n        grad_fn=<MmBackward0>),\n torch.Size([1024, 1024]))"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" tmp_c2p matrix calculation \"\"\"\n",
    "tmp_c2p = torch.matmul(q, kr.transpose(-1, -2))\n",
    "tmp_c2p, tmp_c2p.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T06:36:42.574877Z",
     "start_time": "2023-08-01T06:36:42.569834Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([   0,    1,    2,  ..., 1021, 1022, 1023]),\n tensor([   0,    1,    2,  ..., 1021, 1022, 1023]))"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Build Subtraction of token index in c2p matrix \"\"\"\n",
    "max_seq, max_relative_position = 1024, 512\n",
    "q_index, k_index = torch.arange(max_seq), torch.arange(2*max_relative_position)\n",
    "q_index, k_index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T06:36:42.704118Z",
     "start_time": "2023-08-01T06:36:42.697192Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[   0],\n         [   1],\n         [   2],\n         ...,\n         [1021],\n         [1022],\n         [1023]]),\n tensor([[   0,    1,    2,  ..., 1021, 1022, 1023]]))"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_index.view(-1, 1), k_index.view(1, -1)  # like as transpose & vstack"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T06:36:42.843411Z",
     "start_time": "2023-08-01T06:36:42.838254Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 512,  511,  510,  ..., -509, -510, -511],\n        [ 513,  512,  511,  ..., -508, -509, -510],\n        [ 514,  513,  512,  ..., -507, -508, -509],\n        ...,\n        [1533, 1532, 1531,  ...,  512,  511,  510],\n        [1534, 1533, 1532,  ...,  513,  512,  511],\n        [1535, 1534, 1533,  ...,  514,  513,  512]])"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 지금 이건 i - j 값이 어느 범위에 속하는지 알아내는게 목적이니까 \"\"\"\n",
    "tmp_pos = q_index.view(-1, 1) - k_index.view(1, -1)\n",
    "rel_pos_matrix = tmp_pos + max_relative_position\n",
    "rel_pos_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T06:36:42.998975Z",
     "start_time": "2023-08-01T06:36:42.992769Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 512,  511,  510,  ...,    0,    0,    0],\n          [ 513,  512,  511,  ...,    0,    0,    0],\n          [ 514,  513,  512,  ...,    0,    0,    0],\n          ...,\n          [1023, 1023, 1023,  ...,  512,  511,  510],\n          [1023, 1023, 1023,  ...,  513,  512,  511],\n          [1023, 1023, 1023,  ...,  514,  513,  512]],\n \n         [[ 512,  511,  510,  ...,    0,    0,    0],\n          [ 513,  512,  511,  ...,    0,    0,    0],\n          [ 514,  513,  512,  ...,    0,    0,    0],\n          ...,\n          [1023, 1023, 1023,  ...,  512,  511,  510],\n          [1023, 1023, 1023,  ...,  513,  512,  511],\n          [1023, 1023, 1023,  ...,  514,  513,  512]],\n \n         [[ 512,  511,  510,  ...,    0,    0,    0],\n          [ 513,  512,  511,  ...,    0,    0,    0],\n          [ 514,  513,  512,  ...,    0,    0,    0],\n          ...,\n          [1023, 1023, 1023,  ...,  512,  511,  510],\n          [1023, 1023, 1023,  ...,  513,  512,  511],\n          [1023, 1023, 1023,  ...,  514,  513,  512]],\n \n         ...,\n \n         [[ 512,  511,  510,  ...,    0,    0,    0],\n          [ 513,  512,  511,  ...,    0,    0,    0],\n          [ 514,  513,  512,  ...,    0,    0,    0],\n          ...,\n          [1023, 1023, 1023,  ...,  512,  511,  510],\n          [1023, 1023, 1023,  ...,  513,  512,  511],\n          [1023, 1023, 1023,  ...,  514,  513,  512]],\n \n         [[ 512,  511,  510,  ...,    0,    0,    0],\n          [ 513,  512,  511,  ...,    0,    0,    0],\n          [ 514,  513,  512,  ...,    0,    0,    0],\n          ...,\n          [1023, 1023, 1023,  ...,  512,  511,  510],\n          [1023, 1023, 1023,  ...,  513,  512,  511],\n          [1023, 1023, 1023,  ...,  514,  513,  512]],\n \n         [[ 512,  511,  510,  ...,    0,    0,    0],\n          [ 513,  512,  511,  ...,    0,    0,    0],\n          [ 514,  513,  512,  ...,    0,    0,    0],\n          ...,\n          [1023, 1023, 1023,  ...,  512,  511,  510],\n          [1023, 1023, 1023,  ...,  513,  512,  511],\n          [1023, 1023, 1023,  ...,  514,  513,  512]]]),\n torch.Size([10, 1024, 1024]),\n torch.Size([10, 1024, 1024]))"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_pos_matrix = torch.clamp(rel_pos_matrix, 0, 2*max_relative_position - 1).repeat(10, 1, 1)\n",
    "tmp_c2p = tmp_c2p.repeat(10, 1, 1)\n",
    "rel_pos_matrix, rel_pos_matrix.shape, tmp_c2p.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T06:36:43.163579Z",
     "start_time": "2023-08-01T06:36:43.148776Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 1.7927,  2.9470,  2.6425,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.2741,  0.8960,  4.7799,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.4359,  1.8331, -2.4628,  ..., -2.1064,  0.4153, -1.5477],\n         ...,\n         [ 0.8327,  3.4481,  7.1666,  ...,  1.8588, -0.0798,  3.0647],\n         [ 0.8327,  3.4481,  7.1666,  ..., -0.8953,  3.0796,  1.9657],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.1886,  0.8063, -0.7158]],\n\n        [[ 1.7927,  2.9470,  2.6425,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.2741,  0.8960,  4.7799,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.4359,  1.8331, -2.4628,  ..., -2.1064,  0.4153, -1.5477],\n         ...,\n         [ 0.8327,  3.4481,  7.1666,  ...,  1.8588, -0.0798,  3.0647],\n         [ 0.8327,  3.4481,  7.1666,  ..., -0.8953,  3.0796,  1.9657],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.1886,  0.8063, -0.7158]],\n\n        [[ 1.7927,  2.9470,  2.6425,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.2741,  0.8960,  4.7799,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.4359,  1.8331, -2.4628,  ..., -2.1064,  0.4153, -1.5477],\n         ...,\n         [ 0.8327,  3.4481,  7.1666,  ...,  1.8588, -0.0798,  3.0647],\n         [ 0.8327,  3.4481,  7.1666,  ..., -0.8953,  3.0796,  1.9657],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.1886,  0.8063, -0.7158]],\n\n        ...,\n\n        [[ 1.7927,  2.9470,  2.6425,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.2741,  0.8960,  4.7799,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.4359,  1.8331, -2.4628,  ..., -2.1064,  0.4153, -1.5477],\n         ...,\n         [ 0.8327,  3.4481,  7.1666,  ...,  1.8588, -0.0798,  3.0647],\n         [ 0.8327,  3.4481,  7.1666,  ..., -0.8953,  3.0796,  1.9657],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.1886,  0.8063, -0.7158]],\n\n        [[ 1.7927,  2.9470,  2.6425,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.2741,  0.8960,  4.7799,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.4359,  1.8331, -2.4628,  ..., -2.1064,  0.4153, -1.5477],\n         ...,\n         [ 0.8327,  3.4481,  7.1666,  ...,  1.8588, -0.0798,  3.0647],\n         [ 0.8327,  3.4481,  7.1666,  ..., -0.8953,  3.0796,  1.9657],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.1886,  0.8063, -0.7158]],\n\n        [[ 1.7927,  2.9470,  2.6425,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.2741,  0.8960,  4.7799,  ..., -2.1064,  0.4153, -1.5477],\n         [-0.4359,  1.8331, -2.4628,  ..., -2.1064,  0.4153, -1.5477],\n         ...,\n         [ 0.8327,  3.4481,  7.1666,  ...,  1.8588, -0.0798,  3.0647],\n         [ 0.8327,  3.4481,  7.1666,  ..., -0.8953,  3.0796,  1.9657],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.1886,  0.8063, -0.7158]]],\n       grad_fn=<GatherBackward0>)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.gather 정리 필요 => dim은 내가 인덱싱을 적용하고 싶은 차원을 지정하는 것\n",
    "인덱스 매개변수에 전달하는 행렬 안에 텐서 원소의 인덱스를 의미하는 숫자들이 마구 있는데,\n",
    "저 숫자를 어느 차원에 적용할 것인가 그걸 지정 해주는 것\n",
    "\"\"\"\n",
    "\n",
    "torch.gather(tmp_c2p, dim=-2, index=rel_pos_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T06:50:55.605313Z",
     "start_time": "2023-08-01T06:50:55.586370Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.9275, grad_fn=<SelectBackward0>)"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 검증 완료 \"\"\"\n",
    "tmp_c2p[0][0][512]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T06:38:36.111219Z",
     "start_time": "2023-08-01T06:38:36.103957Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-4.4441, -2.0285,  3.8037,  ..., -2.1064,  0.4153, -1.5477],\n         [ 0.5457, -2.0689,  2.4735,  ...,  2.0869, -0.8964, -1.1958],\n         [-1.5544, -1.4172,  0.4664,  ...,  1.3028,  0.0901,  1.3808],\n         ...,\n         [ 2.2167, -0.3013, -2.5602,  ..., -3.3431, -5.7226, -0.5048],\n         [-1.1391,  1.3073,  4.0575,  ...,  2.8553, -1.3381, -3.5350],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.8348,  8.0450,  1.2452]],\n\n        [[-4.4441, -2.0285,  3.8037,  ..., -2.1064,  0.4153, -1.5477],\n         [ 0.5457, -2.0689,  2.4735,  ...,  2.0869, -0.8964, -1.1958],\n         [-1.5544, -1.4172,  0.4664,  ...,  1.3028,  0.0901,  1.3808],\n         ...,\n         [ 2.2167, -0.3013, -2.5602,  ..., -3.3431, -5.7226, -0.5048],\n         [-1.1391,  1.3073,  4.0575,  ...,  2.8553, -1.3381, -3.5350],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.8348,  8.0450,  1.2452]],\n\n        [[-4.4441, -2.0285,  3.8037,  ..., -2.1064,  0.4153, -1.5477],\n         [ 0.5457, -2.0689,  2.4735,  ...,  2.0869, -0.8964, -1.1958],\n         [-1.5544, -1.4172,  0.4664,  ...,  1.3028,  0.0901,  1.3808],\n         ...,\n         [ 2.2167, -0.3013, -2.5602,  ..., -3.3431, -5.7226, -0.5048],\n         [-1.1391,  1.3073,  4.0575,  ...,  2.8553, -1.3381, -3.5350],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.8348,  8.0450,  1.2452]],\n\n        ...,\n\n        [[-4.4441, -2.0285,  3.8037,  ..., -2.1064,  0.4153, -1.5477],\n         [ 0.5457, -2.0689,  2.4735,  ...,  2.0869, -0.8964, -1.1958],\n         [-1.5544, -1.4172,  0.4664,  ...,  1.3028,  0.0901,  1.3808],\n         ...,\n         [ 2.2167, -0.3013, -2.5602,  ..., -3.3431, -5.7226, -0.5048],\n         [-1.1391,  1.3073,  4.0575,  ...,  2.8553, -1.3381, -3.5350],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.8348,  8.0450,  1.2452]],\n\n        [[-4.4441, -2.0285,  3.8037,  ..., -2.1064,  0.4153, -1.5477],\n         [ 0.5457, -2.0689,  2.4735,  ...,  2.0869, -0.8964, -1.1958],\n         [-1.5544, -1.4172,  0.4664,  ...,  1.3028,  0.0901,  1.3808],\n         ...,\n         [ 2.2167, -0.3013, -2.5602,  ..., -3.3431, -5.7226, -0.5048],\n         [-1.1391,  1.3073,  4.0575,  ...,  2.8553, -1.3381, -3.5350],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.8348,  8.0450,  1.2452]],\n\n        [[-4.4441, -2.0285,  3.8037,  ..., -2.1064,  0.4153, -1.5477],\n         [ 0.5457, -2.0689,  2.4735,  ...,  2.0869, -0.8964, -1.1958],\n         [-1.5544, -1.4172,  0.4664,  ...,  1.3028,  0.0901,  1.3808],\n         ...,\n         [ 2.2167, -0.3013, -2.5602,  ..., -3.3431, -5.7226, -0.5048],\n         [-1.1391,  1.3073,  4.0575,  ...,  2.8553, -1.3381, -3.5350],\n         [ 0.8327,  3.4481,  7.1666,  ...,  0.8348,  8.0450,  1.2452]]],\n       grad_fn=<RepeatBackward0>)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_c2p"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T06:37:25.632155Z",
     "start_time": "2023-08-01T06:37:25.617781Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Test for p2c in disentangled self-attention \"\"\"\n",
    "\n",
    "if tmp_c2p is not None:\n",
    "    print('t')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T07:15:10.401481Z",
     "start_time": "2023-08-01T07:15:10.399262Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
