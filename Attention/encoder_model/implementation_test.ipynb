{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-28T05:56:44.575012035Z",
     "start_time": "2023-07-28T05:56:43.122043996Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/jax/_src/lib/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m   \u001B[0;32mimport\u001B[0m \u001B[0mjaxlib\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mjaxlib\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m \u001B[0;32mexcept\u001B[0m \u001B[0mModuleNotFoundError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'jaxlib'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_31267/981726109.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfunctional\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mjax\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjax\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mjnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrandom\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/jax/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     33\u001B[0m \u001B[0;31m# We want the exported object to be the class, so we first import the module\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[0;31m# to make sure a later import doesn't overwrite the class.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mconfig\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0m_config_module\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m \u001B[0;32mdel\u001B[0m \u001B[0m_config_module\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/jax/config.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;31m# TODO(phawkins): fix users of this alias and delete this file.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_src\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconfig\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mconfig\u001B[0m  \u001B[0;31m# noqa: F401\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/jax/_src/config.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtyping\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mList\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mCallable\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mHashable\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mNamedTuple\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mIterator\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mOptional\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_src\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mlib\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_src\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlib\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mjax_jit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mjax\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_src\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlib\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtransfer_guard_lib\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/lib/python3.9/site-packages/jax/_src/lib/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     24\u001B[0m   \u001B[0;32mimport\u001B[0m \u001B[0mjaxlib\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mjaxlib\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;32mexcept\u001B[0m \u001B[0mModuleNotFoundError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m   raise ModuleNotFoundError(\n\u001B[0m\u001B[1;32m     27\u001B[0m     \u001B[0;34m'jax requires jaxlib to be installed. See '\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m     \u001B[0;34m'https://github.com/google/jax#installation for installation instructions.'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: jax requires jaxlib to be installed. See https://github.com/google/jax#installation for installation instructions."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    \"\"\"\n",
    "    Making projection matrix(Q, K, V) for each attention head\n",
    "    When you call this class, it returns projection matrix of each attention head\n",
    "    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)\n",
    "    Args:\n",
    "        num_heads: number of heads in MHA, default 8\n",
    "        dim_head: dimension of each attention head, default 64\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads: int = 8, dim_head: int = 64) -> None:\n",
    "        super(Projector, self).__init__()\n",
    "        self.dim_model = num_heads * dim_head\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_head = dim_head\n",
    "\n",
    "    def __call__(self):\n",
    "        fc_q = nn.Linear(self.dim_model, self.dim_head)\n",
    "        fc_k = nn.Linear(self.dim_model, self.dim_head)\n",
    "        fc_v = nn.Linear(self.dim_model, self.dim_head)\n",
    "        return fc_q, fc_k, fc_v\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for multi-head attention (MHA) module in vanilla transformer\n",
    "    We apply linear transformation to input vector by each attention head's projection matrix (8, 512, 64)\n",
    "    Other approaches are possible, such as using one projection matrix for all attention heads (1, 512, 512)\n",
    "    and then split into each attention heads (8. 512, 64)\n",
    "    Args:\n",
    "        dim_model: dimension of model's latent vector space, default 512 from official paper\n",
    "        num_heads: number of heads in MHA, default 8 from official paper\n",
    "        dropout: dropout rate, default 0.1\n",
    "    Math:\n",
    "        MHA(Q, K, V) = Concat(Head1, Head2, ... Head8) * W_concat\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/1706.03762\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dim = dim_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dim_head = int(self.dim / self.num_heads)  # dimension of each attention head\n",
    "        self.dot_scale = torch.sqrt(torch.tensor(self.dim_head))  # scale factor for Qâ€¢K^T Result\n",
    "\n",
    "        # linear combination: projection matrix(Q_1, K_1, V_1, ... Q_n, K_n, V_n) for each attention head\n",
    "        self.projector = Projector(self.num_heads, self.dim_head)  # init instance\n",
    "        self.projector_list = [list(self.projector()) for _ in range(self.num_heads)]  # call instance\n",
    "        self.fc_concat = nn.Linear(self.dim, self.dim)  # for concatenation of each attention head\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: bool = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        1) make Q, K, V matrix for each attention head: [BS, HEAD, SEQ_LEN, DIM_HEAD], ex) [10, 8, 512, 64]\n",
    "        2) Do self-attention in each attention head\n",
    "            - Matmul (Q, K^T) with scale factor (sqrt(DIM_HEAD))\n",
    "            - Mask for padding token (Option for Decoder)\n",
    "            - Softmax\n",
    "            - Matmul (Softmax, V)\n",
    "        3) Concatenate each attention head & linear transformation (512, 512)\n",
    "        \"\"\"\n",
    "        # 1) make Q, K, V matrix for each attention head\n",
    "        Q, K, V = [], [], []\n",
    "\n",
    "        for i in range(self.num_heads):\n",
    "            Q.append(self.projector_list[i][0](x))\n",
    "            K.append(self.projector_list[i][1](x))\n",
    "            V.append(self.projector_list[i][2](x))\n",
    "\n",
    "        Q = torch.stack(Q, dim=1)\n",
    "        K = torch.stack(K, dim=1)\n",
    "        V = torch.stack(V, dim=1)\n",
    "        # 2) Do self-attention in each attention head\n",
    "        attention_score = torch.matmul(Q, K.transpose(-1, -2)) / self.dot_scale\n",
    "        if mask is not None:  # for padding token\n",
    "            attention_score[mask] = float('-inf')\n",
    "        attention_dist = F.softmax(attention_score, dim=-1)  # [BS, HEAD, SEQ_LEN, SEQ_LEN]\n",
    "        attention_matrix = torch.matmul(attention_dist, V).transpose(1, 2).reshape(x.shape[0], x.shape[1], self.dim)  # [BS, SEQ_LEN, DIM]\n",
    "\n",
    "        # 3) Concatenate each attention head & linear transformation (512, 512)\n",
    "        x = self.fc_concat(attention_matrix)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T12:22:07.207490343Z",
     "start_time": "2023-07-18T12:22:07.200225153Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Debug for MultiHeadAttention \"\"\"\n",
    "\n",
    "x = torch.randn(10, 512, 512)\n",
    "test_head = MultiHeadAttention()\n",
    "test_result = test_head(x)\n",
    "test_result, test_result.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1024, 768])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" torch.reshape test for making input shape in Vision Transformers \"\"\"\n",
    "patch_size, num_patches = 16, 32\n",
    "x = torch.randn(10, 3, 512, 512)\n",
    "x = x.reshape(x.shape[0], num_patches**2, patch_size**2 * x.shape[1])\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:10.284936109Z",
     "start_time": "2023-07-18T14:17:10.207383191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1024, 1024])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Check Input Embedding shape \"\"\"\n",
    "input_embedding = nn.Linear(768, 1024)\n",
    "x = input_embedding(x)\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:11.601900007Z",
     "start_time": "2023-07-18T14:17:11.545842018Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1, 1024])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" make classification token for Vision Transformers \"\"\"\n",
    "cls_token = torch.zeros(x.shape[0], 1, x.shape[2])  # can change init method\n",
    "cls_token.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:43.334781057Z",
     "start_time": "2023-07-18T14:17:43.332286758Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1025, 1024])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([cls_token, x], dim=1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:53.794588640Z",
     "start_time": "2023-07-18T14:17:53.775080340Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1024, 512])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Test for Hybrid Model \"\"\"\n",
    "x = torch.randn(10, 3, 512, 512)\n",
    "dim_model = 512\n",
    "patch_size = 16\n",
    "num_patches = 32\n",
    "conv = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=dim_model,\n",
    "            kernel_size=patch_size,\n",
    "            stride=16\n",
    ")\n",
    "x = conv(x).reshape(x.shape[0], dim_model, num_patches**2).transpose(-1, -2)\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T13:35:54.918886Z",
     "start_time": "2023-07-24T13:35:54.817215Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "1024"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T13:29:33.814138Z",
     "start_time": "2023-07-24T13:29:33.809814Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 2048])\n",
      "torch.Size([1024, 2048])\n",
      "torch.Size([512, 64])\n",
      "torch.Size([1024, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([512, 1024])"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Test for DeBERTa Disentangled Self-Attention \"\"\"\n",
    "batch, sequence, dim_model, dim_head, k = 10, 512, 2048, 64, 512\n",
    "x = torch.randn(sequence, dim_model)  # [Batch, Sequence, Dim]\n",
    "p_x = torch.randn(2*k, dim_model)\n",
    "print(x.shape)\n",
    "print(p_x.shape)\n",
    "\n",
    "fc_q = nn.Linear(dim_model, dim_head)\n",
    "fc_k = nn.Linear(dim_model, dim_head)\n",
    "fc_v = nn.Linear(dim_model, dim_head)\n",
    "fc_qr = nn.Linear(dim_model, dim_head)  # projector for Relative Position Query matrix\n",
    "fc_kr = nn.Linear(dim_model, dim_head)  # projector for Relative Position Key matrix\n",
    "\n",
    "q = fc_q(x)\n",
    "kr = fc_kr(p_x)\n",
    "print(q.shape)\n",
    "print(kr.shape)\n",
    "\n",
    "tmp_c2p= torch.stack(\n",
    "    [torch.matmul(q[i, :], kr.transpose(-1, -2)) for i in range(x.shape[0])],\n",
    "    dim=0\n",
    ")\n",
    "\n",
    "tmp_c2p.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T06:16:52.703469254Z",
     "start_time": "2023-07-28T06:16:52.649301029Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([512, 1024])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T06:16:03.017210714Z",
     "start_time": "2023-07-28T06:16:02.991988701Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
