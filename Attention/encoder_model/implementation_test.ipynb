{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-19T01:00:37.403153Z",
     "start_time": "2023-07-19T01:00:37.399389Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import flax.linen as fnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 1, 1024)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_ones = jnp.ones((10, 1, 1024))\n",
    "matrix_ones.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T00:58:38.280436Z",
     "start_time": "2023-07-19T00:58:38.275237Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 1, 10)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dense = fnn.Dense(10)\n",
    "variables = test_dense.init(random.PRNGKey(0), jnp.ones((10, 1, 1024)))\n",
    "y = test_dense.apply(variables, jnp.ones((10, 1, 1024)))\n",
    "y.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-19T01:01:57.027494Z",
     "start_time": "2023-07-19T01:01:57.017891Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Projector(nn.Module):\n",
    "    \"\"\"\n",
    "    Making projection matrix(Q, K, V) for each attention head\n",
    "    When you call this class, it returns projection matrix of each attention head\n",
    "    For example, if you call this class with 8 heads, it returns 8 set of projection matrices (Q, K, V)\n",
    "    Args:\n",
    "        num_heads: number of heads in MHA, default 8\n",
    "        dim_head: dimension of each attention head, default 64\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads: int = 8, dim_head: int = 64) -> None:\n",
    "        super(Projector, self).__init__()\n",
    "        self.dim_model = num_heads * dim_head\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_head = dim_head\n",
    "\n",
    "    def __call__(self):\n",
    "        fc_q = nn.Linear(self.dim_model, self.dim_head)\n",
    "        fc_k = nn.Linear(self.dim_model, self.dim_head)\n",
    "        fc_v = nn.Linear(self.dim_model, self.dim_head)\n",
    "        return fc_q, fc_k, fc_v\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for multi-head attention (MHA) module in vanilla transformer\n",
    "    We apply linear transformation to input vector by each attention head's projection matrix (8, 512, 64)\n",
    "    Other approaches are possible, such as using one projection matrix for all attention heads (1, 512, 512)\n",
    "    and then split into each attention heads (8. 512, 64)\n",
    "    Args:\n",
    "        dim_model: dimension of model's latent vector space, default 512 from official paper\n",
    "        num_heads: number of heads in MHA, default 8 from official paper\n",
    "        dropout: dropout rate, default 0.1\n",
    "    Math:\n",
    "        MHA(Q, K, V) = Concat(Head1, Head2, ... Head8) * W_concat\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/1706.03762\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model: int = 512, num_heads: int = 8, dropout: float = 0.1) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dim = dim_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.dim_head = int(self.dim / self.num_heads)  # dimension of each attention head\n",
    "        self.dot_scale = torch.sqrt(torch.tensor(self.dim_head))  # scale factor for Qâ€¢K^T Result\n",
    "\n",
    "        # linear combination: projection matrix(Q_1, K_1, V_1, ... Q_n, K_n, V_n) for each attention head\n",
    "        self.projector = Projector(self.num_heads, self.dim_head)  # init instance\n",
    "        self.projector_list = [list(self.projector()) for _ in range(self.num_heads)]  # call instance\n",
    "        self.fc_concat = nn.Linear(self.dim, self.dim)  # for concatenation of each attention head\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: bool = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        1) make Q, K, V matrix for each attention head: [BS, HEAD, SEQ_LEN, DIM_HEAD], ex) [10, 8, 512, 64]\n",
    "        2) Do self-attention in each attention head\n",
    "            - Matmul (Q, K^T) with scale factor (sqrt(DIM_HEAD))\n",
    "            - Mask for padding token (Option for Decoder)\n",
    "            - Softmax\n",
    "            - Matmul (Softmax, V)\n",
    "        3) Concatenate each attention head & linear transformation (512, 512)\n",
    "        \"\"\"\n",
    "        # 1) make Q, K, V matrix for each attention head\n",
    "        Q, K, V = [], [], []\n",
    "\n",
    "        for i in range(self.num_heads):\n",
    "            Q.append(self.projector_list[i][0](x))\n",
    "            K.append(self.projector_list[i][1](x))\n",
    "            V.append(self.projector_list[i][2](x))\n",
    "\n",
    "        Q = torch.stack(Q, dim=1)\n",
    "        K = torch.stack(K, dim=1)\n",
    "        V = torch.stack(V, dim=1)\n",
    "        # 2) Do self-attention in each attention head\n",
    "        attention_score = torch.matmul(Q, K.transpose(-1, -2)) / self.dot_scale\n",
    "        if mask is not None:  # for padding token\n",
    "            attention_score[mask] = float('-inf')\n",
    "        attention_dist = F.softmax(attention_score, dim=-1)  # [BS, HEAD, SEQ_LEN, SEQ_LEN]\n",
    "        attention_matrix = torch.matmul(attention_dist, V).transpose(1, 2).reshape(x.shape[0], x.shape[1], self.dim)  # [BS, SEQ_LEN, DIM]\n",
    "\n",
    "        # 3) Concatenate each attention head & linear transformation (512, 512)\n",
    "        x = self.fc_concat(attention_matrix)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T12:22:07.207490343Z",
     "start_time": "2023-07-18T12:22:07.200225153Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\" Debug for MultiHeadAttention \"\"\"\n",
    "\n",
    "x = torch.randn(10, 512, 512)\n",
    "test_head = MultiHeadAttention()\n",
    "test_result = test_head(x)\n",
    "test_result, test_result.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1024, 768])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" torch.reshape test for making input shape in Vision Transformers \"\"\"\n",
    "patch_size, num_patches = 16, 32\n",
    "x = torch.randn(10, 3, 512, 512)\n",
    "x = x.reshape(x.shape[0], num_patches**2, patch_size**2 * x.shape[1])\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:10.284936109Z",
     "start_time": "2023-07-18T14:17:10.207383191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1024, 1024])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Check Input Embedding shape \"\"\"\n",
    "input_embedding = nn.Linear(768, 1024)\n",
    "x = input_embedding(x)\n",
    "x.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:11.601900007Z",
     "start_time": "2023-07-18T14:17:11.545842018Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1, 1024])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" make classification token for Vision Transformers \"\"\"\n",
    "cls_token = torch.zeros(x.shape[0], 1, x.shape[2])  # can change init method\n",
    "cls_token.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:43.334781057Z",
     "start_time": "2023-07-18T14:17:43.332286758Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 1025, 1024])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([cls_token, x], dim=1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T14:17:53.794588640Z",
     "start_time": "2023-07-18T14:17:53.775080340Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
