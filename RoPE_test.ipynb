{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import experiment.metrics.metric as metric\n",
    "from experiment.metrics.metric import cosine_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T03:27:07.449794Z",
     "start_time": "2024-03-04T03:27:06.430226Z"
    }
   },
   "id": "f1b867fbf9578cf4",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" set default device to mps \"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T03:27:09.278535Z",
     "start_time": "2024-03-04T03:27:09.208730Z"
    }
   },
   "id": "eedaeeb4c236e989",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(9663676416, 1207959552, 8)"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadratic = 512*64*64*512 + 512*512*512*64\n",
    "linear = 64*512*512*64 + 512*64*64*64\n",
    "\n",
    "quadratic, linear, quadratic // linear"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:48.714062Z",
     "start_time": "2024-03-02T13:30:48.709081Z"
    }
   },
   "id": "5c0366bb014d387d",
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([16, 512, 64]),\n torch.Size([16, 512, 64]),\n torch.Size([16, 512, 64]))"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Inputs for Linear Attentions\n",
    "\"\"\"\n",
    "\n",
    "max_seq = 512\n",
    "dim_model = 768\n",
    "num_heads = 12\n",
    "batch_size = 16\n",
    "dim_head = dim_model // num_heads\n",
    "\n",
    "def kernel_fn(x: Tensor):\n",
    "    return F.elu(x) + 1\n",
    "\n",
    "query = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "key = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "value = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "\n",
    "query.shape, key.shape, value.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T03:52:10.376415Z",
     "start_time": "2024-03-04T03:52:10.372818Z"
    }
   },
   "id": "2db69b58bd5032a",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[379.2272, 363.0186, 405.7908,  ..., 374.7338, 377.7190, 375.9309],\n          [382.8666, 358.6458, 400.0844,  ..., 372.0980, 374.7551, 371.8542],\n          [382.7687, 360.3486, 408.0707,  ..., 377.3307, 377.7459, 373.8784],\n          ...,\n          [380.0161, 362.1071, 402.2729,  ..., 374.3331, 373.1011, 374.5012],\n          [385.2000, 365.0188, 408.9546,  ..., 378.1088, 378.6559, 381.6468],\n          [386.3977, 363.9138, 406.5035,  ..., 377.4268, 378.1671, 373.0061]],\n \n         [[373.5042, 380.5840, 375.6325,  ..., 418.3877, 394.5324, 387.5174],\n          [370.0285, 382.9928, 377.6545,  ..., 418.3036, 395.9686, 387.1384],\n          [366.8623, 378.5196, 371.6406,  ..., 414.5091, 388.5123, 383.9243],\n          ...,\n          [364.2798, 377.8297, 369.8265,  ..., 409.8225, 388.3452, 379.5322],\n          [365.6089, 376.0946, 366.6428,  ..., 405.7575, 384.7750, 378.0374],\n          [372.2574, 383.5679, 375.1379,  ..., 416.8979, 396.5555, 389.8101]],\n \n         [[386.6698, 386.0729, 402.2727,  ..., 373.1522, 384.8525, 392.0824],\n          [382.1305, 384.5549, 403.0435,  ..., 378.4492, 383.5367, 389.9436],\n          [383.8008, 389.3018, 397.8065,  ..., 374.6421, 384.2864, 392.7018],\n          ...,\n          [378.5762, 385.2162, 394.9633,  ..., 366.8360, 380.6320, 383.7389],\n          [377.5324, 379.8900, 392.1647,  ..., 366.0445, 375.6336, 382.2607],\n          [377.4836, 381.8419, 394.6746,  ..., 369.2387, 376.6471, 386.3919]],\n \n         ...,\n \n         [[375.1440, 376.6949, 382.7990,  ..., 382.5022, 389.2272, 359.8630],\n          [364.4118, 374.6257, 380.4706,  ..., 380.0315, 381.6766, 358.5752],\n          [366.6164, 377.1288, 381.9821,  ..., 383.4064, 388.0214, 357.2177],\n          ...,\n          [370.4831, 377.7444, 385.2213,  ..., 385.7294, 388.9689, 359.7659],\n          [369.0250, 376.6795, 384.4611,  ..., 376.1245, 384.0508, 357.1576],\n          [360.2135, 371.8775, 375.5447,  ..., 378.2064, 385.3540, 349.5546]],\n \n         [[367.8559, 385.9541, 363.4925,  ..., 383.6826, 389.1495, 371.7243],\n          [364.7525, 378.8201, 363.5299,  ..., 376.4540, 385.9244, 371.4995],\n          [367.2783, 381.2596, 364.0417,  ..., 383.0886, 389.3735, 373.0959],\n          ...,\n          [362.8450, 375.9204, 359.4895,  ..., 379.6590, 379.5880, 367.9471],\n          [368.9227, 381.7363, 365.5523,  ..., 384.0696, 390.9930, 379.3000],\n          [368.9711, 382.5988, 362.9704,  ..., 380.5606, 389.1813, 373.4011]],\n \n         [[398.4991, 401.6938, 402.3007,  ..., 379.7724, 388.0056, 402.2548],\n          [396.9542, 403.3658, 402.3756,  ..., 379.6757, 389.9531, 403.2303],\n          [395.1298, 393.4580, 396.2539,  ..., 374.3688, 386.0089, 395.4426],\n          ...,\n          [396.8712, 395.0952, 394.6886,  ..., 376.5549, 386.4319, 400.0087],\n          [389.6586, 392.4140, 391.5121,  ..., 371.8892, 380.5839, 393.3044],\n          [395.9511, 399.3736, 401.2036,  ..., 379.3433, 384.5514, 398.1483]]],\n        device='cuda:0'),\n torch.Size([16, 64, 64]))"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: KV Matrix\n",
    "\"\"\"\n",
    "k_query, k_key = kernel_fn(query), kernel_fn(key)\n",
    "KV = torch.matmul(k_key.permute(0, 2, 1), value)\n",
    "KV, KV.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T03:52:11.044725Z",
     "start_time": "2024-03-04T03:52:11.033161Z"
    }
   },
   "id": "1d4d4a249fb2cf68",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[36912.2305, 34959.8398, 39018.5078,  ..., 36197.7031,\n           36283.9922, 36281.0859],\n          [37543.7148, 35541.6406, 39672.7461,  ..., 36797.5664,\n           36884.6523, 36891.7734],\n          [37298.1992, 35312.1914, 39421.0391,  ..., 36573.7734,\n           36654.2070, 36651.3984],\n          ...,\n          [37635.8438, 35628.3555, 39772.7500,  ..., 36887.6562,\n           36976.1797, 36984.2109],\n          [37077.3633, 35104.5781, 39195.0625,  ..., 36351.3633,\n           36436.9688, 36436.4336],\n          [36029.4844, 34113.1367, 38085.0508,  ..., 35314.2383,\n           35399.2734, 35410.6836]],\n \n         [[34804.4492, 35761.9805, 35283.5273,  ..., 39085.5000,\n           36992.9805, 36303.0273],\n          [35909.3945, 36904.7227, 36419.6562,  ..., 40332.0898,\n           38173.2188, 37446.2070],\n          [34745.9336, 35694.1250, 35222.6328,  ..., 39017.3516,\n           36924.3242, 36230.8633],\n          ...,\n          [33928.2227, 34872.9297, 34411.9258,  ..., 38108.4648,\n           36067.2656, 35395.9531],\n          [35694.5586, 36688.6289, 36194.5039,  ..., 40083.9883,\n           37940.2383, 37219.7031],\n          [34220.8125, 35171.6797, 34714.6992,  ..., 38438.2969,\n           36383.4688, 35696.3477]],\n \n         [[37399.6758, 37681.2578, 38922.3828,  ..., 36503.1133,\n           37451.9453, 38000.2461],\n          [35910.3398, 36187.7539, 37372.9648,  ..., 35056.7266,\n           35968.4609, 36493.9766],\n          [36233.2617, 36510.6992, 37703.6328,  ..., 35359.7188,\n           36290.5430, 36814.1406],\n          ...,\n          [37938.0156, 38225.4023, 39479.4414,  ..., 37024.3711,\n           37995.0391, 38560.1875],\n          [36596.4023, 36873.4141, 38082.9688,  ..., 35714.7578,\n           36661.8984, 37198.8164],\n          [35981.5898, 36257.0195, 37444.3555,  ..., 35115.4180,\n           36038.8008, 36574.6523]],\n \n         ...,\n \n         [[34506.6133, 35144.3242, 35697.3711,  ..., 35621.3438,\n           36119.9609, 33483.8438],\n          [35881.9102, 36541.3008, 37116.6055,  ..., 37029.1133,\n           37550.8594, 34809.3633],\n          [35072.6211, 35722.7031, 36292.5703,  ..., 36195.7266,\n           36707.5391, 34028.0586],\n          ...,\n          [34940.7188, 35575.9570, 36145.1758,  ..., 36062.5039,\n           36571.3008, 33899.4844],\n          [35442.4023, 36095.0000, 36668.4961,  ..., 36581.5469,\n           37095.3633, 34391.6875],\n          [36427.7383, 37099.7148, 37687.8594,  ..., 37591.6172,\n           38124.8633, 35340.0039]],\n \n         [[34373.6914, 35680.9844, 34010.0039,  ..., 35644.7148,\n           36150.5000, 34716.2812],\n          [36296.7617, 37674.7070, 35907.5078,  ..., 37635.5000,\n           38175.6953, 36668.2891],\n          [36834.0312, 38233.1523, 36448.1992,  ..., 38198.6211,\n           38739.3359, 37210.1953],\n          ...,\n          [34880.5938, 36197.8750, 34504.3047,  ..., 36168.7695,\n           36671.1562, 35220.9688],\n          [34646.4688, 35953.3711, 34276.6328,  ..., 35932.6367,\n           36435.0508, 34987.9570],\n          [34194.4219, 35492.1523, 33836.2070,  ..., 35459.9883,\n           35959.5195, 34533.5234]],\n \n         [[38272.6719, 38460.1914, 38587.8789,  ..., 36526.4492,\n           37352.9180, 38598.8008],\n          [36291.8867, 36463.6289, 36583.9727,  ..., 34627.8477,\n           35425.0547, 36602.1484],\n          [39074.2539, 39278.1094, 39416.7617,  ..., 37300.5820,\n           38148.5195, 39418.8906],\n          ...,\n          [37937.8125, 38126.3398, 38252.1641,  ..., 36212.6016,\n           37037.6172, 38262.0859],\n          [39154.4336, 39343.9258, 39479.3164,  ..., 37364.7500,\n           38219.3594, 39495.0430],\n          [37307.7500, 37486.8125, 37616.4492,  ..., 35614.1523,\n           36418.9453, 37628.7695]]], device='cuda:0'),\n torch.Size([16, 512, 64]))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: QKV\n",
    "\"\"\"\n",
    "\n",
    "QKV = torch.matmul(k_query, KV)\n",
    "QKV, QKV.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T03:52:11.223651Z",
     "start_time": "2024-03-04T03:52:11.212329Z"
    }
   },
   "id": "56d258a4ac1b2e78",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key, summation_key: (torch.Size([16, 512, 64]), torch.Size([16, 64, 64]))\n",
      "Normalizer Z: (tensor([[[1.3546e-05, 1.3546e-05, 1.3546e-05,  ..., 1.3546e-05,\n",
      "          1.3546e-05, 1.3546e-05],\n",
      "         [1.3323e-05, 1.3323e-05, 1.3323e-05,  ..., 1.3323e-05,\n",
      "          1.3323e-05, 1.3323e-05],\n",
      "         [1.3408e-05, 1.3408e-05, 1.3408e-05,  ..., 1.3408e-05,\n",
      "          1.3408e-05, 1.3408e-05],\n",
      "         ...,\n",
      "         [1.3290e-05, 1.3290e-05, 1.3290e-05,  ..., 1.3290e-05,\n",
      "          1.3290e-05, 1.3290e-05],\n",
      "         [1.3488e-05, 1.3488e-05, 1.3488e-05,  ..., 1.3488e-05,\n",
      "          1.3488e-05, 1.3488e-05],\n",
      "         [1.3881e-05, 1.3881e-05, 1.3881e-05,  ..., 1.3881e-05,\n",
      "          1.3881e-05, 1.3881e-05]],\n",
      "\n",
      "        [[1.3802e-05, 1.3802e-05, 1.3802e-05,  ..., 1.3802e-05,\n",
      "          1.3802e-05, 1.3802e-05],\n",
      "         [1.3376e-05, 1.3376e-05, 1.3376e-05,  ..., 1.3376e-05,\n",
      "          1.3376e-05, 1.3376e-05],\n",
      "         [1.3828e-05, 1.3828e-05, 1.3828e-05,  ..., 1.3828e-05,\n",
      "          1.3828e-05, 1.3828e-05],\n",
      "         ...,\n",
      "         [1.4156e-05, 1.4156e-05, 1.4156e-05,  ..., 1.4156e-05,\n",
      "          1.4156e-05, 1.4156e-05],\n",
      "         [1.3458e-05, 1.3458e-05, 1.3458e-05,  ..., 1.3458e-05,\n",
      "          1.3458e-05, 1.3458e-05],\n",
      "         [1.4035e-05, 1.4035e-05, 1.4035e-05,  ..., 1.4035e-05,\n",
      "          1.4035e-05, 1.4035e-05]],\n",
      "\n",
      "        [[1.3291e-05, 1.3291e-05, 1.3291e-05,  ..., 1.3291e-05,\n",
      "          1.3291e-05, 1.3291e-05],\n",
      "         [1.3840e-05, 1.3840e-05, 1.3840e-05,  ..., 1.3840e-05,\n",
      "          1.3840e-05, 1.3840e-05],\n",
      "         [1.3719e-05, 1.3719e-05, 1.3719e-05,  ..., 1.3719e-05,\n",
      "          1.3719e-05, 1.3719e-05],\n",
      "         ...,\n",
      "         [1.3102e-05, 1.3102e-05, 1.3102e-05,  ..., 1.3102e-05,\n",
      "          1.3102e-05, 1.3102e-05],\n",
      "         [1.3582e-05, 1.3582e-05, 1.3582e-05,  ..., 1.3582e-05,\n",
      "          1.3582e-05, 1.3582e-05],\n",
      "         [1.3813e-05, 1.3813e-05, 1.3813e-05,  ..., 1.3813e-05,\n",
      "          1.3813e-05, 1.3813e-05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.3950e-05, 1.3950e-05, 1.3950e-05,  ..., 1.3950e-05,\n",
      "          1.3950e-05, 1.3950e-05],\n",
      "         [1.3419e-05, 1.3419e-05, 1.3419e-05,  ..., 1.3419e-05,\n",
      "          1.3419e-05, 1.3419e-05],\n",
      "         [1.3726e-05, 1.3726e-05, 1.3726e-05,  ..., 1.3726e-05,\n",
      "          1.3726e-05, 1.3726e-05],\n",
      "         ...,\n",
      "         [1.3779e-05, 1.3779e-05, 1.3779e-05,  ..., 1.3779e-05,\n",
      "          1.3779e-05, 1.3779e-05],\n",
      "         [1.3584e-05, 1.3584e-05, 1.3584e-05,  ..., 1.3584e-05,\n",
      "          1.3584e-05, 1.3584e-05],\n",
      "         [1.3217e-05, 1.3217e-05, 1.3217e-05,  ..., 1.3217e-05,\n",
      "          1.3217e-05, 1.3217e-05]],\n",
      "\n",
      "        [[1.4026e-05, 1.4026e-05, 1.4026e-05,  ..., 1.4026e-05,\n",
      "          1.4026e-05, 1.4026e-05],\n",
      "         [1.3282e-05, 1.3282e-05, 1.3282e-05,  ..., 1.3282e-05,\n",
      "          1.3282e-05, 1.3282e-05],\n",
      "         [1.3088e-05, 1.3088e-05, 1.3088e-05,  ..., 1.3088e-05,\n",
      "          1.3088e-05, 1.3088e-05],\n",
      "         ...,\n",
      "         [1.3824e-05, 1.3824e-05, 1.3824e-05,  ..., 1.3824e-05,\n",
      "          1.3824e-05, 1.3824e-05],\n",
      "         [1.3916e-05, 1.3916e-05, 1.3916e-05,  ..., 1.3916e-05,\n",
      "          1.3916e-05, 1.3916e-05],\n",
      "         [1.4100e-05, 1.4100e-05, 1.4100e-05,  ..., 1.4100e-05,\n",
      "          1.4100e-05, 1.4100e-05]],\n",
      "\n",
      "        [[1.3466e-05, 1.3466e-05, 1.3466e-05,  ..., 1.3466e-05,\n",
      "          1.3466e-05, 1.3466e-05],\n",
      "         [1.4202e-05, 1.4202e-05, 1.4202e-05,  ..., 1.4202e-05,\n",
      "          1.4202e-05, 1.4202e-05],\n",
      "         [1.3186e-05, 1.3186e-05, 1.3186e-05,  ..., 1.3186e-05,\n",
      "          1.3186e-05, 1.3186e-05],\n",
      "         ...,\n",
      "         [1.3584e-05, 1.3584e-05, 1.3584e-05,  ..., 1.3584e-05,\n",
      "          1.3584e-05, 1.3584e-05],\n",
      "         [1.3162e-05, 1.3162e-05, 1.3162e-05,  ..., 1.3162e-05,\n",
      "          1.3162e-05, 1.3162e-05],\n",
      "         [1.3814e-05, 1.3814e-05, 1.3814e-05,  ..., 1.3814e-05,\n",
      "          1.3814e-05, 1.3814e-05]]], device='cuda:0'), torch.Size([16, 512, 64]))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: QKV / normalizer Z\n",
    "softmax는 row-wise하게 정규화 하는데, 우리도 똑같이 정규화가 필요하지 않냐고 그래서 Z가 필요\n",
    "여기서, QKV가 결국 row-wise 하게 정규화 되어야 한다는게 포인트임\n",
    "그렇다면 Z의 크기는 16, 512, 64가 되어야 한다\n",
    "\"\"\"\n",
    "\n",
    "# summation_key = k_key.sum(dim=1).unsqueeze(1).expand(-1, dim_head, -1)\n",
    "summation_key = k_key.sum(dim=1).unsqueeze(1).expand(-1, dim_head, -1).permute(0,2,1)\n",
    "\n",
    "print(f\"key, summation_key: {key.shape, summation_key.shape}\")\n",
    "\n",
    "Z = 1/torch.matmul(k_query, summation_key)\n",
    "print(f\"Normalizer Z: {Z, Z.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T03:52:11.380832Z",
     "start_time": "2024-03-04T03:52:11.368828Z"
    }
   },
   "id": "4b94b3decb9e4270",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_attn_matrix: (torch.Size([16, 512, 64]), tensor([[[0.5000, 0.4736, 0.5286,  ..., 0.4903, 0.4915, 0.4915],\n",
      "         [0.5002, 0.4735, 0.5285,  ..., 0.4902, 0.4914, 0.4915],\n",
      "         [0.5001, 0.4735, 0.5285,  ..., 0.4904, 0.4914, 0.4914],\n",
      "         ...,\n",
      "         [0.5002, 0.4735, 0.5286,  ..., 0.4902, 0.4914, 0.4915],\n",
      "         [0.5001, 0.4735, 0.5287,  ..., 0.4903, 0.4915, 0.4915],\n",
      "         [0.5001, 0.4735, 0.5287,  ..., 0.4902, 0.4914, 0.4915]],\n",
      "\n",
      "        [[0.4804, 0.4936, 0.4870,  ..., 0.5394, 0.5106, 0.5010],\n",
      "         [0.4803, 0.4936, 0.4871,  ..., 0.5395, 0.5106, 0.5009],\n",
      "         [0.4805, 0.4936, 0.4870,  ..., 0.5395, 0.5106, 0.5010],\n",
      "         ...,\n",
      "         [0.4803, 0.4937, 0.4871,  ..., 0.5395, 0.5106, 0.5011],\n",
      "         [0.4804, 0.4937, 0.4871,  ..., 0.5394, 0.5106, 0.5009],\n",
      "         [0.4803, 0.4936, 0.4872,  ..., 0.5395, 0.5106, 0.5010]],\n",
      "\n",
      "        [[0.4971, 0.5008, 0.5173,  ..., 0.4852, 0.4978, 0.5051],\n",
      "         [0.4970, 0.5008, 0.5172,  ..., 0.4852, 0.4978, 0.5051],\n",
      "         [0.4971, 0.5009, 0.5173,  ..., 0.4851, 0.4979, 0.5051],\n",
      "         ...,\n",
      "         [0.4970, 0.5008, 0.5172,  ..., 0.4851, 0.4978, 0.5052],\n",
      "         [0.4970, 0.5008, 0.5172,  ..., 0.4851, 0.4979, 0.5052],\n",
      "         [0.4970, 0.5008, 0.5172,  ..., 0.4850, 0.4978, 0.5052]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.4814, 0.4903, 0.4980,  ..., 0.4969, 0.5039, 0.4671],\n",
      "         [0.4815, 0.4903, 0.4981,  ..., 0.4969, 0.5039, 0.4671],\n",
      "         [0.4814, 0.4903, 0.4981,  ..., 0.4968, 0.5038, 0.4671],\n",
      "         ...,\n",
      "         [0.4815, 0.4902, 0.4981,  ..., 0.4969, 0.5039, 0.4671],\n",
      "         [0.4815, 0.4903, 0.4981,  ..., 0.4969, 0.5039, 0.4672],\n",
      "         [0.4815, 0.4904, 0.4981,  ..., 0.4969, 0.5039, 0.4671]],\n",
      "\n",
      "        [[0.4821, 0.5005, 0.4770,  ..., 0.5000, 0.5070, 0.4869],\n",
      "         [0.4821, 0.5004, 0.4769,  ..., 0.4999, 0.5071, 0.4870],\n",
      "         [0.4821, 0.5004, 0.4770,  ..., 0.5000, 0.5070, 0.4870],\n",
      "         ...,\n",
      "         [0.4822, 0.5004, 0.4770,  ..., 0.5000, 0.5069, 0.4869],\n",
      "         [0.4822, 0.5003, 0.4770,  ..., 0.5001, 0.5070, 0.4869],\n",
      "         [0.4821, 0.5004, 0.4771,  ..., 0.5000, 0.5070, 0.4869]],\n",
      "\n",
      "        [[0.5154, 0.5179, 0.5196,  ..., 0.4919, 0.5030, 0.5198],\n",
      "         [0.5154, 0.5178, 0.5196,  ..., 0.4918, 0.5031, 0.5198],\n",
      "         [0.5152, 0.5179, 0.5198,  ..., 0.4919, 0.5030, 0.5198],\n",
      "         ...,\n",
      "         [0.5153, 0.5179, 0.5196,  ..., 0.4919, 0.5031, 0.5198],\n",
      "         [0.5154, 0.5179, 0.5196,  ..., 0.4918, 0.5031, 0.5198],\n",
      "         [0.5154, 0.5178, 0.5196,  ..., 0.4920, 0.5031, 0.5198]]],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: QKV / normalizer Z \n",
    "\"\"\"\n",
    "\n",
    "linear_attn_matrix = torch.mul(QKV, Z)\n",
    "print(f\"linear_attn_matrix: {linear_attn_matrix.shape, linear_attn_matrix}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T03:52:11.547012Z",
     "start_time": "2024-03-04T03:52:11.534291Z"
    }
   },
   "id": "65459cc6481c7d45",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_matrix: (torch.Size([16, 512, 512]), tensor([[[2.2749, 1.9622, 1.9245,  ..., 2.1339, 1.7951, 1.9170],\n",
      "         [2.2627, 2.0734, 2.0455,  ..., 2.2357, 2.0193, 2.0438],\n",
      "         [2.3995, 1.9777, 2.0033,  ..., 2.3385, 2.0694, 2.0637],\n",
      "         ...,\n",
      "         [2.3627, 2.0644, 2.1282,  ..., 2.2813, 1.9809, 2.0667],\n",
      "         [2.4133, 2.0074, 1.9502,  ..., 2.2852, 1.9838, 2.0096],\n",
      "         [2.1986, 1.6938, 1.7289,  ..., 2.0817, 1.8390, 1.8193]],\n",
      "\n",
      "        [[1.8141, 1.9757, 2.1443,  ..., 1.9633, 1.9246, 2.0287],\n",
      "         [2.1800, 1.8266, 2.1786,  ..., 2.0976, 1.9825, 2.0626],\n",
      "         [1.7646, 1.8651, 1.9468,  ..., 1.9577, 1.9091, 1.8173],\n",
      "         ...,\n",
      "         [1.5570, 1.6722, 1.8400,  ..., 1.7102, 1.5921, 1.8502],\n",
      "         [1.9877, 1.9390, 1.9734,  ..., 2.0285, 2.1212, 1.9973],\n",
      "         [1.7255, 1.7074, 1.8447,  ..., 1.8424, 1.7684, 1.6644]],\n",
      "\n",
      "        [[1.9951, 1.8562, 2.0399,  ..., 2.3830, 2.0018, 2.1632],\n",
      "         [1.6052, 1.8875, 1.9085,  ..., 1.9347, 1.8365, 1.8557],\n",
      "         [1.7836, 1.7922, 1.9152,  ..., 1.9674, 2.0343, 1.8682],\n",
      "         ...,\n",
      "         [1.9376, 2.1379, 2.2995,  ..., 2.1605, 2.1881, 2.2574],\n",
      "         [1.8173, 1.8907, 1.9486,  ..., 1.9892, 2.0173, 2.1309],\n",
      "         [1.6734, 1.9856, 1.8805,  ..., 1.9787, 1.8889, 1.9026]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.0433, 1.7152, 1.7138,  ..., 1.9863, 1.9132, 1.8138],\n",
      "         [2.2417, 2.0986, 1.9276,  ..., 2.0654, 2.0490, 2.1257],\n",
      "         [2.1242, 1.8145, 1.7494,  ..., 1.9188, 1.8633, 2.0695],\n",
      "         ...,\n",
      "         [2.1540, 2.0967, 1.7897,  ..., 2.0216, 1.8898, 1.9308],\n",
      "         [2.1456, 1.9482, 1.8593,  ..., 2.1927, 1.9244, 2.0436],\n",
      "         [2.3065, 2.1697, 2.1093,  ..., 2.2724, 2.1758, 2.1483]],\n",
      "\n",
      "        [[1.9874, 1.7752, 1.9164,  ..., 1.7862, 1.8841, 1.7067],\n",
      "         [2.2715, 1.9972, 2.1688,  ..., 2.1908, 2.1675, 2.0316],\n",
      "         [2.3539, 2.2309, 2.2154,  ..., 2.1325, 2.2402, 2.1354],\n",
      "         ...,\n",
      "         [2.1628, 1.9478, 2.0772,  ..., 1.6707, 1.8986, 1.6331],\n",
      "         [2.2330, 1.7829, 1.9299,  ..., 1.6426, 1.9024, 1.8252],\n",
      "         [1.9050, 1.7112, 1.8274,  ..., 1.8333, 1.9510, 1.7733]],\n",
      "\n",
      "        [[2.0994, 1.8172, 2.2846,  ..., 1.8835, 2.1006, 2.0326],\n",
      "         [1.6360, 1.4752, 1.7671,  ..., 1.7116, 1.7835, 1.7659],\n",
      "         [2.0932, 2.0541, 2.3704,  ..., 2.1069, 2.2746, 2.1997],\n",
      "         ...,\n",
      "         [1.8542, 1.7645, 2.2138,  ..., 2.0077, 2.0925, 2.0758],\n",
      "         [2.2680, 1.9476, 2.3817,  ..., 1.8987, 2.1439, 2.2218],\n",
      "         [1.7802, 1.6819, 2.1766,  ..., 1.7751, 1.8632, 2.0145]]],\n",
      "       device='cuda:0'))\n",
      "attention_dist: (torch.Size([16, 512, 512]), tensor([[[0.0025, 0.0018, 0.0018,  ..., 0.0022, 0.0016, 0.0018],\n",
      "         [0.0022, 0.0019, 0.0018,  ..., 0.0022, 0.0018, 0.0018],\n",
      "         [0.0027, 0.0018, 0.0018,  ..., 0.0025, 0.0019, 0.0019],\n",
      "         ...,\n",
      "         [0.0025, 0.0018, 0.0019,  ..., 0.0023, 0.0017, 0.0018],\n",
      "         [0.0028, 0.0019, 0.0018,  ..., 0.0025, 0.0018, 0.0019],\n",
      "         [0.0027, 0.0016, 0.0017,  ..., 0.0024, 0.0019, 0.0018]],\n",
      "\n",
      "        [[0.0018, 0.0021, 0.0025,  ..., 0.0021, 0.0020, 0.0022],\n",
      "         [0.0021, 0.0015, 0.0021,  ..., 0.0019, 0.0017, 0.0019],\n",
      "         [0.0017, 0.0019, 0.0020,  ..., 0.0021, 0.0020, 0.0018],\n",
      "         ...,\n",
      "         [0.0016, 0.0018, 0.0021,  ..., 0.0019, 0.0017, 0.0021],\n",
      "         [0.0018, 0.0017, 0.0018,  ..., 0.0019, 0.0021, 0.0018],\n",
      "         [0.0018, 0.0018, 0.0020,  ..., 0.0020, 0.0019, 0.0017]],\n",
      "\n",
      "        [[0.0017, 0.0015, 0.0018,  ..., 0.0025, 0.0017, 0.0020],\n",
      "         [0.0015, 0.0019, 0.0020,  ..., 0.0020, 0.0018, 0.0019],\n",
      "         [0.0017, 0.0017, 0.0019,  ..., 0.0020, 0.0021, 0.0018],\n",
      "         ...,\n",
      "         [0.0015, 0.0018, 0.0021,  ..., 0.0018, 0.0019, 0.0020],\n",
      "         [0.0016, 0.0017, 0.0018,  ..., 0.0019, 0.0020, 0.0022],\n",
      "         [0.0015, 0.0021, 0.0019,  ..., 0.0021, 0.0019, 0.0019]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0024, 0.0017, 0.0017,  ..., 0.0023, 0.0021, 0.0019],\n",
      "         [0.0023, 0.0020, 0.0017,  ..., 0.0019, 0.0019, 0.0020],\n",
      "         [0.0023, 0.0017, 0.0016,  ..., 0.0019, 0.0018, 0.0022],\n",
      "         ...,\n",
      "         [0.0025, 0.0023, 0.0017,  ..., 0.0022, 0.0019, 0.0020],\n",
      "         [0.0022, 0.0018, 0.0017,  ..., 0.0024, 0.0018, 0.0020],\n",
      "         [0.0022, 0.0019, 0.0018,  ..., 0.0022, 0.0020, 0.0019]],\n",
      "\n",
      "        [[0.0023, 0.0019, 0.0022,  ..., 0.0019, 0.0021, 0.0018],\n",
      "         [0.0022, 0.0017, 0.0020,  ..., 0.0021, 0.0020, 0.0018],\n",
      "         [0.0022, 0.0020, 0.0019,  ..., 0.0018, 0.0020, 0.0018],\n",
      "         ...,\n",
      "         [0.0025, 0.0020, 0.0023,  ..., 0.0016, 0.0019, 0.0015],\n",
      "         [0.0028, 0.0018, 0.0021,  ..., 0.0016, 0.0020, 0.0019],\n",
      "         [0.0022, 0.0018, 0.0020,  ..., 0.0021, 0.0023, 0.0019]],\n",
      "\n",
      "        [[0.0020, 0.0015, 0.0025,  ..., 0.0016, 0.0020, 0.0019],\n",
      "         [0.0018, 0.0015, 0.0020,  ..., 0.0019, 0.0020, 0.0020],\n",
      "         [0.0018, 0.0017, 0.0023,  ..., 0.0018, 0.0021, 0.0020],\n",
      "         ...,\n",
      "         [0.0017, 0.0015, 0.0024,  ..., 0.0020, 0.0021, 0.0021],\n",
      "         [0.0021, 0.0015, 0.0023,  ..., 0.0014, 0.0018, 0.0020],\n",
      "         [0.0017, 0.0016, 0.0026,  ..., 0.0017, 0.0019, 0.0022]]],\n",
      "       device='cuda:0'))\n",
      "attention_matrix: (torch.Size([16, 512, 64]), tensor([[[0.4986, 0.4749, 0.5258,  ..., 0.4875, 0.4941, 0.4923],\n",
      "         [0.5013, 0.4736, 0.5254,  ..., 0.4858, 0.4922, 0.4930],\n",
      "         [0.4993, 0.4725, 0.5255,  ..., 0.4878, 0.4929, 0.4912],\n",
      "         ...,\n",
      "         [0.5013, 0.4737, 0.5265,  ..., 0.4860, 0.4925, 0.4936],\n",
      "         [0.4993, 0.4731, 0.5276,  ..., 0.4871, 0.4933, 0.4920],\n",
      "         [0.5002, 0.4739, 0.5280,  ..., 0.4854, 0.4919, 0.4936]],\n",
      "\n",
      "        [[0.4787, 0.4934, 0.4863,  ..., 0.5363, 0.5113, 0.5016],\n",
      "         [0.4778, 0.4946, 0.4895,  ..., 0.5360, 0.5119, 0.4984],\n",
      "         [0.4804, 0.4934, 0.4877,  ..., 0.5375, 0.5112, 0.5005],\n",
      "         ...,\n",
      "         [0.4778, 0.4952, 0.4894,  ..., 0.5367, 0.5113, 0.5022],\n",
      "         [0.4786, 0.4969, 0.4885,  ..., 0.5356, 0.5117, 0.4987],\n",
      "         [0.4773, 0.4941, 0.4902,  ..., 0.5366, 0.5121, 0.5006]],\n",
      "\n",
      "        [[0.4974, 0.4972, 0.5242,  ..., 0.4868, 0.5006, 0.5066],\n",
      "         [0.4955, 0.4979, 0.5220,  ..., 0.4868, 0.5007, 0.5068],\n",
      "         [0.4976, 0.4988, 0.5227,  ..., 0.4857, 0.5022, 0.5066],\n",
      "         ...,\n",
      "         [0.4968, 0.4970, 0.5232,  ..., 0.4851, 0.5013, 0.5092],\n",
      "         [0.4966, 0.4973, 0.5221,  ..., 0.4848, 0.5034, 0.5093],\n",
      "         [0.4958, 0.4972, 0.5214,  ..., 0.4844, 0.5002, 0.5084]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.4810, 0.4879, 0.5004,  ..., 0.4964, 0.5019, 0.4678],\n",
      "         [0.4837, 0.4890, 0.5022,  ..., 0.4957, 0.5019, 0.4682],\n",
      "         [0.4815, 0.4886, 0.5036,  ..., 0.4946, 0.5009, 0.4672],\n",
      "         ...,\n",
      "         [0.4830, 0.4866, 0.5015,  ..., 0.4963, 0.5027, 0.4685],\n",
      "         [0.4828, 0.4889, 0.5031,  ..., 0.4966, 0.5024, 0.4698],\n",
      "         [0.4832, 0.4893, 0.5037,  ..., 0.4950, 0.5018, 0.4680]],\n",
      "\n",
      "        [[0.4819, 0.5021, 0.4767,  ..., 0.4987, 0.5085, 0.4867],\n",
      "         [0.4818, 0.5016, 0.4751,  ..., 0.4970, 0.5092, 0.4886],\n",
      "         [0.4816, 0.5015, 0.4768,  ..., 0.4984, 0.5082, 0.4884],\n",
      "         ...,\n",
      "         [0.4833, 0.5013, 0.4760,  ..., 0.4992, 0.5067, 0.4862],\n",
      "         [0.4825, 0.5000, 0.4762,  ..., 0.5003, 0.5084, 0.4862],\n",
      "         [0.4821, 0.5015, 0.4778,  ..., 0.4993, 0.5079, 0.4866]],\n",
      "\n",
      "        [[0.5162, 0.5197, 0.5219,  ..., 0.4929, 0.5021, 0.5202],\n",
      "         [0.5161, 0.5188, 0.5201,  ..., 0.4909, 0.5040, 0.5208],\n",
      "         [0.5139, 0.5205, 0.5245,  ..., 0.4926, 0.5036, 0.5205],\n",
      "         ...,\n",
      "         [0.5155, 0.5200, 0.5215,  ..., 0.4936, 0.5045, 0.5196],\n",
      "         [0.5157, 0.5191, 0.5222,  ..., 0.4920, 0.5034, 0.5219],\n",
      "         [0.5160, 0.5188, 0.5219,  ..., 0.4949, 0.5040, 0.5205]]],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Comparing with pure self-attention\n",
    "\"\"\"\n",
    "\n",
    "attn_matrix = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(dim_head))\n",
    "print(f\"attn_matrix: {attn_matrix.shape, attn_matrix}\")\n",
    "\n",
    "attention_dist = F.softmax(attn_matrix, dim=-1)\n",
    "print(f\"attention_dist: {attention_dist.shape, attention_dist}\")\n",
    "\n",
    "attention_matrix = torch.matmul(attention_dist, value)\n",
    "print(f\"attention_matrix: {attention_matrix.shape, attention_matrix}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T03:52:11.832583Z",
     "start_time": "2024-03-04T03:52:11.804130Z"
    }
   },
   "id": "5b0242ad3d65be20",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.6304, device='cuda:0')"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Comparing with pure self-attention by KL-divergence\n",
    "배치 별, 평균 총합 0.5 ~ 2 정도 차이남, 이게 보니까 처음에 랜덤 초기화 빨로 갈리네\n",
    "\"\"\"\n",
    "\n",
    "kl_div = F.kl_div(linear_attn_matrix.log(), attention_matrix, reduction='batchmean')\n",
    "kl_div"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T03:52:13.025748Z",
     "start_time": "2024-03-04T03:52:13.019657Z"
    }
   },
   "id": "2cdaaf0b6c2d4400",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.0264, -0.0698, -1.3971, -1.5667],\n         [ 0.5706, -1.4718,  0.3398, -0.5280],\n         [ 1.5465,  0.2453,  1.0911,  0.6062],\n         [ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n\n        [[ 0.4773,  1.9033,  0.6571,  0.4388],\n         [ 0.1047, -0.3162, -3.4738, -0.4424],\n         [ 0.4799,  0.7670, -1.0056, -0.4248],\n         [ 1.2279, -0.7639,  1.4043, -0.2604],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n\n        [[-0.1115,  0.3349, -1.0625,  0.5592],\n         [ 0.3361,  1.7460,  1.9226,  0.5757],\n         [ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]], device='cuda:0')"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Test code for applying padding masking to linear attention \"\"\"\n",
    "test_q = torch.randn(3, 5, 4, device=device)\n",
    "test_k = torch.randn(3, 5, 4, device=device)\n",
    "test_v = torch.randn(3, 5, 4, device=device)\n",
    "\n",
    "padding_mask = torch.tensor([\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 1, 1, 1]],\n",
    "    device=device\n",
    ")\n",
    "test_k[padding_mask == 1] = 0\n",
    "test_k"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T04:00:33.032590Z",
     "start_time": "2024-03-04T04:00:33.027595Z"
    }
   },
   "id": "26644eb6ded2d1c8",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, dim_model: int= 768):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.i_arr = torch.arange(1, int(dim_model/2)+1)  # 세타값을 살리려면 \n",
    "        self.theta = 10000**(-2*(self.i_arr - 1)/self.dim_model)\n",
    "    \n",
    "    def forward(self):\n",
    "        print(self.i_arr.shape)\n",
    "        print(self.i_arr)\n",
    "        print(self.theta.shape)\n",
    "        print(self.theta)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:49.209569Z",
     "start_time": "2024-03-02T13:30:49.203796Z"
    }
   },
   "id": "97c1f10595f4412f",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384])\n",
      "tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
      "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
      "         29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
      "         43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
      "         57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
      "         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
      "         85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
      "         99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
      "        113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
      "        127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
      "        141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
      "        155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
      "        183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
      "        197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210,\n",
      "        211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
      "        225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
      "        239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,\n",
      "        253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
      "        267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
      "        281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294,\n",
      "        295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
      "        309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
      "        323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
      "        337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
      "        365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
      "        379, 380, 381, 382, 383, 384])\n",
      "torch.Size([384])\n",
      "tensor([1.0000e+00, 9.7630e-01, 9.5316e-01, 9.3057e-01, 9.0852e-01, 8.8699e-01,\n",
      "        8.6596e-01, 8.4544e-01, 8.2540e-01, 8.0584e-01, 7.8674e-01, 7.6810e-01,\n",
      "        7.4989e-01, 7.3212e-01, 7.1477e-01, 6.9783e-01, 6.8129e-01, 6.6515e-01,\n",
      "        6.4938e-01, 6.3399e-01, 6.1897e-01, 6.0430e-01, 5.8997e-01, 5.7599e-01,\n",
      "        5.6234e-01, 5.4901e-01, 5.3600e-01, 5.2330e-01, 5.1090e-01, 4.9879e-01,\n",
      "        4.8697e-01, 4.7543e-01, 4.6416e-01, 4.5316e-01, 4.4242e-01, 4.3193e-01,\n",
      "        4.2170e-01, 4.1170e-01, 4.0195e-01, 3.9242e-01, 3.8312e-01, 3.7404e-01,\n",
      "        3.6517e-01, 3.5652e-01, 3.4807e-01, 3.3982e-01, 3.3177e-01, 3.2390e-01,\n",
      "        3.1623e-01, 3.0873e-01, 3.0142e-01, 2.9427e-01, 2.8730e-01, 2.8049e-01,\n",
      "        2.7384e-01, 2.6735e-01, 2.6102e-01, 2.5483e-01, 2.4879e-01, 2.4289e-01,\n",
      "        2.3714e-01, 2.3152e-01, 2.2603e-01, 2.2067e-01, 2.1544e-01, 2.1034e-01,\n",
      "        2.0535e-01, 2.0049e-01, 1.9573e-01, 1.9110e-01, 1.8657e-01, 1.8214e-01,\n",
      "        1.7783e-01, 1.7361e-01, 1.6950e-01, 1.6548e-01, 1.6156e-01, 1.5773e-01,\n",
      "        1.5399e-01, 1.5034e-01, 1.4678e-01, 1.4330e-01, 1.3991e-01, 1.3659e-01,\n",
      "        1.3335e-01, 1.3019e-01, 1.2711e-01, 1.2409e-01, 1.2115e-01, 1.1828e-01,\n",
      "        1.1548e-01, 1.1274e-01, 1.1007e-01, 1.0746e-01, 1.0491e-01, 1.0243e-01,\n",
      "        1.0000e-01, 9.7630e-02, 9.5316e-02, 9.3057e-02, 9.0852e-02, 8.8699e-02,\n",
      "        8.6596e-02, 8.4544e-02, 8.2540e-02, 8.0584e-02, 7.8674e-02, 7.6810e-02,\n",
      "        7.4989e-02, 7.3212e-02, 7.1477e-02, 6.9783e-02, 6.8129e-02, 6.6515e-02,\n",
      "        6.4938e-02, 6.3399e-02, 6.1897e-02, 6.0430e-02, 5.8997e-02, 5.7599e-02,\n",
      "        5.6234e-02, 5.4901e-02, 5.3600e-02, 5.2330e-02, 5.1090e-02, 4.9879e-02,\n",
      "        4.8697e-02, 4.7543e-02, 4.6416e-02, 4.5316e-02, 4.4242e-02, 4.3193e-02,\n",
      "        4.2170e-02, 4.1170e-02, 4.0195e-02, 3.9242e-02, 3.8312e-02, 3.7404e-02,\n",
      "        3.6517e-02, 3.5652e-02, 3.4807e-02, 3.3982e-02, 3.3177e-02, 3.2390e-02,\n",
      "        3.1623e-02, 3.0873e-02, 3.0142e-02, 2.9427e-02, 2.8730e-02, 2.8049e-02,\n",
      "        2.7384e-02, 2.6735e-02, 2.6102e-02, 2.5483e-02, 2.4879e-02, 2.4289e-02,\n",
      "        2.3714e-02, 2.3152e-02, 2.2603e-02, 2.2067e-02, 2.1544e-02, 2.1034e-02,\n",
      "        2.0535e-02, 2.0049e-02, 1.9573e-02, 1.9110e-02, 1.8657e-02, 1.8214e-02,\n",
      "        1.7783e-02, 1.7361e-02, 1.6950e-02, 1.6548e-02, 1.6156e-02, 1.5773e-02,\n",
      "        1.5399e-02, 1.5034e-02, 1.4678e-02, 1.4330e-02, 1.3991e-02, 1.3659e-02,\n",
      "        1.3335e-02, 1.3019e-02, 1.2711e-02, 1.2409e-02, 1.2115e-02, 1.1828e-02,\n",
      "        1.1548e-02, 1.1274e-02, 1.1007e-02, 1.0746e-02, 1.0491e-02, 1.0243e-02,\n",
      "        1.0000e-02, 9.7630e-03, 9.5316e-03, 9.3057e-03, 9.0852e-03, 8.8699e-03,\n",
      "        8.6596e-03, 8.4544e-03, 8.2540e-03, 8.0584e-03, 7.8674e-03, 7.6810e-03,\n",
      "        7.4989e-03, 7.3212e-03, 7.1477e-03, 6.9783e-03, 6.8129e-03, 6.6515e-03,\n",
      "        6.4938e-03, 6.3399e-03, 6.1897e-03, 6.0430e-03, 5.8997e-03, 5.7599e-03,\n",
      "        5.6234e-03, 5.4901e-03, 5.3600e-03, 5.2330e-03, 5.1090e-03, 4.9879e-03,\n",
      "        4.8697e-03, 4.7543e-03, 4.6416e-03, 4.5316e-03, 4.4242e-03, 4.3193e-03,\n",
      "        4.2170e-03, 4.1170e-03, 4.0195e-03, 3.9242e-03, 3.8312e-03, 3.7404e-03,\n",
      "        3.6517e-03, 3.5652e-03, 3.4807e-03, 3.3982e-03, 3.3177e-03, 3.2390e-03,\n",
      "        3.1623e-03, 3.0873e-03, 3.0142e-03, 2.9427e-03, 2.8730e-03, 2.8049e-03,\n",
      "        2.7384e-03, 2.6735e-03, 2.6102e-03, 2.5483e-03, 2.4879e-03, 2.4289e-03,\n",
      "        2.3714e-03, 2.3152e-03, 2.2603e-03, 2.2067e-03, 2.1544e-03, 2.1034e-03,\n",
      "        2.0535e-03, 2.0049e-03, 1.9573e-03, 1.9110e-03, 1.8657e-03, 1.8214e-03,\n",
      "        1.7783e-03, 1.7361e-03, 1.6950e-03, 1.6548e-03, 1.6156e-03, 1.5773e-03,\n",
      "        1.5399e-03, 1.5034e-03, 1.4678e-03, 1.4330e-03, 1.3991e-03, 1.3659e-03,\n",
      "        1.3335e-03, 1.3019e-03, 1.2711e-03, 1.2409e-03, 1.2115e-03, 1.1828e-03,\n",
      "        1.1548e-03, 1.1274e-03, 1.1007e-03, 1.0746e-03, 1.0491e-03, 1.0243e-03,\n",
      "        1.0000e-03, 9.7630e-04, 9.5316e-04, 9.3057e-04, 9.0852e-04, 8.8699e-04,\n",
      "        8.6596e-04, 8.4544e-04, 8.2540e-04, 8.0584e-04, 7.8674e-04, 7.6810e-04,\n",
      "        7.4989e-04, 7.3212e-04, 7.1477e-04, 6.9783e-04, 6.8129e-04, 6.6515e-04,\n",
      "        6.4938e-04, 6.3399e-04, 6.1897e-04, 6.0430e-04, 5.8997e-04, 5.7599e-04,\n",
      "        5.6234e-04, 5.4901e-04, 5.3600e-04, 5.2330e-04, 5.1090e-04, 4.9879e-04,\n",
      "        4.8697e-04, 4.7543e-04, 4.6416e-04, 4.5316e-04, 4.4242e-04, 4.3193e-04,\n",
      "        4.2170e-04, 4.1170e-04, 4.0195e-04, 3.9242e-04, 3.8312e-04, 3.7404e-04,\n",
      "        3.6517e-04, 3.5652e-04, 3.4807e-04, 3.3982e-04, 3.3177e-04, 3.2390e-04,\n",
      "        3.1623e-04, 3.0873e-04, 3.0142e-04, 2.9427e-04, 2.8730e-04, 2.8049e-04,\n",
      "        2.7384e-04, 2.6735e-04, 2.6102e-04, 2.5483e-04, 2.4879e-04, 2.4289e-04,\n",
      "        2.3714e-04, 2.3152e-04, 2.2603e-04, 2.2067e-04, 2.1544e-04, 2.1034e-04,\n",
      "        2.0535e-04, 2.0049e-04, 1.9573e-04, 1.9110e-04, 1.8657e-04, 1.8214e-04,\n",
      "        1.7783e-04, 1.7361e-04, 1.6950e-04, 1.6548e-04, 1.6156e-04, 1.5773e-04,\n",
      "        1.5399e-04, 1.5034e-04, 1.4678e-04, 1.4330e-04, 1.3991e-04, 1.3659e-04,\n",
      "        1.3335e-04, 1.3019e-04, 1.2711e-04, 1.2409e-04, 1.2115e-04, 1.1828e-04,\n",
      "        1.1548e-04, 1.1274e-04, 1.1007e-04, 1.0746e-04, 1.0491e-04, 1.0243e-04])\n"
     ]
    }
   ],
   "source": [
    "test = RoPE()\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:49.213790Z",
     "start_time": "2024-03-02T13:30:49.206814Z"
    }
   },
   "id": "938bd92df37c8366",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:49.227687Z",
     "start_time": "2024-03-02T13:30:49.213511Z"
    }
   },
   "id": "e598d761ff00999a",
   "execution_count": 93
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
