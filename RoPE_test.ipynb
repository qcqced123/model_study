{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import experiment.metrics.metric as metric\n",
    "from experiment.metrics.metric import cosine_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T16:10:02.697876Z",
     "start_time": "2024-03-03T16:09:59.819765Z"
    }
   },
   "id": "f1b867fbf9578cf4",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='mps')"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" set default device to mps \"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T16:10:02.703311Z",
     "start_time": "2024-03-03T16:10:02.699062Z"
    }
   },
   "id": "eedaeeb4c236e989",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(9663676416, 1207959552, 8)"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadratic = 512*64*64*512 + 512*512*512*64\n",
    "linear = 64*512*512*64 + 512*64*64*64\n",
    "\n",
    "quadratic, linear, quadratic // linear"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:48.714062Z",
     "start_time": "2024-03-02T13:30:48.709081Z"
    }
   },
   "id": "5c0366bb014d387d",
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([16, 512, 64]),\n torch.Size([16, 512, 64]),\n torch.Size([16, 512, 64]))"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Inputs for Linear Attentions\n",
    "\"\"\"\n",
    "\n",
    "max_seq = 512\n",
    "dim_model = 768\n",
    "num_heads = 12\n",
    "batch_size = 16\n",
    "dim_head = dim_model // num_heads\n",
    "\n",
    "def kernel_fn(x: Tensor):\n",
    "    return F.elu(x) + 1\n",
    "\n",
    "query = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "key = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "value = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "\n",
    "query.shape, key.shape, value.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T16:30:27.068786Z",
     "start_time": "2024-03-03T16:30:27.062843Z"
    }
   },
   "id": "2db69b58bd5032a",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[387.7043, 371.8534, 393.9587,  ..., 366.6582, 389.5907, 388.7321],\n          [384.7640, 366.5524, 393.6349,  ..., 363.6440, 385.3273, 391.2770],\n          [382.3161, 372.1201, 392.7029,  ..., 365.1796, 387.1248, 390.4623],\n          ...,\n          [387.3051, 374.5810, 395.6080,  ..., 364.7379, 387.8368, 391.8069],\n          [383.2746, 371.5667, 395.2921,  ..., 364.0172, 388.0489, 391.9443],\n          [390.4175, 375.6101, 397.2274,  ..., 368.6044, 385.3950, 394.7017]],\n \n         [[382.1408, 385.7607, 388.2805,  ..., 392.3750, 388.4670, 380.8877],\n          [382.6382, 385.0953, 383.5913,  ..., 387.5719, 377.2379, 377.3565],\n          [384.5132, 390.9588, 386.0755,  ..., 384.6969, 380.5969, 381.2653],\n          ...,\n          [382.7216, 385.5114, 383.3484,  ..., 386.4922, 382.5864, 382.0888],\n          [378.1702, 378.6817, 382.9681,  ..., 380.4366, 378.6287, 373.3374],\n          [383.8071, 385.2278, 390.3502,  ..., 387.0571, 387.7598, 385.2114]],\n \n         [[378.5798, 382.2704, 372.5133,  ..., 397.2052, 395.6889, 384.5559],\n          [384.8563, 388.7112, 371.9864,  ..., 397.0045, 403.3707, 385.7776],\n          [384.3153, 389.2483, 375.6970,  ..., 397.2532, 401.3463, 382.9750],\n          ...,\n          [385.6611, 391.2040, 373.7429,  ..., 399.2679, 399.8356, 387.8416],\n          [390.3621, 389.2191, 377.3676,  ..., 401.7670, 404.8122, 387.7436],\n          [383.5791, 385.2593, 371.8275,  ..., 394.8017, 395.8604, 380.9172]],\n \n         ...,\n \n         [[381.8162, 393.2561, 393.5131,  ..., 376.6374, 376.9135, 392.0193],\n          [382.0338, 398.2310, 394.7419,  ..., 379.2096, 382.4135, 395.8701],\n          [376.4683, 384.9442, 387.6196,  ..., 370.9770, 371.9736, 388.4149],\n          ...,\n          [384.3445, 390.2544, 394.1010,  ..., 377.0582, 374.3900, 387.8101],\n          [375.4850, 385.1070, 386.9338,  ..., 364.8930, 374.9947, 387.2665],\n          [382.3772, 389.6734, 393.9269,  ..., 374.5025, 378.8535, 390.9736]],\n \n         [[380.9454, 400.9701, 387.8334,  ..., 384.4292, 385.6967, 377.4725],\n          [372.4789, 390.1824, 379.8209,  ..., 380.0336, 377.0626, 366.7782],\n          [379.2007, 399.2089, 384.3382,  ..., 387.4645, 385.4660, 376.1935],\n          ...,\n          [370.3233, 390.2021, 375.9564,  ..., 373.9791, 372.8083, 371.3492],\n          [372.5183, 398.4876, 382.3155,  ..., 382.4989, 380.8230, 375.2141],\n          [373.7726, 397.8548, 379.4508,  ..., 380.6801, 381.1468, 375.0591]],\n \n         [[372.8792, 400.9789, 385.2145,  ..., 376.8839, 383.1749, 382.3750],\n          [372.7479, 401.3697, 387.2713,  ..., 377.0909, 387.9886, 385.7196],\n          [379.9518, 404.8532, 396.5909,  ..., 380.3535, 389.6241, 390.5255],\n          ...,\n          [376.6323, 403.8600, 392.7627,  ..., 380.3998, 391.8700, 389.6887],\n          [380.9880, 404.3499, 393.4289,  ..., 383.8060, 392.4081, 387.4787],\n          [367.4914, 396.6023, 385.3049,  ..., 372.4923, 384.7232, 382.1786]]],\n        device='mps:0'),\n torch.Size([16, 64, 64]))"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: KV Matrix\n",
    "\"\"\"\n",
    "k_query, k_key = kernel_fn(query), kernel_fn(key)\n",
    "KV = torch.matmul(k_key.permute(0, 2, 1), value)\n",
    "KV, KV.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T16:30:27.947394Z",
     "start_time": "2024-03-03T16:30:27.890734Z"
    }
   },
   "id": "1d4d4a249fb2cf68",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[37280.8555, 36106.5078, 38063.3203,  ..., 35232.7578,\n           37621.9023, 37787.2266],\n          [35636.3633, 34503.3555, 36376.5078,  ..., 33671.8125,\n           35951.5469, 36108.4570],\n          [37547.0391, 36366.0469, 38342.8047,  ..., 35478.3047,\n           37879.2656, 38039.2852],\n          ...,\n          [36239.8320, 35104.4883, 37016.8203,  ..., 34247.1875,\n           36575.5156, 36744.7695],\n          [35854.8789, 34725.8242, 36609.7891,  ..., 33877.2266,\n           36178.7227, 36333.9648],\n          [36674.7148, 35525.2969, 37465.9922,  ..., 34655.2773,\n           37013.2930, 37181.4023]],\n \n         [[37053.4492, 37189.0391, 37369.2539,  ..., 37365.4492,\n           37059.3750, 36788.0547],\n          [36076.2773, 36206.7773, 36375.6758,  ..., 36369.5977,\n           36080.3594, 35822.3477],\n          [37330.4180, 37472.3672, 37639.2539,  ..., 37643.1602,\n           37354.6016, 37074.9961],\n          ...,\n          [35750.8477, 35888.6289, 36049.4062,  ..., 36049.5781,\n           35760.1367, 35494.4922],\n          [37251.3828, 37403.0352, 37568.8711,  ..., 37565.0977,\n           37262.1133, 36996.7227],\n          [37950.3047, 38094.0312, 38264.8164,  ..., 38266.3672,\n           37960.9414, 37690.5781]],\n \n         [[36837.8359, 37161.6250, 35934.1055,  ..., 38102.3867,\n           38282.4805, 36863.4219],\n          [35404.3906, 35717.7070, 34543.2812,  ..., 36621.3125,\n           36799.1953, 35428.4102],\n          [36819.8867, 37156.8867, 35919.3477,  ..., 38084.4805,\n           38274.4336, 36843.1289],\n          ...,\n          [37697.2734, 38031.7422, 36777.1992,  ..., 38998.2695,\n           39181.0391, 37722.4688],\n          [37860.2656, 38202.0195, 36935.3242,  ..., 39165.8594,\n           39349.3711, 37883.4609],\n          [35181.1055, 35494.3164, 34321.6992,  ..., 36400.0312,\n           36575.6641, 35220.6875]],\n \n         ...,\n \n         [[36350.5938, 37313.2266, 37440.7734,  ..., 35802.3555,\n           35962.0625, 37299.3906],\n          [36068.9375, 37001.8477, 37146.0820,  ..., 35513.8320,\n           35682.4922, 37007.6875],\n          [35011.9219, 35911.8164, 36060.5508,  ..., 34471.3164,\n           34636.9414, 35926.4727],\n          ...,\n          [36199.4570, 37147.3945, 37294.6055,  ..., 35656.5234,\n           35821.8242, 37146.0664],\n          [35711.5156, 36641.0156, 36780.5352,  ..., 35164.4648,\n           35328.1836, 36639.0781],\n          [35768.0352, 36710.5156, 36847.2422,  ..., 35217.3867,\n           35385.5273, 36697.3789]],\n \n         [[35989.6602, 38099.6680, 36876.8008,  ..., 36709.8320,\n           36563.1953, 35975.1406],\n          [34846.7344, 36892.1719, 35697.6133,  ..., 35551.1680,\n           35405.6797, 34836.3828],\n          [37095.8203, 39272.3242, 38013.0508,  ..., 37847.9570,\n           37685.1016, 37076.4570],\n          ...,\n          [35847.0742, 37950.7891, 36724.0430,  ..., 36577.6172,\n           36419.9180, 35834.4375],\n          [35777.5703, 37856.6016, 36641.3008,  ..., 36478.2500,\n           36338.2070, 35757.1602],\n          [35172.8594, 37224.9062, 36023.4844,  ..., 35875.3633,\n           35719.8633, 35144.9766]],\n \n         [[38492.2891, 41222.8633, 40003.6719,  ..., 38791.9961,\n           39957.4297, 39473.8438],\n          [36301.8633, 38877.8633, 37737.3828,  ..., 36587.8672,\n           37673.1719, 37238.9453],\n          [37540.7695, 40204.4180, 39016.3398,  ..., 37830.3164,\n           38966.9336, 38497.1992],\n          ...,\n          [36784.3984, 39383.4648, 38232.5898,  ..., 37069.3906,\n           38172.2695, 37723.3398],\n          [37159.6328, 39780.8984, 38612.8008,  ..., 37440.3438,\n           38555.0156, 38099.0547],\n          [35777.1484, 38302.6133, 37170.6562,  ..., 36039.3320,\n           37117.0859, 36677.9961]]], device='mps:0'),\n torch.Size([16, 512, 64]))"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: QKV\n",
    "\"\"\"\n",
    "\n",
    "QKV = torch.matmul(k_query, KV)\n",
    "QKV, QKV.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T16:30:28.007014Z",
     "start_time": "2024-03-03T16:30:27.948190Z"
    }
   },
   "id": "56d258a4ac1b2e78",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key, summation_key: (torch.Size([16, 512, 64]), torch.Size([16, 64, 64]))\n",
      "Normalizer Z: (tensor([[[1.3309e-05, 1.3452e-05, 1.3403e-05,  ..., 1.3358e-05,\n",
      "          1.3345e-05, 1.3336e-05],\n",
      "         [1.3928e-05, 1.4077e-05, 1.4026e-05,  ..., 1.3980e-05,\n",
      "          1.3966e-05, 1.3956e-05],\n",
      "         [1.3218e-05, 1.3360e-05, 1.3312e-05,  ..., 1.3267e-05,\n",
      "          1.3254e-05, 1.3245e-05],\n",
      "         ...,\n",
      "         [1.3690e-05, 1.3837e-05, 1.3787e-05,  ..., 1.3741e-05,\n",
      "          1.3728e-05, 1.3718e-05],\n",
      "         [1.3842e-05, 1.3990e-05, 1.3940e-05,  ..., 1.3893e-05,\n",
      "          1.3879e-05, 1.3870e-05],\n",
      "         [1.3528e-05, 1.3674e-05, 1.3624e-05,  ..., 1.3579e-05,\n",
      "          1.3565e-05, 1.3556e-05]],\n",
      "\n",
      "        [[1.3507e-05, 1.3617e-05, 1.3581e-05,  ..., 1.3526e-05,\n",
      "          1.3750e-05, 1.3504e-05],\n",
      "         [1.3873e-05, 1.3986e-05, 1.3950e-05,  ..., 1.3893e-05,\n",
      "          1.4123e-05, 1.3871e-05],\n",
      "         [1.3408e-05, 1.3518e-05, 1.3482e-05,  ..., 1.3427e-05,\n",
      "          1.3650e-05, 1.3406e-05],\n",
      "         ...,\n",
      "         [1.3996e-05, 1.4110e-05, 1.4074e-05,  ..., 1.4016e-05,\n",
      "          1.4248e-05, 1.3994e-05],\n",
      "         [1.3438e-05, 1.3548e-05, 1.3512e-05,  ..., 1.3458e-05,\n",
      "          1.3680e-05, 1.3436e-05],\n",
      "         [1.3189e-05, 1.3297e-05, 1.3262e-05,  ..., 1.3208e-05,\n",
      "          1.3427e-05, 1.3187e-05]],\n",
      "\n",
      "        [[1.3560e-05, 1.3462e-05, 1.3429e-05,  ..., 1.3466e-05,\n",
      "          1.3376e-05, 1.3579e-05],\n",
      "         [1.4112e-05, 1.4010e-05, 1.3976e-05,  ..., 1.4014e-05,\n",
      "          1.3920e-05, 1.4132e-05],\n",
      "         [1.3564e-05, 1.3466e-05, 1.3433e-05,  ..., 1.3470e-05,\n",
      "          1.3380e-05, 1.3583e-05],\n",
      "         ...,\n",
      "         [1.3249e-05, 1.3154e-05, 1.3122e-05,  ..., 1.3157e-05,\n",
      "          1.3069e-05, 1.3268e-05],\n",
      "         [1.3189e-05, 1.3094e-05, 1.3062e-05,  ..., 1.3098e-05,\n",
      "          1.3010e-05, 1.3208e-05],\n",
      "         [1.4200e-05, 1.4097e-05, 1.4063e-05,  ..., 1.4101e-05,\n",
      "          1.4007e-05, 1.4220e-05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.3653e-05, 1.3556e-05, 1.3842e-05,  ..., 1.3721e-05,\n",
      "          1.3891e-05, 1.3667e-05],\n",
      "         [1.3763e-05, 1.3665e-05, 1.3953e-05,  ..., 1.3831e-05,\n",
      "          1.4003e-05, 1.3777e-05],\n",
      "         [1.4175e-05, 1.4075e-05, 1.4371e-05,  ..., 1.4246e-05,\n",
      "          1.4422e-05, 1.4189e-05],\n",
      "         ...,\n",
      "         [1.3711e-05, 1.3614e-05, 1.3901e-05,  ..., 1.3780e-05,\n",
      "          1.3950e-05, 1.3725e-05],\n",
      "         [1.3900e-05, 1.3802e-05, 1.4093e-05,  ..., 1.3969e-05,\n",
      "          1.4142e-05, 1.3914e-05],\n",
      "         [1.3880e-05, 1.3782e-05, 1.4073e-05,  ..., 1.3950e-05,\n",
      "          1.4122e-05, 1.3894e-05]],\n",
      "\n",
      "        [[1.3378e-05, 1.3634e-05, 1.3393e-05,  ..., 1.3718e-05,\n",
      "          1.3479e-05, 1.3488e-05],\n",
      "         [1.3800e-05, 1.4064e-05, 1.3815e-05,  ..., 1.4151e-05,\n",
      "          1.3904e-05, 1.3913e-05],\n",
      "         [1.2975e-05, 1.3223e-05, 1.2989e-05,  ..., 1.3305e-05,\n",
      "          1.3073e-05, 1.3081e-05],\n",
      "         ...,\n",
      "         [1.3426e-05, 1.3683e-05, 1.3440e-05,  ..., 1.3767e-05,\n",
      "          1.3527e-05, 1.3536e-05],\n",
      "         [1.3457e-05, 1.3714e-05, 1.3471e-05,  ..., 1.3799e-05,\n",
      "          1.3558e-05, 1.3567e-05],\n",
      "         [1.3687e-05, 1.3949e-05, 1.3702e-05,  ..., 1.4035e-05,\n",
      "          1.3790e-05, 1.3799e-05]],\n",
      "\n",
      "        [[1.2863e-05, 1.2799e-05, 1.2696e-05,  ..., 1.2670e-05,\n",
      "          1.2605e-05, 1.2885e-05],\n",
      "         [1.3638e-05, 1.3569e-05, 1.3461e-05,  ..., 1.3433e-05,\n",
      "          1.3364e-05, 1.3661e-05],\n",
      "         [1.3193e-05, 1.3127e-05, 1.3022e-05,  ..., 1.2994e-05,\n",
      "          1.2928e-05, 1.3215e-05],\n",
      "         ...,\n",
      "         [1.3466e-05, 1.3399e-05, 1.3292e-05,  ..., 1.3264e-05,\n",
      "          1.3196e-05, 1.3489e-05],\n",
      "         [1.3333e-05, 1.3266e-05, 1.3160e-05,  ..., 1.3133e-05,\n",
      "          1.3066e-05, 1.3356e-05],\n",
      "         [1.3852e-05, 1.3782e-05, 1.3672e-05,  ..., 1.3643e-05,\n",
      "          1.3574e-05, 1.3875e-05]]], device='mps:0'), torch.Size([16, 512, 64]))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: QKV / normalizer Z\n",
    "softmax는 row-wise하게 정규화 하는데, 우리도 똑같이 정규화가 필요하지 않냐고 그래서 Z가 필요\n",
    "여기서, QKV가 결국 row-wise 하게 정규화 되어야 한다는게 포인트임\n",
    "그렇다면 Z의 크기는 16, 512, 64가 되어야 한다\n",
    "\"\"\"\n",
    "\n",
    "# summation_key = k_key.sum(dim=1).unsqueeze(1).expand(-1, dim_head, -1)\n",
    "summation_key = k_key.sum(dim=1).unsqueeze(1).expand(-1, 64, -1)\n",
    "\n",
    "print(f\"key, summation_key: {key.shape, summation_key.shape}\")\n",
    "\n",
    "Z = 1/torch.matmul(k_query, summation_key)\n",
    "print(f\"Normalizer Z: {Z, Z.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T16:31:41.075292Z",
     "start_time": "2024-03-03T16:31:40.980946Z"
    }
   },
   "id": "4b94b3decb9e4270",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_attn_matrix: (torch.Size([16, 512, 64]), tensor([[[0.4962, 0.4857, 0.5102,  ..., 0.4707, 0.5021, 0.5039],\n",
      "         [0.4963, 0.4857, 0.5102,  ..., 0.4707, 0.5021, 0.5039],\n",
      "         [0.4963, 0.4859, 0.5104,  ..., 0.4707, 0.5021, 0.5038],\n",
      "         ...,\n",
      "         [0.4961, 0.4858, 0.5104,  ..., 0.4706, 0.5021, 0.5041],\n",
      "         [0.4963, 0.4858, 0.5103,  ..., 0.4707, 0.5021, 0.5039],\n",
      "         [0.4962, 0.4858, 0.5104,  ..., 0.4706, 0.5021, 0.5040]],\n",
      "\n",
      "        [[0.5005, 0.5064, 0.5075,  ..., 0.5054, 0.5096, 0.4968],\n",
      "         [0.5005, 0.5064, 0.5074,  ..., 0.5053, 0.5096, 0.4969],\n",
      "         [0.5005, 0.5065, 0.5075,  ..., 0.5055, 0.5099, 0.4970],\n",
      "         ...,\n",
      "         [0.5004, 0.5064, 0.5073,  ..., 0.5053, 0.5095, 0.4967],\n",
      "         [0.5006, 0.5067, 0.5076,  ..., 0.5055, 0.5098, 0.4971],\n",
      "         [0.5005, 0.5065, 0.5075,  ..., 0.5054, 0.5097, 0.4970]],\n",
      "\n",
      "        [[0.4995, 0.5003, 0.4826,  ..., 0.5131, 0.5121, 0.5006],\n",
      "         [0.4996, 0.5004, 0.4828,  ..., 0.5132, 0.5123, 0.5007],\n",
      "         [0.4994, 0.5004, 0.4825,  ..., 0.5130, 0.5121, 0.5004],\n",
      "         ...,\n",
      "         [0.4995, 0.5003, 0.4826,  ..., 0.5131, 0.5121, 0.5005],\n",
      "         [0.4994, 0.5002, 0.4825,  ..., 0.5130, 0.5119, 0.5004],\n",
      "         [0.4996, 0.5004, 0.4827,  ..., 0.5133, 0.5123, 0.5008]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.4963, 0.5058, 0.5183,  ..., 0.4912, 0.4995, 0.5098],\n",
      "         [0.4964, 0.5056, 0.5183,  ..., 0.4912, 0.4996, 0.5098],\n",
      "         [0.4963, 0.5054, 0.5182,  ..., 0.4911, 0.4995, 0.5098],\n",
      "         ...,\n",
      "         [0.4963, 0.5057, 0.5184,  ..., 0.4913, 0.4997, 0.5098],\n",
      "         [0.4964, 0.5057, 0.5183,  ..., 0.4912, 0.4996, 0.5098],\n",
      "         [0.4965, 0.5059, 0.5185,  ..., 0.4913, 0.4997, 0.5099]],\n",
      "\n",
      "        [[0.4815, 0.5195, 0.4939,  ..., 0.5036, 0.4928, 0.4852],\n",
      "         [0.4809, 0.5189, 0.4932,  ..., 0.5031, 0.4923, 0.4847],\n",
      "         [0.4813, 0.5193, 0.4937,  ..., 0.5036, 0.4926, 0.4850],\n",
      "         ...,\n",
      "         [0.4813, 0.5193, 0.4936,  ..., 0.5036, 0.4926, 0.4850],\n",
      "         [0.4814, 0.5192, 0.4936,  ..., 0.5034, 0.4927, 0.4851],\n",
      "         [0.4814, 0.5192, 0.4936,  ..., 0.5035, 0.4926, 0.4850]],\n",
      "\n",
      "        [[0.4951, 0.5276, 0.5079,  ..., 0.4915, 0.5037, 0.5086],\n",
      "         [0.4951, 0.5275, 0.5080,  ..., 0.4915, 0.5035, 0.5087],\n",
      "         [0.4953, 0.5277, 0.5081,  ..., 0.4916, 0.5038, 0.5087],\n",
      "         ...,\n",
      "         [0.4953, 0.5277, 0.5082,  ..., 0.4917, 0.5037, 0.5089],\n",
      "         [0.4955, 0.5277, 0.5082,  ..., 0.4917, 0.5038, 0.5088],\n",
      "         [0.4956, 0.5279, 0.5082,  ..., 0.4917, 0.5038, 0.5089]]],\n",
      "       device='mps:0'))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: QKV / normalizer Z \n",
    "\"\"\"\n",
    "\n",
    "linear_attn_matrix = QKV * Z\n",
    "print(f\"linear_attn_matrix: {linear_attn_matrix.shape, linear_attn_matrix}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T16:31:42.398121Z",
     "start_time": "2024-03-03T16:31:42.324554Z"
    }
   },
   "id": "65459cc6481c7d45",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_matrix: (torch.Size([16, 512, 512]), tensor([[[2.1725, 1.7386, 2.0302,  ..., 1.8410, 2.2081, 2.2750],\n",
      "         [1.6531, 1.3445, 1.7683,  ..., 1.8096, 1.8506, 1.9382],\n",
      "         [1.9809, 1.7924, 2.1559,  ..., 2.0277, 2.0989, 2.2697],\n",
      "         ...,\n",
      "         [1.9047, 1.7059, 1.8541,  ..., 1.8300, 2.0202, 2.1081],\n",
      "         [1.9011, 1.5412, 1.6208,  ..., 1.7042, 1.9113, 1.9657],\n",
      "         [2.0518, 1.5056, 1.9919,  ..., 1.9880, 2.0154, 2.1064]],\n",
      "\n",
      "        [[2.1710, 2.0610, 2.3059,  ..., 1.8151, 1.9755, 1.9143],\n",
      "         [1.9178, 1.8013, 2.1319,  ..., 1.7848, 1.9060, 1.6572],\n",
      "         [2.2061, 2.0478, 2.4350,  ..., 2.0977, 2.0709, 1.8986],\n",
      "         ...,\n",
      "         [1.7587, 1.8317, 2.0257,  ..., 1.6909, 1.7247, 1.5575],\n",
      "         [2.0705, 2.2105, 2.2533,  ..., 1.8328, 2.0666, 1.8621],\n",
      "         [2.2224, 2.3359, 2.3493,  ..., 2.1668, 2.2152, 2.0820]],\n",
      "\n",
      "        [[1.8835, 2.0041, 2.1307,  ..., 1.8779, 1.8816, 1.9613],\n",
      "         [1.8289, 1.8542, 1.9272,  ..., 1.7709, 1.8638, 1.7751],\n",
      "         [2.1128, 2.0384, 2.1394,  ..., 1.9423, 1.9728, 2.1247],\n",
      "         ...,\n",
      "         [2.2570, 2.2805, 2.3387,  ..., 2.2033, 2.0030, 2.1354],\n",
      "         [2.1965, 2.3083, 2.3582,  ..., 2.0634, 2.1961, 2.1152],\n",
      "         [1.7018, 1.8267, 1.9783,  ..., 1.7807, 1.8233, 1.8743]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.6775, 1.8148, 2.0101,  ..., 1.8297, 1.9352, 1.9063],\n",
      "         [1.3469, 1.6615, 2.2357,  ..., 1.8311, 1.9163, 1.9037],\n",
      "         [1.3334, 1.5663, 1.9429,  ..., 1.6509, 1.5221, 1.5998],\n",
      "         ...,\n",
      "         [1.4674, 1.5943, 2.1452,  ..., 2.0153, 1.8090, 1.7888],\n",
      "         [1.4517, 1.7693, 1.9036,  ..., 1.8842, 1.8210, 1.8607],\n",
      "         [1.5441, 1.6540, 2.1017,  ..., 1.9638, 1.7757, 1.7549]],\n",
      "\n",
      "        [[2.1095, 2.1856, 1.9936,  ..., 1.5462, 1.7435, 1.8992],\n",
      "         [1.8488, 2.0550, 1.6791,  ..., 1.3580, 1.8075, 1.6590],\n",
      "         [2.1909, 2.4005, 2.0995,  ..., 1.8419, 1.9434, 1.8899],\n",
      "         ...,\n",
      "         [1.9933, 2.1199, 1.8478,  ..., 1.6274, 1.7750, 1.9108],\n",
      "         [2.1146, 2.1932, 1.7560,  ..., 1.6796, 1.8643, 1.6539],\n",
      "         [2.0417, 1.9257, 1.7967,  ..., 1.4430, 1.7181, 1.6997]],\n",
      "\n",
      "        [[2.4526, 2.4892, 2.7150,  ..., 2.2965, 2.1946, 2.6060],\n",
      "         [2.0740, 2.1069, 2.2276,  ..., 2.0177, 2.1127, 2.3369],\n",
      "         [2.3422, 2.3766, 2.5821,  ..., 2.2303, 2.0929, 2.5219],\n",
      "         ...,\n",
      "         [2.1022, 2.1462, 2.4231,  ..., 2.0712, 1.9083, 2.4535],\n",
      "         [2.1577, 2.2132, 2.5411,  ..., 2.1143, 1.9463, 2.2843],\n",
      "         [2.0131, 2.0940, 2.2473,  ..., 2.0028, 1.7597, 2.0961]]],\n",
      "       device='mps:0'))\n",
      "attention_dist: (torch.Size([16, 512, 512]), tensor([[[0.0021, 0.0014, 0.0019,  ..., 0.0015, 0.0022, 0.0024],\n",
      "         [0.0017, 0.0012, 0.0019,  ..., 0.0020, 0.0020, 0.0022],\n",
      "         [0.0017, 0.0014, 0.0020,  ..., 0.0018, 0.0019, 0.0023],\n",
      "         ...,\n",
      "         [0.0019, 0.0016, 0.0018,  ..., 0.0018, 0.0022, 0.0024],\n",
      "         [0.0021, 0.0014, 0.0016,  ..., 0.0017, 0.0021, 0.0022],\n",
      "         [0.0021, 0.0012, 0.0020,  ..., 0.0020, 0.0020, 0.0022]],\n",
      "\n",
      "        [[0.0023, 0.0020, 0.0026,  ..., 0.0016, 0.0019, 0.0017],\n",
      "         [0.0021, 0.0018, 0.0025,  ..., 0.0018, 0.0020, 0.0016],\n",
      "         [0.0022, 0.0019, 0.0028,  ..., 0.0020, 0.0019, 0.0016],\n",
      "         ...,\n",
      "         [0.0018, 0.0020, 0.0024,  ..., 0.0017, 0.0018, 0.0015],\n",
      "         [0.0020, 0.0023, 0.0024,  ..., 0.0015, 0.0020, 0.0016],\n",
      "         [0.0020, 0.0023, 0.0023,  ..., 0.0019, 0.0020, 0.0018]],\n",
      "\n",
      "        [[0.0017, 0.0019, 0.0022,  ..., 0.0017, 0.0017, 0.0018],\n",
      "         [0.0020, 0.0021, 0.0022,  ..., 0.0019, 0.0021, 0.0019],\n",
      "         [0.0021, 0.0020, 0.0022,  ..., 0.0018, 0.0019, 0.0022],\n",
      "         ...,\n",
      "         [0.0021, 0.0022, 0.0023,  ..., 0.0020, 0.0017, 0.0019],\n",
      "         [0.0020, 0.0022, 0.0023,  ..., 0.0017, 0.0020, 0.0018],\n",
      "         [0.0019, 0.0021, 0.0024,  ..., 0.0020, 0.0021, 0.0022]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0015, 0.0017, 0.0020,  ..., 0.0017, 0.0019, 0.0018],\n",
      "         [0.0011, 0.0015, 0.0027,  ..., 0.0018, 0.0020, 0.0019],\n",
      "         [0.0013, 0.0016, 0.0024,  ..., 0.0018, 0.0016, 0.0017],\n",
      "         ...,\n",
      "         [0.0012, 0.0014, 0.0024,  ..., 0.0021, 0.0017, 0.0017],\n",
      "         [0.0013, 0.0018, 0.0020,  ..., 0.0020, 0.0019, 0.0020],\n",
      "         [0.0014, 0.0016, 0.0025,  ..., 0.0021, 0.0018, 0.0017]],\n",
      "\n",
      "        [[0.0021, 0.0023, 0.0019,  ..., 0.0012, 0.0015, 0.0017],\n",
      "         [0.0020, 0.0024, 0.0017,  ..., 0.0012, 0.0019, 0.0016],\n",
      "         [0.0019, 0.0023, 0.0017,  ..., 0.0013, 0.0015, 0.0014],\n",
      "         ...,\n",
      "         [0.0019, 0.0022, 0.0017,  ..., 0.0013, 0.0015, 0.0018],\n",
      "         [0.0022, 0.0024, 0.0015,  ..., 0.0014, 0.0017, 0.0014],\n",
      "         [0.0023, 0.0020, 0.0018,  ..., 0.0012, 0.0016, 0.0016]],\n",
      "\n",
      "        [[0.0020, 0.0021, 0.0026,  ..., 0.0017, 0.0016, 0.0024],\n",
      "         [0.0020, 0.0021, 0.0024,  ..., 0.0019, 0.0021, 0.0026],\n",
      "         [0.0021, 0.0022, 0.0027,  ..., 0.0019, 0.0017, 0.0026],\n",
      "         ...,\n",
      "         [0.0019, 0.0020, 0.0026,  ..., 0.0018, 0.0016, 0.0027],\n",
      "         [0.0019, 0.0020, 0.0028,  ..., 0.0018, 0.0015, 0.0021],\n",
      "         [0.0021, 0.0022, 0.0026,  ..., 0.0020, 0.0016, 0.0022]]],\n",
      "       device='mps:0'))\n",
      "attention_matrix: (torch.Size([16, 512, 64]), tensor([[[0.5012, 0.4856, 0.5102,  ..., 0.4750, 0.5094, 0.5091],\n",
      "         [0.5032, 0.4850, 0.5101,  ..., 0.4747, 0.5083, 0.5082],\n",
      "         [0.5031, 0.4872, 0.5133,  ..., 0.4750, 0.5084, 0.5065],\n",
      "         ...,\n",
      "         [0.4997, 0.4852, 0.5122,  ..., 0.4732, 0.5088, 0.5108],\n",
      "         [0.5017, 0.4861, 0.5108,  ..., 0.4733, 0.5083, 0.5075],\n",
      "         [0.5007, 0.4862, 0.5142,  ..., 0.4731, 0.5095, 0.5106]],\n",
      "\n",
      "        [[0.5004, 0.5021, 0.5057,  ..., 0.5064, 0.4990, 0.4949],\n",
      "         [0.5014, 0.5027, 0.5051,  ..., 0.5049, 0.4998, 0.4971],\n",
      "         [0.5007, 0.5041, 0.5037,  ..., 0.5062, 0.5035, 0.4981],\n",
      "         ...,\n",
      "         [0.5005, 0.5040, 0.5046,  ..., 0.5059, 0.5002, 0.4953],\n",
      "         [0.4997, 0.5052, 0.5050,  ..., 0.5052, 0.4995, 0.4970],\n",
      "         [0.5003, 0.5032, 0.5039,  ..., 0.5056, 0.5000, 0.4980]],\n",
      "\n",
      "        [[0.4992, 0.5040, 0.4873,  ..., 0.5195, 0.5189, 0.5016],\n",
      "         [0.4988, 0.5038, 0.4884,  ..., 0.5195, 0.5200, 0.5007],\n",
      "         [0.4977, 0.5057, 0.4864,  ..., 0.5185, 0.5200, 0.4993],\n",
      "         ...,\n",
      "         [0.4981, 0.5037, 0.4872,  ..., 0.5203, 0.5193, 0.5005],\n",
      "         [0.4981, 0.5048, 0.4871,  ..., 0.5197, 0.5185, 0.4994],\n",
      "         [0.4974, 0.5032, 0.4864,  ..., 0.5201, 0.5209, 0.5033]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.4922, 0.5154, 0.5100,  ..., 0.4867, 0.4904, 0.5085],\n",
      "         [0.4942, 0.5117, 0.5109,  ..., 0.4860, 0.4920, 0.5099],\n",
      "         [0.4942, 0.5097, 0.5116,  ..., 0.4854, 0.4916, 0.5104],\n",
      "         ...,\n",
      "         [0.4923, 0.5124, 0.5126,  ..., 0.4875, 0.4925, 0.5091],\n",
      "         [0.4939, 0.5128, 0.5110,  ..., 0.4862, 0.4910, 0.5090],\n",
      "         [0.4935, 0.5151, 0.5130,  ..., 0.4851, 0.4915, 0.5086]],\n",
      "\n",
      "        [[0.4867, 0.5154, 0.4998,  ..., 0.4967, 0.4955, 0.4864],\n",
      "         [0.4855, 0.5146, 0.4961,  ..., 0.4969, 0.4953, 0.4857],\n",
      "         [0.4863, 0.5148, 0.4995,  ..., 0.4983, 0.4945, 0.4843],\n",
      "         ...,\n",
      "         [0.4858, 0.5147, 0.4972,  ..., 0.4994, 0.4950, 0.4862],\n",
      "         [0.4891, 0.5133, 0.4976,  ..., 0.4952, 0.4958, 0.4873],\n",
      "         [0.4890, 0.5151, 0.4980,  ..., 0.4987, 0.4946, 0.4853]],\n",
      "\n",
      "        [[0.4893, 0.5244, 0.5060,  ..., 0.4965, 0.5107, 0.5021],\n",
      "         [0.4901, 0.5251, 0.5094,  ..., 0.4975, 0.5082, 0.5052],\n",
      "         [0.4898, 0.5249, 0.5070,  ..., 0.4967, 0.5102, 0.5024],\n",
      "         ...,\n",
      "         [0.4899, 0.5225, 0.5079,  ..., 0.4965, 0.5080, 0.5028],\n",
      "         [0.4918, 0.5236, 0.5077,  ..., 0.4968, 0.5083, 0.5027],\n",
      "         [0.4925, 0.5244, 0.5070,  ..., 0.4951, 0.5082, 0.5026]]],\n",
      "       device='mps:0'))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Comparing with pure self-attention\n",
    "\"\"\"\n",
    "\n",
    "attn_matrix = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(dim_head))\n",
    "print(f\"attn_matrix: {attn_matrix.shape, attn_matrix}\")\n",
    "\n",
    "attention_dist = F.softmax(attn_matrix, dim=-1)\n",
    "print(f\"attention_dist: {attention_dist.shape, attention_dist}\")\n",
    "\n",
    "attention_matrix = torch.matmul(attention_dist, value)\n",
    "print(f\"attention_matrix: {attention_matrix.shape, attention_matrix}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T16:31:44.040082Z",
     "start_time": "2024-03-03T16:31:43.862351Z"
    }
   },
   "id": "5b0242ad3d65be20",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-1.4282, device='mps:0')"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Comparing with pure self-attention by KL-divergence\n",
    "배치 별, 평균 총합 0.5 ~ 2 정도 차이남, 이게 보니까 처음에 랜덤 초기화 빨로 갈리네\n",
    "\"\"\"\n",
    "\n",
    "kl_div = F.kl_div(linear_attn_matrix.log(), attention_matrix, reduction='batchmean')\n",
    "kl_div"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T16:12:49.605571Z",
     "start_time": "2024-03-03T16:12:49.597840Z"
    }
   },
   "id": "2cdaaf0b6c2d4400",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 58,  64],\n",
      "        [139, 154]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "])\n",
    "B = torch.tensor([\n",
    "    [7, 8],\n",
    "    [9, 10],\n",
    "    [11, 12]\n",
    "])\n",
    "result = torch.matmul(A, B)\n",
    "print(result)  # 출력: tensor([[ 58,  64], [139, 154]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T16:32:39.749975Z",
     "start_time": "2024-03-03T16:32:39.745124Z"
    }
   },
   "id": "f33eb0218fbb4b2e",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" Test code for applying padding masking to linear attention \"\"\"\n",
    "test_q = torch.randn(3, 5, 4, device=device)\n",
    "test_k = torch.randn(3, 5, 4, device=device)\n",
    "test_v = torch.randn(3, 5, 4, device=device)\n",
    "\n",
    "padding_mask = torch.tensor([\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 1, 1, 1]],\n",
    "    device=device\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26644eb6ded2d1c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, dim_model: int= 768):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.i_arr = torch.arange(1, int(dim_model/2)+1)  # 세타값을 살리려면 \n",
    "        self.theta = 10000**(-2*(self.i_arr - 1)/self.dim_model)\n",
    "    \n",
    "    def forward(self):\n",
    "        print(self.i_arr.shape)\n",
    "        print(self.i_arr)\n",
    "        print(self.theta.shape)\n",
    "        print(self.theta)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:49.209569Z",
     "start_time": "2024-03-02T13:30:49.203796Z"
    }
   },
   "id": "97c1f10595f4412f",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384])\n",
      "tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
      "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
      "         29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
      "         43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
      "         57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
      "         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
      "         85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
      "         99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
      "        113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
      "        127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
      "        141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
      "        155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
      "        183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
      "        197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210,\n",
      "        211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
      "        225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
      "        239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,\n",
      "        253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
      "        267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
      "        281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294,\n",
      "        295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
      "        309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
      "        323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
      "        337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
      "        365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
      "        379, 380, 381, 382, 383, 384])\n",
      "torch.Size([384])\n",
      "tensor([1.0000e+00, 9.7630e-01, 9.5316e-01, 9.3057e-01, 9.0852e-01, 8.8699e-01,\n",
      "        8.6596e-01, 8.4544e-01, 8.2540e-01, 8.0584e-01, 7.8674e-01, 7.6810e-01,\n",
      "        7.4989e-01, 7.3212e-01, 7.1477e-01, 6.9783e-01, 6.8129e-01, 6.6515e-01,\n",
      "        6.4938e-01, 6.3399e-01, 6.1897e-01, 6.0430e-01, 5.8997e-01, 5.7599e-01,\n",
      "        5.6234e-01, 5.4901e-01, 5.3600e-01, 5.2330e-01, 5.1090e-01, 4.9879e-01,\n",
      "        4.8697e-01, 4.7543e-01, 4.6416e-01, 4.5316e-01, 4.4242e-01, 4.3193e-01,\n",
      "        4.2170e-01, 4.1170e-01, 4.0195e-01, 3.9242e-01, 3.8312e-01, 3.7404e-01,\n",
      "        3.6517e-01, 3.5652e-01, 3.4807e-01, 3.3982e-01, 3.3177e-01, 3.2390e-01,\n",
      "        3.1623e-01, 3.0873e-01, 3.0142e-01, 2.9427e-01, 2.8730e-01, 2.8049e-01,\n",
      "        2.7384e-01, 2.6735e-01, 2.6102e-01, 2.5483e-01, 2.4879e-01, 2.4289e-01,\n",
      "        2.3714e-01, 2.3152e-01, 2.2603e-01, 2.2067e-01, 2.1544e-01, 2.1034e-01,\n",
      "        2.0535e-01, 2.0049e-01, 1.9573e-01, 1.9110e-01, 1.8657e-01, 1.8214e-01,\n",
      "        1.7783e-01, 1.7361e-01, 1.6950e-01, 1.6548e-01, 1.6156e-01, 1.5773e-01,\n",
      "        1.5399e-01, 1.5034e-01, 1.4678e-01, 1.4330e-01, 1.3991e-01, 1.3659e-01,\n",
      "        1.3335e-01, 1.3019e-01, 1.2711e-01, 1.2409e-01, 1.2115e-01, 1.1828e-01,\n",
      "        1.1548e-01, 1.1274e-01, 1.1007e-01, 1.0746e-01, 1.0491e-01, 1.0243e-01,\n",
      "        1.0000e-01, 9.7630e-02, 9.5316e-02, 9.3057e-02, 9.0852e-02, 8.8699e-02,\n",
      "        8.6596e-02, 8.4544e-02, 8.2540e-02, 8.0584e-02, 7.8674e-02, 7.6810e-02,\n",
      "        7.4989e-02, 7.3212e-02, 7.1477e-02, 6.9783e-02, 6.8129e-02, 6.6515e-02,\n",
      "        6.4938e-02, 6.3399e-02, 6.1897e-02, 6.0430e-02, 5.8997e-02, 5.7599e-02,\n",
      "        5.6234e-02, 5.4901e-02, 5.3600e-02, 5.2330e-02, 5.1090e-02, 4.9879e-02,\n",
      "        4.8697e-02, 4.7543e-02, 4.6416e-02, 4.5316e-02, 4.4242e-02, 4.3193e-02,\n",
      "        4.2170e-02, 4.1170e-02, 4.0195e-02, 3.9242e-02, 3.8312e-02, 3.7404e-02,\n",
      "        3.6517e-02, 3.5652e-02, 3.4807e-02, 3.3982e-02, 3.3177e-02, 3.2390e-02,\n",
      "        3.1623e-02, 3.0873e-02, 3.0142e-02, 2.9427e-02, 2.8730e-02, 2.8049e-02,\n",
      "        2.7384e-02, 2.6735e-02, 2.6102e-02, 2.5483e-02, 2.4879e-02, 2.4289e-02,\n",
      "        2.3714e-02, 2.3152e-02, 2.2603e-02, 2.2067e-02, 2.1544e-02, 2.1034e-02,\n",
      "        2.0535e-02, 2.0049e-02, 1.9573e-02, 1.9110e-02, 1.8657e-02, 1.8214e-02,\n",
      "        1.7783e-02, 1.7361e-02, 1.6950e-02, 1.6548e-02, 1.6156e-02, 1.5773e-02,\n",
      "        1.5399e-02, 1.5034e-02, 1.4678e-02, 1.4330e-02, 1.3991e-02, 1.3659e-02,\n",
      "        1.3335e-02, 1.3019e-02, 1.2711e-02, 1.2409e-02, 1.2115e-02, 1.1828e-02,\n",
      "        1.1548e-02, 1.1274e-02, 1.1007e-02, 1.0746e-02, 1.0491e-02, 1.0243e-02,\n",
      "        1.0000e-02, 9.7630e-03, 9.5316e-03, 9.3057e-03, 9.0852e-03, 8.8699e-03,\n",
      "        8.6596e-03, 8.4544e-03, 8.2540e-03, 8.0584e-03, 7.8674e-03, 7.6810e-03,\n",
      "        7.4989e-03, 7.3212e-03, 7.1477e-03, 6.9783e-03, 6.8129e-03, 6.6515e-03,\n",
      "        6.4938e-03, 6.3399e-03, 6.1897e-03, 6.0430e-03, 5.8997e-03, 5.7599e-03,\n",
      "        5.6234e-03, 5.4901e-03, 5.3600e-03, 5.2330e-03, 5.1090e-03, 4.9879e-03,\n",
      "        4.8697e-03, 4.7543e-03, 4.6416e-03, 4.5316e-03, 4.4242e-03, 4.3193e-03,\n",
      "        4.2170e-03, 4.1170e-03, 4.0195e-03, 3.9242e-03, 3.8312e-03, 3.7404e-03,\n",
      "        3.6517e-03, 3.5652e-03, 3.4807e-03, 3.3982e-03, 3.3177e-03, 3.2390e-03,\n",
      "        3.1623e-03, 3.0873e-03, 3.0142e-03, 2.9427e-03, 2.8730e-03, 2.8049e-03,\n",
      "        2.7384e-03, 2.6735e-03, 2.6102e-03, 2.5483e-03, 2.4879e-03, 2.4289e-03,\n",
      "        2.3714e-03, 2.3152e-03, 2.2603e-03, 2.2067e-03, 2.1544e-03, 2.1034e-03,\n",
      "        2.0535e-03, 2.0049e-03, 1.9573e-03, 1.9110e-03, 1.8657e-03, 1.8214e-03,\n",
      "        1.7783e-03, 1.7361e-03, 1.6950e-03, 1.6548e-03, 1.6156e-03, 1.5773e-03,\n",
      "        1.5399e-03, 1.5034e-03, 1.4678e-03, 1.4330e-03, 1.3991e-03, 1.3659e-03,\n",
      "        1.3335e-03, 1.3019e-03, 1.2711e-03, 1.2409e-03, 1.2115e-03, 1.1828e-03,\n",
      "        1.1548e-03, 1.1274e-03, 1.1007e-03, 1.0746e-03, 1.0491e-03, 1.0243e-03,\n",
      "        1.0000e-03, 9.7630e-04, 9.5316e-04, 9.3057e-04, 9.0852e-04, 8.8699e-04,\n",
      "        8.6596e-04, 8.4544e-04, 8.2540e-04, 8.0584e-04, 7.8674e-04, 7.6810e-04,\n",
      "        7.4989e-04, 7.3212e-04, 7.1477e-04, 6.9783e-04, 6.8129e-04, 6.6515e-04,\n",
      "        6.4938e-04, 6.3399e-04, 6.1897e-04, 6.0430e-04, 5.8997e-04, 5.7599e-04,\n",
      "        5.6234e-04, 5.4901e-04, 5.3600e-04, 5.2330e-04, 5.1090e-04, 4.9879e-04,\n",
      "        4.8697e-04, 4.7543e-04, 4.6416e-04, 4.5316e-04, 4.4242e-04, 4.3193e-04,\n",
      "        4.2170e-04, 4.1170e-04, 4.0195e-04, 3.9242e-04, 3.8312e-04, 3.7404e-04,\n",
      "        3.6517e-04, 3.5652e-04, 3.4807e-04, 3.3982e-04, 3.3177e-04, 3.2390e-04,\n",
      "        3.1623e-04, 3.0873e-04, 3.0142e-04, 2.9427e-04, 2.8730e-04, 2.8049e-04,\n",
      "        2.7384e-04, 2.6735e-04, 2.6102e-04, 2.5483e-04, 2.4879e-04, 2.4289e-04,\n",
      "        2.3714e-04, 2.3152e-04, 2.2603e-04, 2.2067e-04, 2.1544e-04, 2.1034e-04,\n",
      "        2.0535e-04, 2.0049e-04, 1.9573e-04, 1.9110e-04, 1.8657e-04, 1.8214e-04,\n",
      "        1.7783e-04, 1.7361e-04, 1.6950e-04, 1.6548e-04, 1.6156e-04, 1.5773e-04,\n",
      "        1.5399e-04, 1.5034e-04, 1.4678e-04, 1.4330e-04, 1.3991e-04, 1.3659e-04,\n",
      "        1.3335e-04, 1.3019e-04, 1.2711e-04, 1.2409e-04, 1.2115e-04, 1.1828e-04,\n",
      "        1.1548e-04, 1.1274e-04, 1.1007e-04, 1.0746e-04, 1.0491e-04, 1.0243e-04])\n"
     ]
    }
   ],
   "source": [
    "test = RoPE()\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:49.213790Z",
     "start_time": "2024-03-02T13:30:49.206814Z"
    }
   },
   "id": "938bd92df37c8366",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:49.227687Z",
     "start_time": "2024-03-02T13:30:49.213511Z"
    }
   },
   "id": "e598d761ff00999a",
   "execution_count": 93
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
