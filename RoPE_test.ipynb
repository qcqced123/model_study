{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import experiment.metrics.metric as metric\n",
    "from experiment.metrics.metric import cosine_similarity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T07:22:27.241405Z",
     "start_time": "2024-03-05T07:22:26.337393Z"
    }
   },
   "id": "f1b867fbf9578cf4",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='mps')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" set default device to mps \"\"\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T07:37:56.709335Z",
     "start_time": "2024-03-05T07:37:56.705524Z"
    }
   },
   "id": "eedaeeb4c236e989",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" parallel multi head attention \"\"\"\n",
    "def kernel_fn(x: Tensor, kernel_name: str) -> Tensor:\n",
    "    \"\"\" Select kernel function for attention head\n",
    "    This is temporary function, we will implement more kernel function in future\n",
    "    \"\"\"\n",
    "    hidden_state = None\n",
    "    if kernel_name == 'elu':\n",
    "        hidden_state = F.elu(x) + 1\n",
    "    return hidden_state \n",
    "\n",
    "\n",
    "def linear_attention(\n",
    "    q: Tensor,\n",
    "    k: Tensor,\n",
    "    v: Tensor,\n",
    "    kernel: str = 'elu',\n",
    "    eps: float = 1e-6,\n",
    "    attention_dropout: nn.Dropout = None,\n",
    "    padding_mask: Tensor = None,\n",
    "    attention_mask: Tensor = None,\n",
    ") -> Tensor:\n",
    "    \"\"\" Linear attention with masking for padding token\n",
    "    This function is designed for parallel computation with head dimension, not using loop & concatenate method\n",
    "\n",
    "    Args:\n",
    "        q: query matrix, shape (batch_size, seq_len, dim_head)\n",
    "        k: key matrix, shape (batch_size, seq_len, dim_head)\n",
    "        v: value matrix, shape (batch_size, seq_len, dim_head)\n",
    "        dim_head: default 64 (int), dimension of each attention head\n",
    "        kernel: default elu (str), which is used in original paper\n",
    "        eps: default 1e-8 (float), for numerical stability\n",
    "        attention_dropout: default rate is 0.1, dropout for attention matrix\n",
    "        padding_mask: mask for attention matrix for MLM, you must check whether or not padding token is 1\n",
    "        attention_mask: mask for attention matrix for CLM\n",
    "\n",
    "    Math:\n",
    "        A = normalize(Φ(Q).mm(Φ(K).t())).mm(V)\n",
    "        \n",
    "    Einsum:\n",
    "        b: batch_size\n",
    "        s: sequence length of query\n",
    "        h: number of heads\n",
    "        q: dimension size of each query's heads\n",
    "        k: dimension size of each key's heads\n",
    "        v: dimension size of each value's heads\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/2006.16236\n",
    "        https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/linear_attention.py\n",
    "        \n",
    "    \"\"\"\n",
    "    BS, SEQ_LEN, NUM_HEADS, DIM_HEADS = q.shape\n",
    "    projected_q, projected_k = kernel_fn(q, kernel), kernel_fn(k, kernel)\n",
    "    if padding_mask is not None:  # applying padding mask, calculating normalizer\n",
    "        projected_q[padding_mask == 1], projected_k[padding_mask == 1], v[padding_mask == 1] = 0, 0, 0\n",
    "\n",
    "    kv = torch.matmul(v.permute(0, 2, 3, 1).contiguous(), projected_k.permute(0, 2, 1, 3).contiguous())\n",
    "    z = 1 / torch.clamp(torch.mul(projected_q, projected_k.sum(dim=1).unsqueeze(1)).sum(dim=-1), min=eps)\n",
    "    attention_matrix = torch.einsum(\"bshq,bhvk,bsh->bshv\", projected_q, kv, z).reshape(-1, SEQ_LEN, NUM_HEADS*DIM_HEADS)\n",
    "\n",
    "    # attention dropout\n",
    "    if attention_dropout is not None:\n",
    "        attention_matrix = attention_dropout(\n",
    "            attention_matrix\n",
    "        )\n",
    "    return attention_matrix\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" In this class, we implement workflow of Multi-Head Self-attention for Linear Transformers\n",
    "    This class has same role as Module \"BertAttention\" in official Repo (bert.py)\n",
    "    In official repo, they use post-layer norm, but we use pre-layer norm which is more stable & efficient for training\n",
    "\n",
    "    Args:\n",
    "        dim_model: dimension of model's latent vector space, default 1024 from official paper\n",
    "        num_attention_heads: number of heads in MHSA, default 16 from official paper for Transformer\n",
    "        dim_head: dimension of each attention head, default 64 from official paper (1024 / 16)\n",
    "        kernel: kernel function for attention head, default 'elu' from official paper\n",
    "        attention_dropout_prob: dropout rate, default 0.1\n",
    "\n",
    "    Math:\n",
    "        A = softmax(attention Matrix/sqrt(3*D_h)), SA(z) = Av\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/1706.03762\n",
    "        https://arxiv.org/pdf/1810.04805.pdf\n",
    "        https://arxiv.org/abs/2006.16236\n",
    "        https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/linear_attention.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim_model: int = 1024,\n",
    "            num_attention_heads: int = 16,\n",
    "            dim_head: int = 64,\n",
    "            kernel: str = 'elu',\n",
    "            attention_dropout_prob: float = 0.1\n",
    "    ) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.dim_head = dim_head\n",
    "        self.fc_q = nn.Linear(self.dim_model, self.dim_model)\n",
    "        self.fc_k = nn.Linear(self.dim_model, self.dim_model)\n",
    "        self.fc_v = nn.Linear(self.dim_model, self.dim_model)\n",
    "        self.fc_concat = nn.Linear(self.dim_model, self.dim_model)\n",
    "        self.attention = linear_attention\n",
    "        self.attention_dropout = nn.Dropout(p=attention_dropout_prob)\n",
    "        self.kernel = kernel\n",
    "        self.eps = 1e-6\n",
    "        \n",
    "    def forward(self, x: Tensor, padding_mask: Tensor, attention_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\" x is already passed nn.Layernorm, already multiplied with rotary position encoding \"\"\"\n",
    "        assert x.ndim == 3, f'Expected (batch, seq, hidden) got {x.shape}'\n",
    "        \n",
    "        # size: bs, seq, nums head, dim head, linear projection\n",
    "        q = self.fc_q(x).reshape(-1, x.shape[1], self.num_attention_heads, self.dim_head)  \n",
    "        k = self.fc_k(x).reshape(-1, x.shape[1], self.num_attention_heads, self.dim_head)\n",
    "        v = self.fc_v(x).reshape(-1, x.shape[1], self.num_attention_heads, self.dim_head)\n",
    "        \n",
    "        attention_matrix = self.attention(\n",
    "            q,\n",
    "            k,\n",
    "            v,\n",
    "            self.kernel,\n",
    "            self.eps,\n",
    "            self.attention_dropout,\n",
    "            padding_mask,\n",
    "            attention_mask\n",
    "        )\n",
    "        attention_output = self.fc_concat(attention_matrix)\n",
    "        return attention_output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68d8a60b5674d01d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\" testing for rotary position encoding \n",
    "\n",
    "[cos m*theta1, -sin m*theta1]\n",
    "[sin m*theta1, cos m*theta1]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self, dim_model: int= 768):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.i_arr = torch.arange(1, int(dim_model/2)+1)  # 세타값을 살리려면 \n",
    "        self.theta = 10000**(-2*(self.i_arr - 1)/self.dim_model)\n",
    "    \n",
    "    def forward(self):\n",
    "        print(self.i_arr.shape)\n",
    "        print(self.i_arr)\n",
    "        print(self.theta.shape)\n",
    "        print(self.theta)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T06:41:59.909528Z",
     "start_time": "2024-03-05T06:41:59.905096Z"
    }
   },
   "id": "9db9021356d10b2b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kv: (torch.Size([3, 4, 2, 2]), tensor([[[[1.8712, 1.7441],\n",
      "          [2.0152, 1.8818]],\n",
      "\n",
      "         [[1.2480, 1.6586],\n",
      "          [0.9948, 1.8648]],\n",
      "\n",
      "         [[1.8897, 1.1054],\n",
      "          [1.4765, 1.0960]],\n",
      "\n",
      "         [[1.1829, 2.3455],\n",
      "          [1.1215, 1.7871]]],\n",
      "\n",
      "\n",
      "        [[[1.2551, 1.4782],\n",
      "          [0.6835, 0.8580]],\n",
      "\n",
      "         [[2.0954, 2.2804],\n",
      "          [1.2951, 1.9219]],\n",
      "\n",
      "         [[1.5423, 1.0609],\n",
      "          [2.4590, 1.7339]],\n",
      "\n",
      "         [[2.1012, 1.5119],\n",
      "          [1.1772, 0.9965]]],\n",
      "\n",
      "\n",
      "        [[[1.3803, 2.0247],\n",
      "          [1.0918, 1.3187]],\n",
      "\n",
      "         [[1.6456, 1.3337],\n",
      "          [0.9049, 1.3710]],\n",
      "\n",
      "         [[1.0539, 1.3264],\n",
      "          [1.2296, 1.8057]],\n",
      "\n",
      "         [[1.3767, 1.8384],\n",
      "          [0.7315, 0.7558]]]], device='mps:0'))\n",
      "z: (torch.Size([3, 5, 4]), tensor([[[0.4404, 0.2922, 0.5909, 0.2315],\n",
      "         [0.2317, 0.4030, 0.4618, 0.2901],\n",
      "         [0.6622, 0.5419, 0.2251, 0.2437],\n",
      "         [0.6661, 0.3618, 0.3576, 0.9171],\n",
      "         [0.2153, 0.3808, 0.4363, 0.4122]],\n",
      "\n",
      "        [[1.4899, 0.4322, 0.3827, 0.5344],\n",
      "         [0.3850, 0.3527, 0.3429, 0.4290],\n",
      "         [0.3780, 0.5035, 0.6515, 0.7208],\n",
      "         [0.4250, 0.3972, 0.3370, 1.3863],\n",
      "         [0.3472, 0.3455, 0.2217, 1.5013]],\n",
      "\n",
      "        [[0.3272, 0.7635, 0.5159, 0.2584],\n",
      "         [0.9012, 0.3209, 0.9350, 0.3100],\n",
      "         [0.4084, 0.2545, 0.4274, 1.0087],\n",
      "         [0.5627, 0.4500, 0.4330, 0.4545],\n",
      "         [1.5795, 0.3925, 0.2638, 0.2200]]], device='mps:0'))\n",
      "qkv: (torch.Size([3, 5, 4, 2]), tensor([[[[1.2408, 1.3386],\n",
      "          [1.9509, 1.9588],\n",
      "          [0.9196, 0.8562],\n",
      "          [3.3162, 2.7151]],\n",
      "\n",
      "         [[2.4284, 2.6169],\n",
      "          [1.4205, 1.3660],\n",
      "          [1.1989, 0.9490],\n",
      "          [2.7088, 2.1410]],\n",
      "\n",
      "         [[0.8548, 0.9209],\n",
      "          [1.0627, 0.9601],\n",
      "          [2.4392, 2.0793],\n",
      "          [3.1278, 2.5885]],\n",
      "\n",
      "         [[0.8536, 0.9195],\n",
      "          [1.5770, 1.5684],\n",
      "          [1.5383, 1.2891],\n",
      "          [0.8249, 0.6902]],\n",
      "\n",
      "         [[2.6101, 2.8128],\n",
      "          [1.4883, 1.5801],\n",
      "          [1.2475, 1.1469],\n",
      "          [1.8955, 1.5112]]],\n",
      "\n",
      "\n",
      "        [[[0.4177, 0.2318],\n",
      "          [1.5713, 0.9904],\n",
      "          [1.2565, 2.0138],\n",
      "          [1.4761, 0.9572]],\n",
      "\n",
      "         [[1.6114, 0.8961],\n",
      "          [2.0765, 1.6904],\n",
      "          [1.3481, 2.1707],\n",
      "          [1.8239, 1.0630]],\n",
      "\n",
      "         [[1.5629, 0.8965],\n",
      "          [1.3791, 0.9452],\n",
      "          [0.7315, 1.1736],\n",
      "          [1.0948, 0.7137]],\n",
      "\n",
      "         [[1.4697, 0.8137],\n",
      "          [1.7890, 1.3274],\n",
      "          [1.4375, 2.3020],\n",
      "          [0.5640, 0.3250]],\n",
      "\n",
      "         [[1.7515, 0.9863],\n",
      "          [2.1310, 1.7614],\n",
      "          [2.0671, 3.3321],\n",
      "          [0.5222, 0.3126]]],\n",
      "\n",
      "\n",
      "        [[[2.2275, 1.6126],\n",
      "          [0.8339, 0.5236],\n",
      "          [1.1866, 1.4508],\n",
      "          [2.3908, 1.0708]],\n",
      "\n",
      "         [[0.8616, 0.5745],\n",
      "          [1.8054, 1.5124],\n",
      "          [0.5985, 0.8084],\n",
      "          [1.9112, 0.9052]],\n",
      "\n",
      "         [[1.8351, 1.2817],\n",
      "          [2.3440, 1.8055],\n",
      "          [1.4109, 1.7541],\n",
      "          [0.6324, 0.2711]],\n",
      "\n",
      "         [[1.3795, 0.9202],\n",
      "          [1.2058, 1.1998],\n",
      "          [1.3451, 1.7382],\n",
      "          [1.1998, 0.6342]],\n",
      "\n",
      "         [[0.4345, 0.3398],\n",
      "          [1.5145, 1.1788],\n",
      "          [2.2275, 2.8506],\n",
      "          [2.7646, 1.2644]]]], device='mps:0'))\n",
      "V: (torch.Size([3, 5, 4, 2]), tensor([[[[0.5465, 0.5896],\n",
      "          [0.5701, 0.5724],\n",
      "          [0.5434, 0.5059],\n",
      "          [0.7676, 0.6285]],\n",
      "\n",
      "         [[0.5626, 0.6063],\n",
      "          [0.5725, 0.5505],\n",
      "          [0.5536, 0.4382],\n",
      "          [0.7858, 0.6211]],\n",
      "\n",
      "         [[0.5660, 0.6098],\n",
      "          [0.5759, 0.5203],\n",
      "          [0.5491, 0.4681],\n",
      "          [0.7621, 0.6307]],\n",
      "\n",
      "         [[0.5686, 0.6125],\n",
      "          [0.5706, 0.5675],\n",
      "          [0.5502, 0.4610],\n",
      "          [0.7565, 0.6330]],\n",
      "\n",
      "         [[0.5618, 0.6055],\n",
      "          [0.5668, 0.6018],\n",
      "          [0.5442, 0.5004],\n",
      "          [0.7813, 0.6229]]],\n",
      "\n",
      "\n",
      "        [[[0.6224, 0.3454],\n",
      "          [0.6792, 0.4281],\n",
      "          [0.4809, 0.7707],\n",
      "          [0.7887, 0.5115]],\n",
      "\n",
      "         [[0.6203, 0.3450],\n",
      "          [0.7324, 0.5963],\n",
      "          [0.4623, 0.7444],\n",
      "          [0.7825, 0.4561]],\n",
      "\n",
      "         [[0.5908, 0.3389],\n",
      "          [0.6943, 0.4759],\n",
      "          [0.4765, 0.7645],\n",
      "          [0.7891, 0.5144]],\n",
      "\n",
      "         [[0.6247, 0.3459],\n",
      "          [0.7106, 0.5272],\n",
      "          [0.4844, 0.7757],\n",
      "          [0.7819, 0.4505]],\n",
      "\n",
      "         [[0.6082, 0.3425],\n",
      "          [0.7364, 0.6086],\n",
      "          [0.4583, 0.7388],\n",
      "          [0.7840, 0.4692]]],\n",
      "\n",
      "\n",
      "        [[[0.7290, 0.5277],\n",
      "          [0.6367, 0.3998],\n",
      "          [0.6122, 0.7485],\n",
      "          [0.6177, 0.2766]],\n",
      "\n",
      "         [[0.7765, 0.5178],\n",
      "          [0.5793, 0.4853],\n",
      "          [0.5596, 0.7558],\n",
      "          [0.5926, 0.2807]],\n",
      "\n",
      "         [[0.7494, 0.5234],\n",
      "          [0.5966, 0.4595],\n",
      "          [0.6031, 0.7497],\n",
      "          [0.6378, 0.2734]],\n",
      "\n",
      "         [[0.7763, 0.5178],\n",
      "          [0.5426, 0.5399],\n",
      "          [0.5824, 0.7526],\n",
      "          [0.5453, 0.2882]],\n",
      "\n",
      "         [[0.6864, 0.5367],\n",
      "          [0.5945, 0.4627],\n",
      "          [0.5876, 0.7519],\n",
      "          [0.6082, 0.2782]]]], device='mps:0'))\n",
      "test_v: (torch.Size([3, 5, 4, 2]), tensor([[[[0.5465, 0.5896],\n",
      "          [0.5701, 0.5724],\n",
      "          [0.5434, 0.5059],\n",
      "          [0.7676, 0.6285]],\n",
      "\n",
      "         [[0.5626, 0.6063],\n",
      "          [0.5725, 0.5505],\n",
      "          [0.5536, 0.4382],\n",
      "          [0.7858, 0.6211]],\n",
      "\n",
      "         [[0.5660, 0.6098],\n",
      "          [0.5759, 0.5203],\n",
      "          [0.5491, 0.4681],\n",
      "          [0.7621, 0.6307]],\n",
      "\n",
      "         [[0.5686, 0.6125],\n",
      "          [0.5706, 0.5675],\n",
      "          [0.5502, 0.4610],\n",
      "          [0.7565, 0.6330]],\n",
      "\n",
      "         [[0.5618, 0.6055],\n",
      "          [0.5668, 0.6018],\n",
      "          [0.5442, 0.5004],\n",
      "          [0.7813, 0.6229]]],\n",
      "\n",
      "\n",
      "        [[[0.6224, 0.3454],\n",
      "          [0.6792, 0.4281],\n",
      "          [0.4809, 0.7707],\n",
      "          [0.7887, 0.5115]],\n",
      "\n",
      "         [[0.6203, 0.3450],\n",
      "          [0.7324, 0.5963],\n",
      "          [0.4623, 0.7444],\n",
      "          [0.7825, 0.4561]],\n",
      "\n",
      "         [[0.5908, 0.3389],\n",
      "          [0.6943, 0.4759],\n",
      "          [0.4765, 0.7645],\n",
      "          [0.7891, 0.5144]],\n",
      "\n",
      "         [[0.6247, 0.3459],\n",
      "          [0.7106, 0.5272],\n",
      "          [0.4844, 0.7757],\n",
      "          [0.7819, 0.4505]],\n",
      "\n",
      "         [[0.6082, 0.3425],\n",
      "          [0.7364, 0.6086],\n",
      "          [0.4583, 0.7388],\n",
      "          [0.7840, 0.4692]]],\n",
      "\n",
      "\n",
      "        [[[0.7290, 0.5277],\n",
      "          [0.6367, 0.3998],\n",
      "          [0.6122, 0.7485],\n",
      "          [0.6177, 0.2766]],\n",
      "\n",
      "         [[0.7765, 0.5178],\n",
      "          [0.5793, 0.4853],\n",
      "          [0.5596, 0.7558],\n",
      "          [0.5926, 0.2807]],\n",
      "\n",
      "         [[0.7494, 0.5234],\n",
      "          [0.5966, 0.4595],\n",
      "          [0.6031, 0.7497],\n",
      "          [0.6378, 0.2734]],\n",
      "\n",
      "         [[0.7763, 0.5178],\n",
      "          [0.5426, 0.5399],\n",
      "          [0.5824, 0.7526],\n",
      "          [0.5453, 0.2882]],\n",
      "\n",
      "         [[0.6864, 0.5367],\n",
      "          [0.5945, 0.4627],\n",
      "          [0.5876, 0.7519],\n",
      "          [0.6082, 0.2782]]]], device='mps:0'))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" source code from original linear transformers paper github\n",
    "\"\"\"\n",
    "\n",
    "max_seq = 5\n",
    "dim_model = 8\n",
    "num_heads = 4\n",
    "dim_head = dim_model // num_heads\n",
    "batch_size = 3\n",
    "\n",
    "# 3, 5, 4, 2\n",
    "query = torch.rand(batch_size, max_seq, num_heads, dim_head, device=device)\n",
    "key = torch.rand(batch_size, max_seq, num_heads, dim_head, device=device)\n",
    "value = torch.rand(batch_size, max_seq, num_heads, dim_head, device=device)\n",
    "\n",
    "kv = torch.einsum(\"nshd,nshm->nhmd\", key, value)\n",
    "print(f\"kv: {kv.shape, kv}\")\n",
    "\n",
    "z = 1/(torch.einsum(\"nlhd,nhd->nlh\", query, key.sum(dim=1))+1e-6)\n",
    "print(f\"z: {z.shape, z}\")\n",
    "\n",
    "qkv = torch.einsum(\"nlhd,nhmd->nlhm\", query, kv)\n",
    "print(f\"qkv: {qkv.shape, qkv}\")\n",
    "\n",
    "V = torch.einsum(\"nlhd,nhmd,nlh->nlhm\", query, kv, z)\n",
    "print(f\"V: {V.shape, V}\")\n",
    "\n",
    "test_v = torch.einsum(\"nlhm,nlh->nlhm\", qkv, z)\n",
    "print(f\"test_v: {test_v.shape, test_v}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T07:44:32.680278Z",
     "start_time": "2024-03-05T07:44:31.593837Z"
    }
   },
   "id": "79e47d3676ad6a6d",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV: (torch.Size([3, 2, 2]), tensor([[[1.6945, 2.5599],\n",
      "         [1.6986, 2.4920]],\n",
      "\n",
      "        [[3.7326, 3.1925],\n",
      "         [3.5027, 3.1093]],\n",
      "\n",
      "        [[1.4581, 2.1359],\n",
      "         [2.1605, 2.8166]]], device='cuda:0'))\n",
      "Z: (torch.Size([3, 5]), tensor([[5.9278e-02, 8.4242e-02, 8.2484e-02, 1.0000e+06, 1.0000e+06],\n",
      "        [4.6290e-02, 4.3917e-02, 6.5639e-02, 6.5267e-02, 1.0000e+06],\n",
      "        [1.0978e-01, 1.0496e-01, 1.0000e+06, 1.0000e+06, 1.0000e+06]],\n",
      "       device='cuda:0'))\n",
      "V: (torch.Size([3, 5, 2]), tensor([[[0.9854, 0.9707],\n",
      "         [1.0248, 1.0095],\n",
      "         [1.0131, 0.9979],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[1.1236, 1.0728],\n",
      "         [1.1181, 1.0675],\n",
      "         [1.1167, 1.0662],\n",
      "         [1.1213, 1.0706],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[1.0962, 1.5180],\n",
      "         [1.1320, 1.5677],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000]]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" alias of each tensor dimension for torch einsum\n",
    "b: batch_size\n",
    "s: sequence length\n",
    "q: dim_head of query\n",
    "k: dim_head of key\n",
    "v: dim_head of value\n",
    "\"\"\"\n",
    "def kernel_fn(x: Tensor):\n",
    "    return F.elu(x) + 1\n",
    "\n",
    "query = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "key = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "value = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "padding_mask = torch.tensor([\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 1, 1, 1]],\n",
    "    device=device\n",
    ")\n",
    "query, key = kernel_fn(query), kernel_fn(key)\n",
    "query[padding_mask == 1], key[padding_mask == 1], value[padding_mask == 1] = 0, 0, 0\n",
    "\n",
    "KV = torch.matmul(value.permute(0, 2, 1), key.permute(0, 1, 2))\n",
    "print(f\"KV: {KV.shape, KV}\")\n",
    "\n",
    "Z = 1 / torch.clamp(torch.mul(query,key.sum(dim=1).unsqueeze(1)).sum(dim=-1), min=1e-6)\n",
    "print(f\"Z: {Z.shape, Z}\")\n",
    "\n",
    "V = torch.einsum(\"bsq,bvk,bs->bsv\", query, KV, Z)\n",
    "print(f\"V: {V.shape, V}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T02:09:58.073747Z",
     "start_time": "2024-03-05T02:09:58.058927Z"
    }
   },
   "id": "87b8384ff530d8c3",
   "execution_count": 114
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV: (torch.Size([3, 4, 2, 2]), tensor([[[[1.8712, 1.7441],\n",
      "          [2.0152, 1.8818]],\n",
      "\n",
      "         [[1.2480, 1.6586],\n",
      "          [0.9948, 1.8648]],\n",
      "\n",
      "         [[1.8897, 1.1054],\n",
      "          [1.4765, 1.0960]],\n",
      "\n",
      "         [[1.1829, 2.3455],\n",
      "          [1.1215, 1.7871]]],\n",
      "\n",
      "\n",
      "        [[[1.2551, 1.4782],\n",
      "          [0.6835, 0.8580]],\n",
      "\n",
      "         [[2.0954, 2.2804],\n",
      "          [1.2951, 1.9219]],\n",
      "\n",
      "         [[1.5423, 1.0609],\n",
      "          [2.4590, 1.7339]],\n",
      "\n",
      "         [[2.1012, 1.5119],\n",
      "          [1.1772, 0.9965]]],\n",
      "\n",
      "\n",
      "        [[[1.3803, 2.0247],\n",
      "          [1.0918, 1.3187]],\n",
      "\n",
      "         [[1.6456, 1.3337],\n",
      "          [0.9049, 1.3710]],\n",
      "\n",
      "         [[1.0539, 1.3264],\n",
      "          [1.2296, 1.8057]],\n",
      "\n",
      "         [[1.3767, 1.8384],\n",
      "          [0.7315, 0.7558]]]], device='mps:0'))\n",
      "query: torch.Size([3, 5, 4, 2])\n",
      "key sum: torch.Size([3, 4, 2])\n",
      "Z: (tensor([[[0.4404, 0.2922, 0.5909, 0.2315],\n",
      "         [0.2317, 0.4030, 0.4618, 0.2901],\n",
      "         [0.6622, 0.5419, 0.2251, 0.2437],\n",
      "         [0.6661, 0.3618, 0.3576, 0.9171],\n",
      "         [0.2153, 0.3808, 0.4363, 0.4122]],\n",
      "\n",
      "        [[1.4899, 0.4323, 0.3827, 0.5344],\n",
      "         [0.3850, 0.3527, 0.3429, 0.4290],\n",
      "         [0.3780, 0.5035, 0.6515, 0.7208],\n",
      "         [0.4250, 0.3972, 0.3370, 1.3863],\n",
      "         [0.3472, 0.3455, 0.2217, 1.5013]],\n",
      "\n",
      "        [[0.3272, 0.7635, 0.5159, 0.2584],\n",
      "         [0.9012, 0.3209, 0.9350, 0.3100],\n",
      "         [0.4084, 0.2545, 0.4274, 1.0087],\n",
      "         [0.5627, 0.4500, 0.4330, 0.4545],\n",
      "         [1.5795, 0.3925, 0.2638, 0.2200]]], device='mps:0'), torch.Size([3, 5, 4]))\n"
     ]
    }
   ],
   "source": [
    "# [bs, num_heads, dim_head, max_seq]*[bs, num_heads, max_seq, dim_head] = [bs, num_heads, dim_head, dim_head]\n",
    "KV = torch.matmul(value.permute(0, 2, 3, 1), key.permute(0, 2, 1, 3))  \n",
    "print(f\"KV: {KV.shape, KV}\")\n",
    "\n",
    "print(f\"query: {query.shape}\")\n",
    "print(f\"key sum: {key.sum(dim=1).shape}\")\n",
    " \n",
    "Z = 1 / torch.clamp(torch.mul(query,key.sum(dim=1).unsqueeze(1)).sum(dim=-1), min=1e-6)\n",
    "print(f\"Z: {Z, Z.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T07:45:38.051623Z",
     "start_time": "2024-03-05T07:45:37.998378Z"
    }
   },
   "id": "aa54418f2826a466",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 5, 2]' is invalid for input of size 120",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_5978/321065780.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtest_query\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mquery\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_seq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim_head\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtest_key\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_seq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim_head\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mtest_value\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_seq\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim_head\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mKV\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatmul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_key\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpermute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_value\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: shape '[3, 5, 2]' is invalid for input of size 120"
     ]
    }
   ],
   "source": [
    "# test_query = query.reshape(batch_size, max_seq, dim_head)\n",
    "# test_key = key.reshape(batch_size, max_seq, dim_head)\n",
    "# test_value = value.reshape(batch_size, max_seq, dim_head)\n",
    "# \n",
    "# KV = torch.matmul(test_key.permute(0, 2, 1), test_value)\n",
    "# print(f\"KV: {KV.shape, KV}\")\n",
    "# QKV = torch.matmul(test_query, KV)\n",
    "# print(f\"QKV: {QKV.shape, QKV}\")\n",
    "\n",
    "# summation_key = test_key.sum(dim=1).unsqueeze(1)\n",
    "# print(f\"key, summation_key: {test_key.shape, summation_key.shape}\")\n",
    "# \n",
    "# Z = 1/torch.clamp(torch.mul(test_query, summation_key), min=1e-6)\n",
    "# print(f\"Normalizer Z: {Z, Z.shape}\")\n",
    "# \n",
    "# linear_attn_matrix = torch.matmul(QKV, Z)\n",
    "# print(f\"linear_attn_matrix: {linear_attn_matrix.shape, linear_attn_matrix}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T00:13:28.653319Z",
     "start_time": "2024-03-05T00:13:28.647237Z"
    }
   },
   "id": "47061c931e144f11",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(9663676416, 1207959552, 8)"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadratic = 512*64*64*512 + 512*512*512*64\n",
    "linear = 64*512*512*64 + 512*64*64*64\n",
    "\n",
    "quadratic, linear, quadratic // linear"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:48.714062Z",
     "start_time": "2024-03-02T13:30:48.709081Z"
    }
   },
   "id": "5c0366bb014d387d",
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3, 5, 2]),\n torch.Size([3, 5, 2]),\n torch.Size([3, 5, 2]),\n torch.Size([3, 5]))"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Inputs for Linear Attentions\n",
    "\"\"\"\n",
    "\n",
    "max_seq = 5\n",
    "dim_model = 2\n",
    "num_heads = 12\n",
    "batch_size = 3\n",
    "# dim_head = dim_model // num_heads\n",
    "dim_head = dim_model\n",
    "\n",
    "def kernel_fn(x: Tensor):\n",
    "    return F.elu(x) + 1\n",
    "\n",
    "query = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "key = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "value = torch.rand(batch_size, max_seq, dim_head, device=device)\n",
    "\n",
    "padding_mask = torch.tensor([\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 1, 1, 1]],\n",
    "    device=device\n",
    ")\n",
    "query.shape, key.shape, value.shape, padding_mask.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T16:47:05.824614Z",
     "start_time": "2024-03-04T16:47:05.816133Z"
    }
   },
   "id": "2db69b58bd5032a",
   "execution_count": 98
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[1.6423, 2.3725],\n          [1.9186, 2.7227]],\n \n         [[2.5099, 4.3219],\n          [2.0842, 3.6178]],\n \n         [[1.7249, 2.7095],\n          [1.6283, 2.5576]]], device='cuda:0'),\n torch.Size([3, 2, 2]))"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: KV Matrix\n",
    "\"\"\"\n",
    "k_query, k_key = kernel_fn(query), kernel_fn(key)\n",
    "k_query[padding_mask == 1], k_key[padding_mask == 1], value[padding_mask == 1] = 0, 0, 0 \n",
    "\n",
    "KV = torch.matmul(k_key.permute(0, 2, 1), value)\n",
    "KV, KV.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T16:47:05.999245Z",
     "start_time": "2024-03-04T16:47:05.993702Z"
    }
   },
   "id": "1d4d4a249fb2cf68",
   "execution_count": 99
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 6.9833,  9.9921],\n          [ 4.8467,  6.9438],\n          [ 5.7299,  8.2035],\n          [ 0.2364,  0.8221],\n          [ 0.5723,  0.9037]],\n \n         [[ 6.3969, 11.0627],\n          [ 6.1451, 10.6128],\n          [ 6.6618, 11.5190],\n          [ 7.9548, 13.7416],\n          [ 0.5258,  0.1820]],\n \n         [[ 4.7064,  7.3925],\n          [ 4.8010,  7.5412],\n          [ 0.4323,  0.3144],\n          [ 0.6062,  0.6203],\n          [ 0.8389,  0.5675]]], device='cuda:0'),\n torch.Size([3, 5, 2]))"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: QKV\n",
    "\"\"\"\n",
    "\n",
    "QKV = torch.matmul(k_query, KV)\n",
    "QKV, QKV.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T16:47:06.172677Z",
     "start_time": "2024-03-04T16:47:06.166589Z"
    }
   },
   "id": "56d258a4ac1b2e78",
   "execution_count": 100
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key, summation_key: (torch.Size([3, 5, 2]), torch.Size([3, 2, 2]))\n",
      "Normalizer Z: (tensor([[[5.6876e-02, 5.6876e-02],\n",
      "         [8.1770e-02, 8.1770e-02],\n",
      "         [6.9246e-02, 6.9246e-02],\n",
      "         [1.0000e+06, 1.0000e+06],\n",
      "         [1.0000e+06, 1.0000e+06]],\n",
      "\n",
      "        [[6.3809e-02, 6.3809e-02],\n",
      "         [6.7458e-02, 6.7458e-02],\n",
      "         [6.1383e-02, 6.1383e-02],\n",
      "         [5.1966e-02, 5.1966e-02],\n",
      "         [1.0000e+06, 1.0000e+06]],\n",
      "\n",
      "        [[9.9465e-02, 9.9465e-02],\n",
      "         [9.7511e-02, 9.7511e-02],\n",
      "         [1.0000e+06, 1.0000e+06],\n",
      "         [1.0000e+06, 1.0000e+06],\n",
      "         [1.0000e+06, 1.0000e+06]]], device='cuda:0'), torch.Size([3, 5, 2]))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: QKV / normalizer Z\n",
    "softmax는 row-wise하게 정규화 하는데, 우리도 똑같이 정규화가 필요하지 않냐고 그래서 Z가 필요\n",
    "여기서, QKV가 결국 row-wise 하게 정규화 되어야 한다는게 포인트임\n",
    "그렇다면 Z의 크기는 16, 512, 64가 되어야 한다\n",
    "\"\"\"\n",
    "\n",
    "summation_key = k_key.sum(dim=1).unsqueeze(1).expand(-1, dim_head, -1).permute(0,2,1)\n",
    "\n",
    "print(f\"key, summation_key: {key.shape, summation_key.shape}\")\n",
    "\n",
    "Z = 1/torch.clamp(torch.matmul(k_query, summation_key), min=1e-6)\n",
    "print(f\"Normalizer Z: {Z, Z.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T16:47:09.815960Z",
     "start_time": "2024-03-04T16:47:09.810687Z"
    }
   },
   "id": "4b94b3decb9e4270",
   "execution_count": 101
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_attn_matrix: (torch.Size([3, 5, 2]), tensor([[[3.9718e-01, 5.6831e-01],\n",
      "         [3.9631e-01, 5.6779e-01],\n",
      "         [3.9677e-01, 5.6806e-01],\n",
      "         [2.3637e+05, 8.2210e+05],\n",
      "         [5.7225e+05, 9.0375e+05]],\n",
      "\n",
      "        [[4.0818e-01, 7.0590e-01],\n",
      "         [4.1454e-01, 7.1592e-01],\n",
      "         [4.0892e-01, 7.0707e-01],\n",
      "         [4.1338e-01, 7.1410e-01],\n",
      "         [5.2581e+05, 1.8200e+05]],\n",
      "\n",
      "        [[4.6812e-01, 7.3529e-01],\n",
      "         [4.6815e-01, 7.3535e-01],\n",
      "         [4.3231e+05, 3.1439e+05],\n",
      "         [6.0615e+05, 6.2035e+05],\n",
      "         [8.3891e+05, 5.6749e+05]]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Testing for Linear Attentions: QKV / normalizer Z \n",
    "\"\"\"\n",
    "\n",
    "linear_attn_matrix = torch.mul(QKV, Z)\n",
    "print(f\"linear_attn_matrix: {linear_attn_matrix.shape, linear_attn_matrix}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T16:47:12.928666Z",
     "start_time": "2024-03-04T16:47:12.922616Z"
    }
   },
   "id": "65459cc6481c7d45",
   "execution_count": 102
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_matrix: (torch.Size([3, 5, 5]), tensor([[[0.3713, 0.6363, 1.0094, 0.5870, 0.7410],\n",
      "         [0.1055, 0.2372, 0.3701, 0.2420, 0.2239],\n",
      "         [0.2164, 0.4022, 0.6346, 0.3840, 0.4393],\n",
      "         [0.3069, 0.4024, 0.6519, 0.3202, 0.5834],\n",
      "         [0.1555, 0.2623, 0.4167, 0.2403, 0.3093]],\n",
      "\n",
      "        [[0.2487, 0.1702, 0.1442, 0.2789, 0.3663],\n",
      "         [0.0394, 0.2411, 0.1327, 0.3839, 0.3884],\n",
      "         [0.2519, 0.2133, 0.1670, 0.3475, 0.4341],\n",
      "         [0.2112, 0.4541, 0.2813, 0.7280, 0.7887],\n",
      "         [0.0527, 0.1607, 0.0945, 0.2568, 0.2699]],\n",
      "\n",
      "        [[0.3726, 0.5146, 0.2369, 0.3336, 0.5193],\n",
      "         [0.4214, 0.5751, 0.1496, 0.1591, 0.4101],\n",
      "         [0.7383, 1.0163, 0.4114, 0.5539, 0.9420],\n",
      "         [0.6791, 0.9325, 0.3396, 0.4380, 0.8084],\n",
      "         [0.5018, 0.6880, 0.2337, 0.2919, 0.5716]]], device='cuda:0'))\n",
      "attention_dist: (torch.Size([3, 5, 5]), tensor([[[0.2383, 0.3106, 0.4511, 0.0000, 0.0000],\n",
      "         [0.2904, 0.3313, 0.3783, 0.0000, 0.0000],\n",
      "         [0.2686, 0.3234, 0.4080, 0.0000, 0.0000],\n",
      "         [0.2847, 0.3132, 0.4020, 0.0000, 0.0000],\n",
      "         [0.2931, 0.3262, 0.3806, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2593, 0.2398, 0.2336, 0.2673, 0.0000],\n",
      "         [0.2113, 0.2585, 0.2320, 0.2982, 0.0000],\n",
      "         [0.2512, 0.2417, 0.2308, 0.2764, 0.0000],\n",
      "         [0.1990, 0.2538, 0.2135, 0.3337, 0.0000],\n",
      "         [0.2281, 0.2542, 0.2379, 0.2798, 0.0000]],\n",
      "\n",
      "        [[0.4646, 0.5354, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4617, 0.5383, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4310, 0.5690, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4370, 0.5630, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4536, 0.5464, 0.0000, 0.0000, 0.0000]]], device='cuda:0'))\n",
      "attention_matrix: (torch.Size([3, 5, 2]), tensor([[[0.3691, 0.5519],\n",
      "         [0.4015, 0.5708],\n",
      "         [0.3881, 0.5630],\n",
      "         [0.3950, 0.5671],\n",
      "         [0.4020, 0.5712]],\n",
      "\n",
      "        [[0.4038, 0.6930],\n",
      "         [0.4199, 0.7168],\n",
      "         [0.4049, 0.6984],\n",
      "         [0.4116, 0.7319],\n",
      "         [0.4174, 0.7058]],\n",
      "\n",
      "        [[0.4683, 0.7356],\n",
      "         [0.4691, 0.7369],\n",
      "         [0.4780, 0.7515],\n",
      "         [0.4763, 0.7486],\n",
      "         [0.4715, 0.7408]]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Comparing with pure self-attention\n",
    "\"\"\"\n",
    "\n",
    "attn_matrix = torch.matmul(query, key.transpose(-1, -2)) / torch.sqrt(torch.tensor(dim_head))\n",
    "print(f\"attn_matrix: {attn_matrix.shape, attn_matrix}\")\n",
    "\n",
    "padding_mask = padding_mask.unsqueeze(1)\n",
    "attn_matrix = attn_matrix.masked_fill(padding_mask == 1, float('-inf'))\n",
    "attention_dist = F.softmax(attn_matrix, dim=-1)\n",
    "print(f\"attention_dist: {attention_dist.shape, attention_dist}\")\n",
    "\n",
    "attention_matrix = torch.matmul(attention_dist, value)\n",
    "print(f\"attention_matrix: {attention_matrix.shape, attention_matrix}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T16:47:17.615429Z",
     "start_time": "2024-03-04T16:47:17.591879Z"
    }
   },
   "id": "5b0242ad3d65be20",
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(inf, device='cuda:0')"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Comparing with pure self-attention by KL-divergence\n",
    "배치 별, 평균 총합 0.5 ~ 2 정도 차이남, 이게 보니까 처음에 랜덤 초기화 빨로 갈리네\n",
    "\"\"\"\n",
    "\n",
    "kl_div = F.kl_div(linear_attn_matrix.log(), attention_matrix, reduction='batchmean')\n",
    "kl_div"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T13:13:39.924499Z",
     "start_time": "2024-03-04T13:13:39.919385Z"
    }
   },
   "id": "2cdaaf0b6c2d4400",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[-0.0264, -0.0698, -1.3971, -1.5667],\n         [ 0.5706, -1.4718,  0.3398, -0.5280],\n         [ 1.5465,  0.2453,  1.0911,  0.6062],\n         [ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n\n        [[ 0.4773,  1.9033,  0.6571,  0.4388],\n         [ 0.1047, -0.3162, -3.4738, -0.4424],\n         [ 0.4799,  0.7670, -1.0056, -0.4248],\n         [ 1.2279, -0.7639,  1.4043, -0.2604],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n\n        [[-0.1115,  0.3349, -1.0625,  0.5592],\n         [ 0.3361,  1.7460,  1.9226,  0.5757],\n         [ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]], device='cuda:0')"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Test code for applying padding masking to linear attention \"\"\"\n",
    "test_q = torch.randn(3, 5, 4, device=device)\n",
    "test_k = torch.randn(3, 5, 4, device=device)\n",
    "test_v = torch.randn(3, 5, 4, device=device)\n",
    "\n",
    "padding_mask = torch.tensor([\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 0, 1, 1, 1]],\n",
    "    device=device\n",
    ")\n",
    "test_k[padding_mask == 1] = 0\n",
    "test_k"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T04:00:33.032590Z",
     "start_time": "2024-03-04T04:00:33.027595Z"
    }
   },
   "id": "26644eb6ded2d1c8",
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    def __init__(self, dim_model: int= 768):\n",
    "        super().__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.i_arr = torch.arange(1, int(dim_model/2)+1)  # 세타값을 살리려면 \n",
    "        self.theta = 10000**(-2*(self.i_arr - 1)/self.dim_model)\n",
    "    \n",
    "    def forward(self):\n",
    "        print(self.i_arr.shape)\n",
    "        print(self.i_arr)\n",
    "        print(self.theta.shape)\n",
    "        print(self.theta)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:49.209569Z",
     "start_time": "2024-03-02T13:30:49.203796Z"
    }
   },
   "id": "97c1f10595f4412f",
   "execution_count": 92
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384])\n",
      "tensor([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,\n",
      "         15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,\n",
      "         29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,\n",
      "         43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,  56,\n",
      "         57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,\n",
      "         71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,  84,\n",
      "         85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,  98,\n",
      "         99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
      "        113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126,\n",
      "        127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140,\n",
      "        141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154,\n",
      "        155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
      "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
      "        183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
      "        197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210,\n",
      "        211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
      "        225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
      "        239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252,\n",
      "        253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
      "        267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
      "        281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294,\n",
      "        295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308,\n",
      "        309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322,\n",
      "        323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
      "        337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
      "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
      "        365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378,\n",
      "        379, 380, 381, 382, 383, 384])\n",
      "torch.Size([384])\n",
      "tensor([1.0000e+00, 9.7630e-01, 9.5316e-01, 9.3057e-01, 9.0852e-01, 8.8699e-01,\n",
      "        8.6596e-01, 8.4544e-01, 8.2540e-01, 8.0584e-01, 7.8674e-01, 7.6810e-01,\n",
      "        7.4989e-01, 7.3212e-01, 7.1477e-01, 6.9783e-01, 6.8129e-01, 6.6515e-01,\n",
      "        6.4938e-01, 6.3399e-01, 6.1897e-01, 6.0430e-01, 5.8997e-01, 5.7599e-01,\n",
      "        5.6234e-01, 5.4901e-01, 5.3600e-01, 5.2330e-01, 5.1090e-01, 4.9879e-01,\n",
      "        4.8697e-01, 4.7543e-01, 4.6416e-01, 4.5316e-01, 4.4242e-01, 4.3193e-01,\n",
      "        4.2170e-01, 4.1170e-01, 4.0195e-01, 3.9242e-01, 3.8312e-01, 3.7404e-01,\n",
      "        3.6517e-01, 3.5652e-01, 3.4807e-01, 3.3982e-01, 3.3177e-01, 3.2390e-01,\n",
      "        3.1623e-01, 3.0873e-01, 3.0142e-01, 2.9427e-01, 2.8730e-01, 2.8049e-01,\n",
      "        2.7384e-01, 2.6735e-01, 2.6102e-01, 2.5483e-01, 2.4879e-01, 2.4289e-01,\n",
      "        2.3714e-01, 2.3152e-01, 2.2603e-01, 2.2067e-01, 2.1544e-01, 2.1034e-01,\n",
      "        2.0535e-01, 2.0049e-01, 1.9573e-01, 1.9110e-01, 1.8657e-01, 1.8214e-01,\n",
      "        1.7783e-01, 1.7361e-01, 1.6950e-01, 1.6548e-01, 1.6156e-01, 1.5773e-01,\n",
      "        1.5399e-01, 1.5034e-01, 1.4678e-01, 1.4330e-01, 1.3991e-01, 1.3659e-01,\n",
      "        1.3335e-01, 1.3019e-01, 1.2711e-01, 1.2409e-01, 1.2115e-01, 1.1828e-01,\n",
      "        1.1548e-01, 1.1274e-01, 1.1007e-01, 1.0746e-01, 1.0491e-01, 1.0243e-01,\n",
      "        1.0000e-01, 9.7630e-02, 9.5316e-02, 9.3057e-02, 9.0852e-02, 8.8699e-02,\n",
      "        8.6596e-02, 8.4544e-02, 8.2540e-02, 8.0584e-02, 7.8674e-02, 7.6810e-02,\n",
      "        7.4989e-02, 7.3212e-02, 7.1477e-02, 6.9783e-02, 6.8129e-02, 6.6515e-02,\n",
      "        6.4938e-02, 6.3399e-02, 6.1897e-02, 6.0430e-02, 5.8997e-02, 5.7599e-02,\n",
      "        5.6234e-02, 5.4901e-02, 5.3600e-02, 5.2330e-02, 5.1090e-02, 4.9879e-02,\n",
      "        4.8697e-02, 4.7543e-02, 4.6416e-02, 4.5316e-02, 4.4242e-02, 4.3193e-02,\n",
      "        4.2170e-02, 4.1170e-02, 4.0195e-02, 3.9242e-02, 3.8312e-02, 3.7404e-02,\n",
      "        3.6517e-02, 3.5652e-02, 3.4807e-02, 3.3982e-02, 3.3177e-02, 3.2390e-02,\n",
      "        3.1623e-02, 3.0873e-02, 3.0142e-02, 2.9427e-02, 2.8730e-02, 2.8049e-02,\n",
      "        2.7384e-02, 2.6735e-02, 2.6102e-02, 2.5483e-02, 2.4879e-02, 2.4289e-02,\n",
      "        2.3714e-02, 2.3152e-02, 2.2603e-02, 2.2067e-02, 2.1544e-02, 2.1034e-02,\n",
      "        2.0535e-02, 2.0049e-02, 1.9573e-02, 1.9110e-02, 1.8657e-02, 1.8214e-02,\n",
      "        1.7783e-02, 1.7361e-02, 1.6950e-02, 1.6548e-02, 1.6156e-02, 1.5773e-02,\n",
      "        1.5399e-02, 1.5034e-02, 1.4678e-02, 1.4330e-02, 1.3991e-02, 1.3659e-02,\n",
      "        1.3335e-02, 1.3019e-02, 1.2711e-02, 1.2409e-02, 1.2115e-02, 1.1828e-02,\n",
      "        1.1548e-02, 1.1274e-02, 1.1007e-02, 1.0746e-02, 1.0491e-02, 1.0243e-02,\n",
      "        1.0000e-02, 9.7630e-03, 9.5316e-03, 9.3057e-03, 9.0852e-03, 8.8699e-03,\n",
      "        8.6596e-03, 8.4544e-03, 8.2540e-03, 8.0584e-03, 7.8674e-03, 7.6810e-03,\n",
      "        7.4989e-03, 7.3212e-03, 7.1477e-03, 6.9783e-03, 6.8129e-03, 6.6515e-03,\n",
      "        6.4938e-03, 6.3399e-03, 6.1897e-03, 6.0430e-03, 5.8997e-03, 5.7599e-03,\n",
      "        5.6234e-03, 5.4901e-03, 5.3600e-03, 5.2330e-03, 5.1090e-03, 4.9879e-03,\n",
      "        4.8697e-03, 4.7543e-03, 4.6416e-03, 4.5316e-03, 4.4242e-03, 4.3193e-03,\n",
      "        4.2170e-03, 4.1170e-03, 4.0195e-03, 3.9242e-03, 3.8312e-03, 3.7404e-03,\n",
      "        3.6517e-03, 3.5652e-03, 3.4807e-03, 3.3982e-03, 3.3177e-03, 3.2390e-03,\n",
      "        3.1623e-03, 3.0873e-03, 3.0142e-03, 2.9427e-03, 2.8730e-03, 2.8049e-03,\n",
      "        2.7384e-03, 2.6735e-03, 2.6102e-03, 2.5483e-03, 2.4879e-03, 2.4289e-03,\n",
      "        2.3714e-03, 2.3152e-03, 2.2603e-03, 2.2067e-03, 2.1544e-03, 2.1034e-03,\n",
      "        2.0535e-03, 2.0049e-03, 1.9573e-03, 1.9110e-03, 1.8657e-03, 1.8214e-03,\n",
      "        1.7783e-03, 1.7361e-03, 1.6950e-03, 1.6548e-03, 1.6156e-03, 1.5773e-03,\n",
      "        1.5399e-03, 1.5034e-03, 1.4678e-03, 1.4330e-03, 1.3991e-03, 1.3659e-03,\n",
      "        1.3335e-03, 1.3019e-03, 1.2711e-03, 1.2409e-03, 1.2115e-03, 1.1828e-03,\n",
      "        1.1548e-03, 1.1274e-03, 1.1007e-03, 1.0746e-03, 1.0491e-03, 1.0243e-03,\n",
      "        1.0000e-03, 9.7630e-04, 9.5316e-04, 9.3057e-04, 9.0852e-04, 8.8699e-04,\n",
      "        8.6596e-04, 8.4544e-04, 8.2540e-04, 8.0584e-04, 7.8674e-04, 7.6810e-04,\n",
      "        7.4989e-04, 7.3212e-04, 7.1477e-04, 6.9783e-04, 6.8129e-04, 6.6515e-04,\n",
      "        6.4938e-04, 6.3399e-04, 6.1897e-04, 6.0430e-04, 5.8997e-04, 5.7599e-04,\n",
      "        5.6234e-04, 5.4901e-04, 5.3600e-04, 5.2330e-04, 5.1090e-04, 4.9879e-04,\n",
      "        4.8697e-04, 4.7543e-04, 4.6416e-04, 4.5316e-04, 4.4242e-04, 4.3193e-04,\n",
      "        4.2170e-04, 4.1170e-04, 4.0195e-04, 3.9242e-04, 3.8312e-04, 3.7404e-04,\n",
      "        3.6517e-04, 3.5652e-04, 3.4807e-04, 3.3982e-04, 3.3177e-04, 3.2390e-04,\n",
      "        3.1623e-04, 3.0873e-04, 3.0142e-04, 2.9427e-04, 2.8730e-04, 2.8049e-04,\n",
      "        2.7384e-04, 2.6735e-04, 2.6102e-04, 2.5483e-04, 2.4879e-04, 2.4289e-04,\n",
      "        2.3714e-04, 2.3152e-04, 2.2603e-04, 2.2067e-04, 2.1544e-04, 2.1034e-04,\n",
      "        2.0535e-04, 2.0049e-04, 1.9573e-04, 1.9110e-04, 1.8657e-04, 1.8214e-04,\n",
      "        1.7783e-04, 1.7361e-04, 1.6950e-04, 1.6548e-04, 1.6156e-04, 1.5773e-04,\n",
      "        1.5399e-04, 1.5034e-04, 1.4678e-04, 1.4330e-04, 1.3991e-04, 1.3659e-04,\n",
      "        1.3335e-04, 1.3019e-04, 1.2711e-04, 1.2409e-04, 1.2115e-04, 1.1828e-04,\n",
      "        1.1548e-04, 1.1274e-04, 1.1007e-04, 1.0746e-04, 1.0491e-04, 1.0243e-04])\n"
     ]
    }
   ],
   "source": [
    "test = RoPE()\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:49.213790Z",
     "start_time": "2024-03-02T13:30:49.206814Z"
    }
   },
   "id": "938bd92df37c8366",
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T13:30:49.227687Z",
     "start_time": "2024-03-02T13:30:49.213511Z"
    }
   },
   "id": "e598d761ff00999a",
   "execution_count": 93
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
